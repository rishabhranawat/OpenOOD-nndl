{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bhw8NPwVRbdT",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "\n",
    "import time\n",
    "import json\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "\n",
    "from torchvision.datasets import mnist, FashionMNIST, CIFAR10, CIFAR100\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.nn import Module\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torchvision.models.resnet import Bottleneck, ResNet\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "from wilds import get_dataset\n",
    "from wilds.common.data_loaders import get_train_loader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from openood.evaluators import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "11glohxERbdU",
    "outputId": "a412b2d2-d294-470d-b27a-abd3627aa087",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.2\n"
     ]
    }
   ],
   "source": [
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1tHAXNmwRbdU",
    "outputId": "ae15afd4-3c31-4e5c-d557-178f188de89c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WtgqLe2SajQb",
    "outputId": "3961662e-2a97-4bb0-a56c-11d9e69e79a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rdr2143/oodn-final-project/OpenOOD-nndl\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWs4cttKRbdU",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Supported Activation Functions\n",
    "\n",
    "For activation functions, we are considering ReLU, Softplus, Swish. *Note that we may conduct experiments for a subset based on the compute resources available*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "wFDrLIC4RbdV",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_activation_fn(activation):\n",
    "    if activation == 'relu':\n",
    "        return nn.ReLU()\n",
    "    elif activation == 'softplus':\n",
    "        return nn.Softplus()\n",
    "    elif activation == 'swish':\n",
    "        return nn.Swish()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ORnYhf-1RbdV",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7vRFe847RbdV",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-H6TJweYRbdV",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self, num_classes, num_channel=3, activation='relu'):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.feature_size = 84\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=num_channel,\n",
    "                      out_channels=6,\n",
    "                      kernel_size=5,\n",
    "                      stride=1,\n",
    "                      padding=2), get_activation_fn(activation), nn.MaxPool2d(kernel_size=2))\n",
    "\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1),\n",
    "             get_activation_fn(activation), nn.MaxPool2d(kernel_size=2))\n",
    "\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16,\n",
    "                      out_channels=120,\n",
    "                      kernel_size=5,\n",
    "                      stride=1), get_activation_fn(activation))\n",
    "\n",
    "        self.classifier1 = nn.Linear(in_features=120, out_features=84)\n",
    "        self.relu = get_activation_fn(activation)\n",
    "        self.fc = nn.Linear(in_features=84, out_features=num_classes)\n",
    "\n",
    "    def get_fc(self):\n",
    "        fc = self.fc\n",
    "        return fc.weight.cpu().detach().numpy(), fc.bias.cpu().detach().numpy()\n",
    "\n",
    "    def forward(self, x, return_feature=False, return_feature_list=False):\n",
    "        feature1 = self.block1(x)\n",
    "        feature2 = self.block2(feature1)\n",
    "        feature3 = self.block3(feature2)\n",
    "        feature3 = feature3.view(feature3.shape[0], -1)\n",
    "        feature = self.relu(self.classifier1(feature3))\n",
    "        logits_cls = self.fc(feature)\n",
    "        feature_list = [feature1, feature2, feature3, feature]\n",
    "        if return_feature:\n",
    "            return logits_cls, feature\n",
    "        elif return_feature_list:\n",
    "            return logits_cls, feature_list\n",
    "        else:\n",
    "            return logits_cls\n",
    "\n",
    "    def forward_threshold(self, x, threshold):\n",
    "        feature1 = self.block1(x)\n",
    "        feature2 = self.block2(feature1)\n",
    "        feature3 = self.block3(feature2)\n",
    "        feature3 = feature3.view(feature3.shape[0], -1)\n",
    "        feature = self.relu(self.classifier1(feature3))\n",
    "        feature = feature.clip(max=threshold)\n",
    "        logits_cls = self.fc(feature)\n",
    "\n",
    "        return logits_cls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XYwc4beHRbdV",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "4EaTD1nVRbdW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model):\n",
    "    for name,param in model.named_parameters():\n",
    "        if not (name.startswith('layer4') or name.startswith('fc')):\n",
    "            param.requires_grad = False\n",
    "\n",
    "def get_resnet_model(activation_function_type, n_classes, use_pretrained=True):\n",
    "    resnet_model = models.resnet50(pretrained=use_pretrained)\n",
    "    \n",
    "    # if we use pretrained, then freeze the corresponding layers\n",
    "    if use_pretrained:\n",
    "        set_parameter_requires_grad(resnet_model, feature_extract)\n",
    "\n",
    "    set_activation_function(resnet_model,activation_function_type)\n",
    "    num_ftrs = resnet_model.fc.in_features\n",
    "    resnet_model.fc = nn.Linear(num_ftrs, n_classes)\n",
    "    resnet_model.to(device)\n",
    "    return resnet_model\n",
    "\n",
    "def set_activation_function(resnet_model, activation_function_type):\n",
    "    resnet_model.relu = get_activation_fn(activation_function_type)\n",
    "    resnet_model.layer1[0].relu = get_activation_fn(activation_function_type)\n",
    "    resnet_model.layer1[1].relu = get_activation_fn(activation_function_type)\n",
    "    resnet_model.layer1[2].relu = get_activation_fn(activation_function_type)\n",
    "\n",
    "    resnet_model.layer2[0].relu = get_activation_fn(activation_function_type)\n",
    "    resnet_model.layer2[1].relu = get_activation_fn(activation_function_type)\n",
    "    resnet_model.layer2[2].relu = get_activation_fn(activation_function_type)\n",
    "    resnet_model.layer2[3].relu = get_activation_fn(activation_function_type)\n",
    "\n",
    "    resnet_model.layer3[0].relu = get_activation_fn(activation_function_type)\n",
    "    resnet_model.layer3[1].relu = get_activation_fn(activation_function_type)\n",
    "    resnet_model.layer3[2].relu = get_activation_fn(activation_function_type)\n",
    "    resnet_model.layer3[3].relu = get_activation_fn(activation_function_type)\n",
    "    resnet_model.layer3[4].relu = get_activation_fn(activation_function_type)\n",
    "    resnet_model.layer3[5].relu = get_activation_fn(activation_function_type)\n",
    "\n",
    "\n",
    "    resnet_model.layer4[0].relu = get_activation_fn(activation_function_type)\n",
    "    resnet_model.layer4[1].relu = get_activation_fn(activation_function_type)\n",
    "    resnet_model.layer4[2].relu = get_activation_fn(activation_function_type)\n",
    "\n",
    "    return resnet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Mm1fB4piRbdW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_model(config):\n",
    "    activation_function_type = config[\"activation_function_type\"]\n",
    "    network_type = config[\"network\"]\n",
    "    n_classes = config[\"n_classes\"]\n",
    "\n",
    "    if network_type == \"lenet\":\n",
    "        model =  LeNet(num_classes=n_classes, num_channel=1, activation=activation_function_type)\n",
    "    elif network_type == \"resnet50\":\n",
    "        model = get_resnet_model(activation_function_type, n_classes, config['pretrained'])\n",
    "    else:\n",
    "        raise Exception(\"Currently we only support lenet or resnet50\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9veLq_LDRbdW",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKbri3J-RbdW",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Supported Post-Hoc OODN Processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OODPostprocessor():\n",
    "    \n",
    "    def inference(self, net: nn.Module, data_loader: DataLoader):\n",
    "        pred_list, conf_list, label_list = [], [], []\n",
    "        for idx, loaded_data in enumerate(data_loader):\n",
    "            data, label = loaded_data[0], loaded_data[1]\n",
    "            if idx % 50 == 0:\n",
    "                print(f'Performing inference on batch: {idx}')\n",
    "            pred, conf = self.postprocess(net, data.to(device))\n",
    "            for idx in range(len(data)):\n",
    "                pred_list.append(pred[idx].tolist())\n",
    "                conf_list.append(conf[idx].tolist())\n",
    "                label_list.append(label[idx].tolist())\n",
    "\n",
    "        # convert values into numpy array\n",
    "        pred_list = np.array(pred_list, dtype=int)\n",
    "        conf_list = np.array(conf_list)\n",
    "        label_list = np.array(label_list, dtype=int)\n",
    "\n",
    "        return pred_list, conf_list, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODINPostprocessor(OODPostprocessor):\n",
    "    def __init__(self, temperature, noise):\n",
    "        super(OODPostprocessor)\n",
    "        self.temperature = temperature\n",
    "        self.noise = noise\n",
    "        \n",
    "    def postprocess(self, net: nn.Module, data):\n",
    "        net.eval()\n",
    "        data.requires_grad = True\n",
    "        output = net(data)\n",
    "\n",
    "        # Calculating the perturbation we need to add, that is,\n",
    "        # the sign of gradient of cross entropy loss w.r.t. input\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        labels = output.detach().argmax(axis=1)\n",
    "\n",
    "        # Using temperature scaling\n",
    "        output = output / self.temperature\n",
    "\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Normalizing the gradient to binary in {0, 1}\n",
    "        gradient = torch.ge(data.grad.detach(), 0)\n",
    "        gradient = (gradient.float() - 0.5) * 2\n",
    "\n",
    "        # Scaling values taken from original code       \n",
    "        gradient[:, 0] = (gradient[:, 0]) / (63.0 / 255.0)\n",
    "        if gradient.shape[1] == 3:\n",
    "            gradient[:, 1] = (gradient[:, 1]) / (62.1 / 255.0)\n",
    "            gradient[:, 2] = (gradient[:, 2]) / (66.7 / 255.0)\n",
    "\n",
    "        # Adding small perturbations to images\n",
    "        tempInputs = torch.add(data.detach(), gradient, alpha=-self.noise)\n",
    "        output = net(tempInputs)\n",
    "        output = output / self.temperature\n",
    "\n",
    "        # Calculating the confidence after adding perturbations\n",
    "        nnOutput = output.detach()\n",
    "        nnOutput = nnOutput - nnOutput.max(dim=1, keepdims=True).values\n",
    "        nnOutput = nnOutput.exp() / nnOutput.exp().sum(dim=1, keepdims=True)\n",
    "\n",
    "        conf, pred = nnOutput.max(dim=1)\n",
    "\n",
    "        return pred, conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCDPostprocessor(OODPostprocessor):\n",
    "    def __init__(self, samples: int = 30):\n",
    "        super(OODPostprocessor)\n",
    "        self.samples = samples  #: number :math:`N` of samples\n",
    "\n",
    "    def postprocess(self, model: torch.nn.Module, x: torch.Tensor) -> torch.Tensor:\n",
    "        mode_switch = False\n",
    "        if not model.training:\n",
    "            mode_switch = True\n",
    "\n",
    "            model.train()\n",
    "\n",
    "            for mod in model.modules():\n",
    "                # reset batch norm layers.\n",
    "                # TODO: are there other layers?\n",
    "                if isinstance(mod, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):\n",
    "                    mod.train(False)\n",
    "\n",
    "        results = None\n",
    "        with torch.no_grad():\n",
    "            for i in range(self.samples):\n",
    "                output = model(x).softmax(dim=1)\n",
    "                if results is None:\n",
    "                    results = torch.zeros(size=output.shape).to(device)\n",
    "                results += output\n",
    "        results /= self.samples\n",
    "\n",
    "        if mode_switch:\n",
    "            model.eval()\n",
    "        \n",
    "        conf, pred = results.max(dim=1)\n",
    "\n",
    "        return pred, conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_postprocessor(postprocessor_type=\"odin\"):\n",
    "    if postprocessor_type == \"odin\":\n",
    "        postprocessor = ODINPostprocessor(1000, 0.0014)\n",
    "    elif postprocessor_type == \"mcd\":\n",
    "        postprocessor = MCDPostprocessor(30)\n",
    "    return postprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bGtatMpzRbdX",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Supported Out of Distribution Detection Metrics\n",
    "\n",
    "What metrics do we specifically care about here?\n",
    "\n",
    "**FPR@95** measures the false positive rate (FPR) when the true positive rate (TPR) is\n",
    "equal to 95%. Lower scores indicate better performance.\n",
    "\n",
    "**AUROC** measures the area under the\n",
    "Receiver Operating Characteristic (ROC) curve, which displays the relationship between TPR and\n",
    "FPR. The area under the ROC curve can be interpreted as the probability that a positive ID example\n",
    "will have a higher detection score than a negative OOD example.\n",
    "\n",
    "**AUPR** measures the area under\n",
    "the Precision-Recall (PR) curve. The PR curve is created by plotting precision versus recall. Similar\n",
    "to AUROC, we consider ID samples as positive, so that the score corresponds to the AUPR-In metric\n",
    "in some works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Xm89cB-XRbdX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_oodn_metrics(model, postprocessor_type, id_test_loader, ood_test_loader, ood_name):\n",
    "    postprocessor = get_postprocessor(postprocessor_type)\n",
    "    id_pred, id_conf, id_gt = postprocessor.inference(\n",
    "                model, id_test_loader)\n",
    "\n",
    "    ood_pred, ood_conf, ood_gt = postprocessor.inference(\n",
    "        model, ood_test_loader)\n",
    "\n",
    "    ood_gt = -1 * np.ones_like(ood_gt)  # hard set to -1 as ood\n",
    "    pred = np.concatenate([id_pred, ood_pred])\n",
    "    conf = np.concatenate([id_conf, ood_conf])\n",
    "    label = np.concatenate([id_gt, ood_gt])\n",
    "    ood_metrics = metrics.compute_all_metrics(conf, label, pred)\n",
    "\n",
    "    return print_and_get_formatted_metrics(ood_metrics, ood_name)\n",
    "\n",
    "def print_and_get_formatted_metrics(metrics, dataset_name):\n",
    "    [fpr, auroc, aupr_in, aupr_out,\n",
    "     ccr_4, ccr_3, ccr_2, ccr_1, accuracy] \\\n",
    "     = metrics\n",
    "\n",
    "    write_content = {\n",
    "        'dataset': dataset_name,\n",
    "        'FPR@95': '{:.2f}'.format(100 * fpr),\n",
    "        'AUROC': '{:.2f}'.format(100 * auroc),\n",
    "        'AUPR_IN': '{:.2f}'.format(100 * aupr_in),\n",
    "        'AUPR_OUT': '{:.2f}'.format(100 * aupr_out),\n",
    "        'CCR_4': '{:.2f}'.format(100 * ccr_4),\n",
    "        'CCR_3': '{:.2f}'.format(100 * ccr_3),\n",
    "        'CCR_2': '{:.2f}'.format(100 * ccr_2),\n",
    "        'CCR_1': '{:.2f}'.format(100 * ccr_1),\n",
    "        'ACC': '{:.2f}'.format(100 * accuracy)\n",
    "    }\n",
    "\n",
    "    fieldnames = list(write_content.keys())\n",
    "\n",
    "    # print ood metric results\n",
    "    print('FPR@95: {:.2f}, AUROC: {:.2f}'.format(100 * fpr, 100 * auroc),\n",
    "          end=' ',\n",
    "          flush=True)\n",
    "    print('AUPR_IN: {:.2f}, AUPR_OUT: {:.2f}'.format(\n",
    "        100 * aupr_in, 100 * aupr_out),\n",
    "          flush=True)\n",
    "    print('CCR: {:.2f}, {:.2f}, {:.2f}, {:.2f},'.format(\n",
    "        ccr_4 * 100, ccr_3 * 100, ccr_2 * 100, ccr_1 * 100),\n",
    "          end=' ',\n",
    "          flush=True)\n",
    "    print('ACC: {:.2f}'.format(accuracy * 100), flush=True)\n",
    "    print(u'\\u2500' * 70, flush=True)\n",
    "    return write_content\n",
    "\n",
    "def load_results_into_df(dir_path):\n",
    "    res_files = [dir_path+each for each in listdir(dir_path)]\n",
    "    all_results = []\n",
    "    columns = ['optimizer_type', 'activation_function_type', 'postprocessor_type', 'trial', 'AUROC']\n",
    "    for fp in res_files:\n",
    "        f = open(fp)\n",
    "        data = json.load(f)\n",
    "        for trial, results in data.items():\n",
    "            all_results.append([\n",
    "                    results['optimizer_type'],\n",
    "                    results['activation_function_type'],\n",
    "                    results['postprocessor_type'],\n",
    "                    trial,\n",
    "                    float(results['AUROC'])\n",
    "                ])\n",
    "    df = pd.DataFrame(all_results, columns=columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "aEJLZ75aRbdX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_optimizer(model, config):\n",
    "    params = model.parameters()\n",
    "    lr = config['lr']\n",
    "    momentum = config['momentum']\n",
    "    weight_decay = config['weight_decay']\n",
    "    optimizer_type = config['optimizer_type']\n",
    "\n",
    "    print(f'Getting optimizer for type: {optimizer_type}...')\n",
    "    if optimizer_type == 'SGD':\n",
    "        return SGD(params,\n",
    "              lr=lr,\n",
    "              momentum=momentum,\n",
    "              weight_decay=weight_decay)\n",
    "    elif optimizer_type == 'Adam':\n",
    "        return Adam(params,\n",
    "                    lr=lr)\n",
    "    else:\n",
    "        raise Exception(\"Invalid optimizer_type provided, only SGD and Adam are supported currently\")\n",
    "\n",
    "def get_wilds_loader(dataset, split, batch_size):\n",
    "    d = dataset.get_subset(\n",
    "        split,\n",
    "        # frac=0.1,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.Resize((448, 448)), transforms.ToTensor()]\n",
    "        ),\n",
    "    )\n",
    "    # Prepare the standard data loader\n",
    "    return get_train_loader(\"standard\", d, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "def get_data_loaders(config):\n",
    "    data_loaders = {}\n",
    "    dataset_name = config[\"dataset_name\"]\n",
    "    dataset_type = config[\"dataset_type\"]\n",
    "    batch_size = config['batch_size']\n",
    "\n",
    "    wilds_id_test_split = \"id_val\" if dataset_name == \"camelyon17\" else \"id_test\"\n",
    "    if dataset_type == \"wilds\":\n",
    "        # wilds dataset\n",
    "        dataset = get_dataset(dataset=dataset_name, download=True)\n",
    "        data_loaders[\"train\"] = get_wilds_loader(dataset, \"train\", batch_size)\n",
    "        data_loaders[\"ood_test\"] = get_wilds_loader(dataset, \"test\", batch_size)\n",
    "        data_loaders[\"id_test\"] = get_wilds_loader(dataset, wilds_id_test_split, batch_size)\n",
    "        return\n",
    "    elif dataset_name == \"cifar\":\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "\n",
    "        transform_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "        train_dataset = CIFAR10(root='data', download=True, train=True, transform=transform_train)\n",
    "        test_dataset = CIFAR10(root='data', download=True, train=False, transform=transform_test)\n",
    "        ood_test_dataset = CIFAR100(root='data', download=True, train=False, transform=transform_test)\n",
    "        \n",
    "    elif dataset_name == \"mnist\":\n",
    "        # mnist dataset\n",
    "        train_dataset = mnist.MNIST(root='data', download=True, train=True, transform=ToTensor())\n",
    "        test_dataset = mnist.MNIST(root='data', download=True, train=False, transform=ToTensor())\n",
    "        ood_test_dataset = mnist.FashionMNIST(root='data', download=True,train=False,transform=ToTensor())\n",
    "\n",
    "    data_loaders[\"train\"] = DataLoader(train_dataset, batch_size=batch_size, num_workers=4)\n",
    "    data_loaders[\"id_test\"] = DataLoader(test_dataset, batch_size=batch_size, num_workers=4)\n",
    "    data_loaders[\"ood_test\"] = DataLoader(ood_test_dataset, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "    return data_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "JfRfN2fNRbdX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_resnet_model_given_opti_activation_fn(config):\n",
    "    # get the train loader\n",
    "    train_loader = config[\"data_loaders\"][\"train\"]\n",
    "\n",
    "    # get the resnet model with the replaced activation functions\n",
    "    model = get_model(config)\n",
    "    model.to(device)\n",
    "\n",
    "    # get the optimizer\n",
    "    sgd = get_optimizer(model, config)\n",
    "\n",
    "    loss_fn = CrossEntropyLoss()\n",
    "    for current_epoch in range(config['epochs']):\n",
    "        tic=time.time()\n",
    "        per_batch_time = time.time()\n",
    "        model.train()\n",
    "        print('Training epoch: {}'.format(current_epoch))\n",
    "        for idx, (loader_data) in enumerate(train_loader):\n",
    "            train_x, train_label = loader_data[0].to(device), loader_data[1].to(device)\n",
    "            sgd.zero_grad()\n",
    "            predict_y = model(train_x.float())\n",
    "            loss = loss_fn(predict_y, train_label.long())\n",
    "            if idx % 100 == 0:\n",
    "                print('idx: {}, loss: {} time take: {}'.format(idx, loss.sum().item(), time.time() - per_batch_time))\n",
    "                per_batch_time = time.time()\n",
    "            loss.backward()\n",
    "            sgd.step()\n",
    "        print(f\"epoch {current_epoch} time taken: {time.time()-tic}s\")\n",
    "    torch.save(model, config['model_name'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def run_full_oodn_pipeline(config):\n",
    "    metrics = {}\n",
    "    for i in range(config[\"trials\"]):\n",
    "        model_name = f\"models/{config['dataset_name']}_{config['network']}_{config['postprocessor_type']}_{config['activation_function_type']}_{config['optimizer_type']}_{i}.pkl\"\n",
    "        print(f'Running model: {model_name}...')\n",
    "        config['model_name'] = model_name\n",
    "        # train model\n",
    "        model = train_resnet_model_given_opti_activation_fn(config)\n",
    "        # calculate oodn metrics\n",
    "        metrics[i] = calculate_oodn_metrics(model,\n",
    "                               config['postprocessor_type'],\n",
    "                               config[\"data_loaders\"][\"id_test\"],\n",
    "                               config[\"data_loaders\"][\"ood_test\"],\n",
    "                               config[\"dataset_name\"])\n",
    "        metrics[i]['optimizer_type'] = config['optimizer_type']\n",
    "        metrics[i]['activation_function_type'] = config['activation_function_type']\n",
    "        metrics[i]['postprocessor_type'] = config['postprocessor_type']\n",
    "\n",
    "    experiment_name = f\"{config['results_dir']}/{config['dataset_name']}_{config['network']}_{config['postprocessor_type']}_{config['activation_function_type']}_{config['optimizer_type']}.json\"\n",
    "    with open(experiment_name, 'w') as fp:\n",
    "        json.dump(metrics, fp)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study 2: Resnet, CIFAR-10 (ID), CIFAR-100 (OOD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Study 2(a): Adam + ReLU + Odin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lzf8wgoTRbdY",
    "outputId": "b0d274c3-4075-4581-97dc-e3fce18f24d2",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Running model: models/cifar_resnet50_odin_relu_Adam_0.pkl...\n",
      "Getting optimizer for type: Adam...\n",
      "Training epoch: 0\n",
      "idx: 0, loss: 2.8178350925445557 time take: 0.2426450252532959\n",
      "idx: 100, loss: 3.1174871921539307 time take: 6.610335350036621\n",
      "idx: 200, loss: 2.9341554641723633 time take: 6.59183406829834\n",
      "idx: 300, loss: 1.8614654541015625 time take: 6.668043613433838\n",
      "epoch 0 time taken: 26.097416400909424s\n",
      "Training epoch: 1\n",
      "idx: 0, loss: 1.8956336975097656 time take: 0.2481977939605713\n",
      "idx: 100, loss: 1.6939016580581665 time take: 6.58957839012146\n",
      "idx: 200, loss: 1.5685464143753052 time take: 6.630019426345825\n",
      "idx: 300, loss: 1.6713684797286987 time take: 6.604220151901245\n",
      "epoch 1 time taken: 26.034528970718384s\n",
      "Training epoch: 2\n",
      "idx: 0, loss: 1.499733567237854 time take: 0.25389885902404785\n",
      "idx: 100, loss: 1.3913094997406006 time take: 6.598061800003052\n",
      "idx: 200, loss: 1.4621082544326782 time take: 6.5833094120025635\n",
      "idx: 300, loss: 1.3565441370010376 time take: 6.584050178527832\n",
      "epoch 2 time taken: 25.991873741149902s\n",
      "Training epoch: 3\n",
      "idx: 0, loss: 1.3809436559677124 time take: 0.2573387622833252\n",
      "idx: 100, loss: 1.2603024244308472 time take: 6.595066070556641\n",
      "idx: 200, loss: 1.304347276687622 time take: 6.584744930267334\n",
      "idx: 300, loss: 1.2405844926834106 time take: 6.64442253112793\n",
      "epoch 3 time taken: 26.157347679138184s\n",
      "Training epoch: 4\n",
      "idx: 0, loss: 1.231187343597412 time take: 0.235548734664917\n",
      "idx: 100, loss: 1.097686529159546 time take: 6.605839729309082\n",
      "idx: 200, loss: 1.197885274887085 time take: 6.585491895675659\n",
      "idx: 300, loss: 1.104506492614746 time take: 6.589738607406616\n",
      "epoch 4 time taken: 25.992867469787598s\n",
      "Training epoch: 5\n",
      "idx: 0, loss: 1.092205286026001 time take: 0.23789572715759277\n",
      "idx: 100, loss: 1.0655491352081299 time take: 6.696850299835205\n",
      "idx: 200, loss: 1.1274880170822144 time take: 6.593417406082153\n",
      "idx: 300, loss: 1.0855573415756226 time take: 6.5869691371917725\n",
      "epoch 5 time taken: 26.088666200637817s\n",
      "Training epoch: 6\n",
      "idx: 0, loss: 1.100014567375183 time take: 0.23308157920837402\n",
      "idx: 100, loss: 0.9996551871299744 time take: 6.719158887863159\n",
      "idx: 200, loss: 1.143564224243164 time take: 6.672304391860962\n",
      "idx: 300, loss: 1.0197150707244873 time take: 6.625519514083862\n",
      "epoch 6 time taken: 26.2484290599823s\n",
      "Training epoch: 7\n",
      "idx: 0, loss: 0.9541996121406555 time take: 0.25489139556884766\n",
      "idx: 100, loss: 1.0103418827056885 time take: 6.674681663513184\n",
      "idx: 200, loss: 0.9313892126083374 time take: 6.590013742446899\n",
      "idx: 300, loss: 0.9462883472442627 time take: 6.583564519882202\n",
      "epoch 7 time taken: 26.073886394500732s\n",
      "Training epoch: 8\n",
      "idx: 0, loss: 1.0277804136276245 time take: 0.2471756935119629\n",
      "idx: 100, loss: 0.916275143623352 time take: 6.598319053649902\n",
      "idx: 200, loss: 1.05243980884552 time take: 6.6232264041900635\n",
      "idx: 300, loss: 0.8365287184715271 time take: 6.595332860946655\n",
      "epoch 8 time taken: 26.033037662506104s\n",
      "Training epoch: 9\n",
      "idx: 0, loss: 0.9662333726882935 time take: 0.24139690399169922\n",
      "idx: 100, loss: 0.8445675373077393 time take: 6.604330062866211\n",
      "idx: 200, loss: 1.0025018453598022 time take: 6.58539605140686\n",
      "idx: 300, loss: 0.9896714091300964 time take: 6.584846019744873\n",
      "epoch 9 time taken: 25.987028121948242s\n",
      "Training epoch: 10\n",
      "idx: 0, loss: 0.7518019080162048 time take: 0.24101805686950684\n",
      "idx: 100, loss: 0.7640998363494873 time take: 6.589263200759888\n",
      "idx: 200, loss: 0.8171430826187134 time take: 6.684139251708984\n",
      "idx: 300, loss: 0.7074208855628967 time take: 6.688973665237427\n",
      "epoch 10 time taken: 26.173725366592407s\n",
      "Training epoch: 11\n",
      "idx: 0, loss: 0.8717806339263916 time take: 0.2439415454864502\n",
      "idx: 100, loss: 0.7276459336280823 time take: 6.60034441947937\n",
      "idx: 200, loss: 0.6912395358085632 time take: 6.5891382694244385\n",
      "idx: 300, loss: 0.7005816698074341 time take: 6.587058067321777\n",
      "epoch 11 time taken: 25.99103045463562s\n",
      "Training epoch: 12\n",
      "idx: 0, loss: 0.7703108191490173 time take: 0.257340669631958\n",
      "idx: 100, loss: 0.7129254341125488 time take: 6.683871507644653\n",
      "idx: 200, loss: 0.9390954375267029 time take: 6.5914411544799805\n",
      "idx: 300, loss: 0.9135555624961853 time take: 6.663856506347656\n",
      "epoch 12 time taken: 26.280179500579834s\n",
      "Training epoch: 13\n",
      "idx: 0, loss: 0.7766303420066833 time take: 0.2443861961364746\n",
      "idx: 100, loss: 0.7321804165840149 time take: 6.592090845108032\n",
      "idx: 200, loss: 0.7223373055458069 time take: 6.606055736541748\n",
      "idx: 300, loss: 0.7480923533439636 time take: 6.597455739974976\n",
      "epoch 13 time taken: 26.00940442085266s\n",
      "Training epoch: 14\n",
      "idx: 0, loss: 0.6221829652786255 time take: 0.25738525390625\n",
      "idx: 100, loss: 0.6738644242286682 time take: 6.598472833633423\n",
      "idx: 200, loss: 0.7918227910995483 time take: 6.594327926635742\n",
      "idx: 300, loss: 0.7595647573471069 time take: 6.60014796257019\n",
      "epoch 14 time taken: 26.02428126335144s\n",
      "Training epoch: 15\n",
      "idx: 0, loss: 0.6369002461433411 time take: 0.2650883197784424\n",
      "idx: 100, loss: 0.64480060338974 time take: 6.644681692123413\n",
      "idx: 200, loss: 0.6936512589454651 time take: 6.60359787940979\n",
      "idx: 300, loss: 0.6124206185340881 time take: 6.634951114654541\n",
      "epoch 15 time taken: 26.239566802978516s\n",
      "Training epoch: 16\n",
      "idx: 0, loss: 0.4697725474834442 time take: 0.24224543571472168\n",
      "idx: 100, loss: 0.6781654953956604 time take: 6.600543022155762\n",
      "idx: 200, loss: 0.6297709345817566 time take: 6.590779066085815\n",
      "idx: 300, loss: 0.6075583696365356 time take: 6.606874465942383\n",
      "epoch 16 time taken: 26.022387504577637s\n",
      "Training epoch: 17\n",
      "idx: 0, loss: 0.6340683102607727 time take: 0.26718568801879883\n",
      "idx: 100, loss: 0.5361355543136597 time take: 6.608100652694702\n",
      "idx: 200, loss: 0.599744975566864 time take: 6.628429651260376\n",
      "idx: 300, loss: 0.6332449316978455 time take: 6.5990729331970215\n",
      "epoch 17 time taken: 26.150186777114868s\n",
      "Training epoch: 18\n",
      "idx: 0, loss: 0.4828881025314331 time take: 0.2596559524536133\n",
      "idx: 100, loss: 0.6055520176887512 time take: 6.63593053817749\n",
      "idx: 200, loss: 0.6745406985282898 time take: 6.703634977340698\n",
      "idx: 300, loss: 0.6488682627677917 time take: 6.681850910186768\n",
      "epoch 18 time taken: 26.3624370098114s\n",
      "Training epoch: 19\n",
      "idx: 0, loss: 0.5119972229003906 time take: 0.24587464332580566\n",
      "idx: 100, loss: 0.5030176043510437 time take: 6.599092721939087\n",
      "idx: 200, loss: 0.6431630849838257 time take: 6.5943474769592285\n",
      "idx: 300, loss: 0.5281841158866882 time take: 6.594871997833252\n",
      "epoch 19 time taken: 26.057192087173462s\n",
      "Training epoch: 20\n",
      "idx: 0, loss: 0.5486277937889099 time take: 0.2490391731262207\n",
      "idx: 100, loss: 0.5560277104377747 time take: 6.603156805038452\n",
      "idx: 200, loss: 0.6868072748184204 time take: 6.596942186355591\n",
      "idx: 300, loss: 0.4993624985218048 time take: 6.596649408340454\n",
      "epoch 20 time taken: 26.02179789543152s\n",
      "Training epoch: 21\n",
      "idx: 0, loss: 0.4751289188861847 time take: 0.2639319896697998\n",
      "idx: 100, loss: 0.5667522549629211 time take: 6.63759183883667\n",
      "idx: 200, loss: 0.6025890707969666 time take: 6.594936370849609\n",
      "idx: 300, loss: 0.48755064606666565 time take: 6.6018757820129395\n",
      "epoch 21 time taken: 26.07924747467041s\n",
      "Training epoch: 22\n",
      "idx: 0, loss: 0.4676923453807831 time take: 0.2503936290740967\n",
      "idx: 100, loss: 0.570304811000824 time take: 6.639370679855347\n",
      "idx: 200, loss: 0.5659276843070984 time take: 6.594677925109863\n",
      "idx: 300, loss: 0.4728659391403198 time take: 6.594385147094727\n",
      "epoch 22 time taken: 26.05586886405945s\n",
      "Training epoch: 23\n",
      "idx: 0, loss: 0.39460742473602295 time take: 0.2503929138183594\n",
      "idx: 100, loss: 0.3950304687023163 time take: 6.605727910995483\n",
      "idx: 200, loss: 0.5872106552124023 time take: 6.59245753288269\n",
      "idx: 300, loss: 0.45785799622535706 time take: 6.5943708419799805\n",
      "epoch 23 time taken: 26.024161338806152s\n",
      "Training epoch: 24\n",
      "idx: 0, loss: 0.4985029697418213 time take: 0.2618422508239746\n",
      "idx: 100, loss: 0.4679175019264221 time take: 6.654573678970337\n",
      "idx: 200, loss: 0.5091748833656311 time take: 6.645348787307739\n",
      "idx: 300, loss: 0.5054524540901184 time take: 6.603520393371582\n",
      "epoch 24 time taken: 26.14381766319275s\n",
      "Training epoch: 25\n",
      "idx: 0, loss: 0.4620252549648285 time take: 0.24412846565246582\n",
      "idx: 100, loss: 0.9522682428359985 time take: 6.6015465259552\n",
      "idx: 200, loss: 0.9690046906471252 time take: 6.5990941524505615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx: 300, loss: 0.5411957502365112 time take: 6.709514379501343\n",
      "epoch 25 time taken: 26.26641321182251s\n",
      "Training epoch: 26\n",
      "idx: 0, loss: 0.6170827746391296 time take: 0.2566368579864502\n",
      "idx: 100, loss: 0.5026325583457947 time take: 6.604597091674805\n",
      "idx: 200, loss: 0.6255069375038147 time take: 6.596994161605835\n",
      "idx: 300, loss: 0.6075899004936218 time take: 6.639557123184204\n",
      "epoch 26 time taken: 26.081337213516235s\n",
      "Training epoch: 27\n",
      "idx: 0, loss: 0.403511643409729 time take: 0.26786375045776367\n",
      "idx: 100, loss: 0.38416874408721924 time take: 6.615552186965942\n",
      "idx: 200, loss: 0.5412840247154236 time take: 6.61034631729126\n",
      "idx: 300, loss: 0.4574718773365021 time take: 6.618563652038574\n",
      "epoch 27 time taken: 26.1037859916687s\n",
      "Training epoch: 28\n",
      "idx: 0, loss: 0.3946287930011749 time take: 0.26188039779663086\n",
      "idx: 100, loss: 0.5084614753723145 time take: 6.697983503341675\n",
      "idx: 200, loss: 0.6018609404563904 time take: 6.6684346199035645\n",
      "idx: 300, loss: 0.3823794722557068 time take: 6.699779033660889\n",
      "epoch 28 time taken: 26.34361481666565s\n",
      "Training epoch: 29\n",
      "idx: 0, loss: 0.4187454879283905 time take: 0.25797200202941895\n",
      "idx: 100, loss: 0.3680906593799591 time take: 6.620298385620117\n",
      "idx: 200, loss: 0.5049220323562622 time take: 6.607706546783447\n",
      "idx: 300, loss: 0.4196177124977112 time take: 6.607934951782227\n",
      "epoch 29 time taken: 26.08321523666382s\n",
      "Training epoch: 30\n",
      "idx: 0, loss: 0.3408471643924713 time take: 0.24580144882202148\n",
      "idx: 100, loss: 0.5475807189941406 time take: 6.672343492507935\n",
      "idx: 200, loss: 0.464445024728775 time take: 6.649256706237793\n",
      "idx: 300, loss: 0.4593000113964081 time take: 6.718154668807983\n",
      "epoch 30 time taken: 26.276362419128418s\n",
      "Training epoch: 31\n",
      "idx: 0, loss: 0.37415260076522827 time take: 0.2609260082244873\n",
      "idx: 100, loss: 0.3552103340625763 time take: 6.61834192276001\n",
      "idx: 200, loss: 0.4467204809188843 time take: 6.749507188796997\n",
      "idx: 300, loss: 0.4659051299095154 time take: 6.648792028427124\n",
      "epoch 31 time taken: 26.26959538459778s\n",
      "Training epoch: 32\n",
      "idx: 0, loss: 0.3194481432437897 time take: 0.25137996673583984\n",
      "idx: 100, loss: 0.2628454566001892 time take: 6.655586004257202\n",
      "idx: 200, loss: 0.41860267519950867 time take: 6.634029865264893\n",
      "idx: 300, loss: 0.3347657024860382 time take: 6.60984992980957\n",
      "epoch 32 time taken: 26.14183282852173s\n",
      "Training epoch: 33\n",
      "idx: 0, loss: 0.33466875553131104 time take: 0.23581719398498535\n",
      "idx: 100, loss: 0.3500867486000061 time take: 6.647799253463745\n",
      "idx: 200, loss: 0.37810277938842773 time take: 6.613985776901245\n",
      "idx: 300, loss: 0.39657488465309143 time take: 6.67130184173584\n",
      "epoch 33 time taken: 26.21547508239746s\n",
      "Training epoch: 34\n",
      "idx: 0, loss: 0.312301367521286 time take: 0.25864195823669434\n",
      "idx: 100, loss: 0.42019736766815186 time take: 6.615976333618164\n",
      "idx: 200, loss: 0.7591380476951599 time take: 6.605388164520264\n",
      "idx: 300, loss: 0.37353193759918213 time take: 6.612374782562256\n",
      "epoch 34 time taken: 26.08442759513855s\n",
      "Training epoch: 35\n",
      "idx: 0, loss: 0.43302831053733826 time take: 0.2561197280883789\n",
      "idx: 100, loss: 0.29414650797843933 time take: 6.624843120574951\n",
      "idx: 200, loss: 0.4798527657985687 time take: 6.608822822570801\n",
      "idx: 300, loss: 0.3512585759162903 time take: 6.641167640686035\n",
      "epoch 35 time taken: 26.157042980194092s\n",
      "Training epoch: 36\n",
      "idx: 0, loss: 0.45431771874427795 time take: 0.2474677562713623\n",
      "idx: 100, loss: 0.31042391061782837 time take: 6.659501314163208\n",
      "idx: 200, loss: 0.4453412890434265 time take: 6.6491382122039795\n",
      "idx: 300, loss: 0.36144396662712097 time take: 6.645128011703491\n",
      "epoch 36 time taken: 26.217857837677002s\n",
      "Training epoch: 37\n",
      "idx: 0, loss: 0.32348084449768066 time take: 0.2619447708129883\n",
      "idx: 100, loss: 0.3220823109149933 time take: 6.624263286590576\n",
      "idx: 200, loss: 0.4505513906478882 time take: 6.609130144119263\n",
      "idx: 300, loss: 0.4214478135108948 time take: 6.607827663421631\n",
      "epoch 37 time taken: 26.091858863830566s\n",
      "Training epoch: 38\n",
      "idx: 0, loss: 0.25020045042037964 time take: 0.2467024326324463\n",
      "idx: 100, loss: 0.2889567017555237 time take: 6.718876838684082\n",
      "idx: 200, loss: 0.3805902302265167 time take: 6.707982063293457\n",
      "idx: 300, loss: 0.3508170545101166 time take: 6.744188547134399\n",
      "epoch 38 time taken: 26.409533739089966s\n",
      "Training epoch: 39\n",
      "idx: 0, loss: 0.3076651096343994 time take: 0.2532672882080078\n",
      "idx: 100, loss: 0.24876950681209564 time take: 6.613989353179932\n",
      "idx: 200, loss: 0.4030241072177887 time take: 6.614982604980469\n",
      "idx: 300, loss: 0.34236961603164673 time take: 6.614205837249756\n",
      "epoch 39 time taken: 26.08667826652527s\n",
      "Training epoch: 40\n",
      "idx: 0, loss: 0.31798550486564636 time take: 0.26187682151794434\n",
      "idx: 100, loss: 0.22832359373569489 time take: 6.630655765533447\n",
      "idx: 200, loss: 0.3800548315048218 time take: 6.646011590957642\n",
      "idx: 300, loss: 0.31558868288993835 time take: 6.607701778411865\n",
      "epoch 40 time taken: 26.13564896583557s\n",
      "Training epoch: 41\n",
      "idx: 0, loss: 0.3397219181060791 time take: 0.26412105560302734\n",
      "idx: 100, loss: 0.20582541823387146 time take: 6.611636400222778\n",
      "idx: 200, loss: 0.36301547288894653 time take: 6.684226036071777\n",
      "idx: 300, loss: 0.29244616627693176 time take: 6.630023956298828\n",
      "epoch 41 time taken: 26.212366342544556s\n",
      "Training epoch: 42\n",
      "idx: 0, loss: 0.2070791870355606 time take: 0.244920015335083\n",
      "idx: 100, loss: 0.22966700792312622 time take: 6.612443923950195\n",
      "idx: 200, loss: 0.3344747722148895 time take: 6.60677170753479\n",
      "idx: 300, loss: 0.3249383866786957 time take: 6.635450601577759\n",
      "epoch 42 time taken: 26.160377740859985s\n",
      "Training epoch: 43\n",
      "idx: 0, loss: 0.3753402829170227 time take: 0.25731635093688965\n",
      "idx: 100, loss: 0.24281100928783417 time take: 6.622331380844116\n",
      "idx: 200, loss: 0.3951782286167145 time take: 6.607774019241333\n",
      "idx: 300, loss: 0.2805335223674774 time take: 6.607661962509155\n",
      "epoch 43 time taken: 26.08643078804016s\n",
      "Training epoch: 44\n",
      "idx: 0, loss: 0.2289452701807022 time take: 0.2575387954711914\n",
      "idx: 100, loss: 1.1619774103164673 time take: 6.628606081008911\n",
      "idx: 200, loss: 0.6800194978713989 time take: 6.6078455448150635\n",
      "idx: 300, loss: 0.34599047899246216 time take: 6.608563423156738\n",
      "epoch 44 time taken: 26.0949809551239s\n",
      "Training epoch: 45\n",
      "idx: 0, loss: 0.3110477030277252 time take: 0.2583296298980713\n",
      "idx: 100, loss: 0.31093478202819824 time take: 6.653860807418823\n",
      "idx: 200, loss: 0.387352854013443 time take: 6.609008073806763\n",
      "idx: 300, loss: 0.3515397310256958 time take: 6.6176557540893555\n",
      "epoch 45 time taken: 26.128042936325073s\n",
      "Training epoch: 46\n",
      "idx: 0, loss: 0.2804870009422302 time take: 0.24314165115356445\n",
      "idx: 100, loss: 0.22214443981647491 time take: 6.676421403884888\n",
      "idx: 200, loss: 0.4012267291545868 time take: 6.614298105239868\n",
      "idx: 300, loss: 0.3534156084060669 time take: 6.664867162704468\n",
      "epoch 46 time taken: 26.2016658782959s\n",
      "Training epoch: 47\n",
      "idx: 0, loss: 0.200423926115036 time take: 0.22424030303955078\n",
      "idx: 100, loss: 0.16429421305656433 time take: 6.666496276855469\n",
      "idx: 200, loss: 0.3806793987751007 time take: 6.6692163944244385\n",
      "idx: 300, loss: 0.2351911962032318 time take: 6.6153786182403564\n",
      "epoch 47 time taken: 26.200879096984863s\n",
      "Training epoch: 48\n",
      "idx: 0, loss: 0.2550075054168701 time take: 0.2541799545288086\n",
      "idx: 100, loss: 0.23915623128414154 time take: 6.614470720291138\n",
      "idx: 200, loss: 0.3435882031917572 time take: 6.60736608505249\n",
      "idx: 300, loss: 0.26180505752563477 time take: 6.6123948097229\n",
      "epoch 48 time taken: 26.102644205093384s\n",
      "Training epoch: 49\n",
      "idx: 0, loss: 0.1827702820301056 time take: 0.2252492904663086\n",
      "idx: 100, loss: 0.12718498706817627 time take: 6.719830513000488\n",
      "idx: 200, loss: 0.43010321259498596 time take: 6.693219423294067\n",
      "idx: 300, loss: 0.2972687780857086 time take: 6.70890212059021\n",
      "epoch 49 time taken: 26.387429237365723s\n",
      "Performing inference on batch: 0\n",
      "Performing inference on batch: 50\n",
      "Performing inference on batch: 0\n",
      "Performing inference on batch: 50\n",
      "FPR@95: 73.75, AUROC: 81.36 AUPR_IN: 83.13, AUPR_OUT: 78.00\n",
      "CCR: 3.19, 6.01, 22.05, 53.57, ACC: 83.93\n",
      "──────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: {'dataset': 'cifar',\n",
       "  'FPR@95': '73.75',\n",
       "  'AUROC': '81.36',\n",
       "  'AUPR_IN': '83.13',\n",
       "  'AUPR_OUT': '78.00',\n",
       "  'CCR_4': '3.19',\n",
       "  'CCR_3': '6.01',\n",
       "  'CCR_2': '22.05',\n",
       "  'CCR_1': '53.57',\n",
       "  'ACC': '83.93',\n",
       "  'optimizer_type': 'Adam',\n",
       "  'activation_function_type': 'relu',\n",
       "  'postprocessor_type': 'odin'}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_cifar_adam_relu_odin = {\n",
    "    \"batch_size\": 128,\n",
    "    \"n_classes\": 10,\n",
    "    \"dataset_name\": \"cifar\",\n",
    "    \"epochs\": 100,\n",
    "    \"version\": time.time(),\n",
    "    \"lr\": 0.01,\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 0.0005,\n",
    "    \"optimizer_type\": \"Adam\",\n",
    "    \"activation_function_type\": \"relu\",\n",
    "    \"network\": \"resnet50\",\n",
    "    \"postprocessor_type\": \"odin\",\n",
    "    \"trials\": 3,\n",
    "    \"dataset_type\": \"cifar\",\n",
    "    \"results_dir\": \"cifar10-study\",\n",
    "    \"pretrained\": False\n",
    "} \n",
    "config_cifar_adam_relu_odin[\"data_loaders\"] = get_data_loaders(config_cifar_adam_relu_odin)\n",
    "run_full_oodn_pipeline(config_cifar_adam_relu_odin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Study 2 (b.) Adam + Softplus + odin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Running model: models/cifar_resnet50_odin_softplus_Adam_0.pkl...\n",
      "Getting optimizer for type: Adam...\n",
      "Training epoch: 0\n",
      "idx: 0, loss: 2.5709333419799805 time take: 0.24824285507202148\n",
      "idx: 100, loss: 3.132185459136963 time take: 6.721421241760254\n",
      "idx: 200, loss: 3.2185091972351074 time take: 6.609935283660889\n",
      "idx: 300, loss: 2.3499503135681152 time take: 6.611340045928955\n",
      "epoch 0 time taken: 26.187455892562866s\n",
      "Training epoch: 1\n",
      "idx: 0, loss: 3.0913891792297363 time take: 0.267345666885376\n",
      "idx: 100, loss: 2.5008647441864014 time take: 6.636933088302612\n",
      "idx: 200, loss: 2.9766957759857178 time take: 6.615807056427002\n",
      "idx: 300, loss: 1.9372632503509521 time take: 6.628815650939941\n",
      "epoch 1 time taken: 26.139310121536255s\n",
      "Training epoch: 2\n",
      "idx: 0, loss: 2.0130720138549805 time take: 0.25510263442993164\n",
      "idx: 100, loss: 1.980995774269104 time take: 6.6487414836883545\n",
      "idx: 200, loss: 1.7976371049880981 time take: 6.644731283187866\n",
      "idx: 300, loss: 1.6165297031402588 time take: 6.617252349853516\n",
      "epoch 2 time taken: 26.156768321990967s\n",
      "Training epoch: 3\n",
      "idx: 0, loss: 2.0010015964508057 time take: 0.22882819175720215\n",
      "idx: 100, loss: 1.543738603591919 time take: 6.622219800949097\n",
      "idx: 200, loss: 1.6196879148483276 time take: 6.611144065856934\n",
      "idx: 300, loss: 1.655661702156067 time take: 6.641811370849609\n",
      "epoch 3 time taken: 26.155332565307617s\n",
      "Training epoch: 4\n",
      "idx: 0, loss: 1.6349128484725952 time take: 0.26592087745666504\n",
      "idx: 100, loss: 1.4616341590881348 time take: 6.61929988861084\n",
      "idx: 200, loss: 1.366461157798767 time take: 6.611501216888428\n",
      "idx: 300, loss: 1.4768292903900146 time take: 6.622096300125122\n",
      "epoch 4 time taken: 26.121315956115723s\n",
      "Training epoch: 5\n",
      "idx: 0, loss: 1.4415644407272339 time take: 0.25556397438049316\n",
      "idx: 100, loss: 1.4239177703857422 time take: 6.6239588260650635\n",
      "idx: 200, loss: 1.3708031177520752 time take: 6.612925291061401\n",
      "idx: 300, loss: 1.2411184310913086 time take: 6.612318515777588\n",
      "epoch 5 time taken: 26.095054388046265s\n",
      "Training epoch: 6\n",
      "idx: 0, loss: 1.322738528251648 time take: 0.25857090950012207\n",
      "idx: 100, loss: 1.2205005884170532 time take: 6.624561786651611\n",
      "idx: 200, loss: 1.3038314580917358 time take: 6.670221328735352\n",
      "idx: 300, loss: 1.1368844509124756 time take: 6.612375497817993\n",
      "epoch 6 time taken: 26.158684253692627s\n",
      "Training epoch: 7\n",
      "idx: 0, loss: 1.1943022012710571 time take: 0.2414388656616211\n",
      "idx: 100, loss: 1.0913985967636108 time take: 6.694554328918457\n",
      "idx: 200, loss: 1.2970560789108276 time take: 6.667722702026367\n",
      "idx: 300, loss: 1.0382829904556274 time take: 6.642889738082886\n",
      "epoch 7 time taken: 26.263634204864502s\n",
      "Training epoch: 8\n",
      "idx: 0, loss: 1.2438416481018066 time take: 0.23694539070129395\n",
      "idx: 100, loss: 1.0313081741333008 time take: 6.705114841461182\n",
      "idx: 200, loss: 1.0489805936813354 time take: 6.6810455322265625\n",
      "idx: 300, loss: 0.9930161237716675 time take: 6.644643545150757\n",
      "epoch 8 time taken: 26.259684801101685s\n",
      "Training epoch: 9\n",
      "idx: 0, loss: 0.9890609979629517 time take: 0.24551606178283691\n",
      "idx: 100, loss: 0.9028406739234924 time take: 6.621272087097168\n",
      "idx: 200, loss: 1.1335046291351318 time take: 6.609987020492554\n",
      "idx: 300, loss: 0.8693881034851074 time take: 6.626498460769653\n",
      "epoch 9 time taken: 26.15752100944519s\n",
      "Training epoch: 10\n",
      "idx: 0, loss: 0.9198686480522156 time take: 0.25513553619384766\n",
      "idx: 100, loss: 0.8690658211708069 time take: 6.62633204460144\n",
      "idx: 200, loss: 0.9425967931747437 time take: 6.6077165603637695\n",
      "idx: 300, loss: 0.8342977166175842 time take: 6.6096367835998535\n",
      "epoch 10 time taken: 26.121041536331177s\n",
      "Training epoch: 11\n",
      "idx: 0, loss: 0.9112898111343384 time take: 0.2568533420562744\n",
      "idx: 100, loss: 0.8705319166183472 time take: 6.637484550476074\n",
      "idx: 200, loss: 0.9796090126037598 time take: 6.63161826133728\n",
      "idx: 300, loss: 0.8070971369743347 time take: 6.7268548011779785\n",
      "epoch 11 time taken: 26.330254793167114s\n",
      "Training epoch: 12\n",
      "idx: 0, loss: 0.9139910936355591 time take: 0.2590196132659912\n",
      "idx: 100, loss: 0.789341151714325 time take: 6.619831323623657\n",
      "idx: 200, loss: 0.8972188234329224 time take: 6.611892461776733\n",
      "idx: 300, loss: 0.6977963447570801 time take: 6.612717390060425\n",
      "epoch 12 time taken: 26.102701425552368s\n",
      "Training epoch: 13\n",
      "idx: 0, loss: 0.803830087184906 time take: 0.23858284950256348\n",
      "idx: 100, loss: 0.7320205569267273 time take: 6.690917015075684\n",
      "idx: 200, loss: 0.8561508655548096 time take: 6.61502742767334\n",
      "idx: 300, loss: 0.7170718312263489 time take: 6.610913276672363\n",
      "epoch 13 time taken: 26.150835752487183s\n",
      "Training epoch: 14\n",
      "idx: 0, loss: 0.7960009574890137 time take: 0.2349538803100586\n",
      "idx: 100, loss: 0.7169318795204163 time take: 6.649355888366699\n",
      "idx: 200, loss: 0.8532872200012207 time take: 6.610311985015869\n",
      "idx: 300, loss: 0.7315592169761658 time take: 6.6093645095825195\n",
      "epoch 14 time taken: 26.093783140182495s\n",
      "Training epoch: 15\n",
      "idx: 0, loss: 0.677420437335968 time take: 0.25128960609436035\n",
      "idx: 100, loss: 0.7085722088813782 time take: 6.624763488769531\n",
      "idx: 200, loss: 0.7980043292045593 time take: 6.644584655761719\n",
      "idx: 300, loss: 0.6692144274711609 time take: 6.614803791046143\n",
      "epoch 15 time taken: 26.129483222961426s\n",
      "Training epoch: 16\n",
      "idx: 0, loss: 0.7016695737838745 time take: 0.27161145210266113\n",
      "idx: 100, loss: 0.6766695976257324 time take: 6.618178129196167\n",
      "idx: 200, loss: 0.7693836688995361 time take: 6.6100218296051025\n",
      "idx: 300, loss: 0.5580624341964722 time take: 6.610769748687744\n",
      "epoch 16 time taken: 26.102073431015015s\n",
      "Training epoch: 17\n",
      "idx: 0, loss: 0.7118880748748779 time take: 0.2546722888946533\n",
      "idx: 100, loss: 0.6530086994171143 time take: 6.6407928466796875\n",
      "idx: 200, loss: 0.728145956993103 time take: 6.610393285751343\n",
      "idx: 300, loss: 0.7115606665611267 time take: 6.652395009994507\n",
      "epoch 17 time taken: 26.162302255630493s\n",
      "Training epoch: 18\n",
      "idx: 0, loss: 0.6233440041542053 time take: 0.25029516220092773\n",
      "idx: 100, loss: 0.6854356527328491 time take: 6.628533124923706\n",
      "idx: 200, loss: 0.6731933951377869 time take: 6.611523866653442\n",
      "idx: 300, loss: 0.6477405428886414 time take: 6.613018751144409\n",
      "epoch 18 time taken: 26.094841718673706s\n",
      "Training epoch: 19\n",
      "idx: 0, loss: 0.5366702675819397 time take: 0.25752925872802734\n",
      "idx: 100, loss: 0.5481449961662292 time take: 6.625324487686157\n",
      "idx: 200, loss: 0.7226295471191406 time take: 6.6111180782318115\n",
      "idx: 300, loss: 0.6443467140197754 time take: 6.634118556976318\n",
      "epoch 19 time taken: 26.142433166503906s\n",
      "Training epoch: 20\n",
      "idx: 0, loss: 0.5452685356140137 time take: 0.27281975746154785\n",
      "idx: 100, loss: 0.5541412234306335 time take: 6.7187488079071045\n",
      "idx: 200, loss: 0.6791269183158875 time take: 6.6089231967926025\n",
      "idx: 300, loss: 0.5556255578994751 time take: 6.61107873916626\n",
      "epoch 20 time taken: 26.208913564682007s\n",
      "Training epoch: 21\n",
      "idx: 0, loss: 0.5534171462059021 time take: 0.2374744415283203\n",
      "idx: 100, loss: 0.5643559694290161 time take: 6.62453556060791\n",
      "idx: 200, loss: 0.7191112041473389 time take: 6.613988876342773\n",
      "idx: 300, loss: 0.5500833988189697 time take: 6.6174821853637695\n",
      "epoch 21 time taken: 26.087111473083496s\n",
      "Training epoch: 22\n",
      "idx: 0, loss: 0.4272131323814392 time take: 0.26331233978271484\n",
      "idx: 100, loss: 0.5222880244255066 time take: 6.625757932662964\n",
      "idx: 200, loss: 0.6327598094940186 time take: 6.644445419311523\n",
      "idx: 300, loss: 0.5346505641937256 time take: 6.610148906707764\n",
      "epoch 22 time taken: 26.182558298110962s\n",
      "Training epoch: 23\n",
      "idx: 0, loss: 0.4523211717605591 time take: 0.2366950511932373\n",
      "idx: 100, loss: 0.42004257440567017 time take: 6.624120712280273\n",
      "idx: 200, loss: 0.5532612204551697 time take: 6.610381364822388\n",
      "idx: 300, loss: 0.5081896185874939 time take: 6.608315467834473\n",
      "epoch 23 time taken: 26.07933759689331s\n",
      "Training epoch: 24\n",
      "idx: 0, loss: 0.443806529045105 time take: 0.24672913551330566\n",
      "idx: 100, loss: 0.46520689129829407 time take: 6.624578237533569\n",
      "idx: 200, loss: 0.647087574005127 time take: 6.606216669082642\n",
      "idx: 300, loss: 0.5326889753341675 time take: 6.709202766418457\n",
      "epoch 24 time taken: 26.23517155647278s\n",
      "Training epoch: 25\n",
      "idx: 0, loss: 0.44360482692718506 time take: 0.2643413543701172\n",
      "idx: 100, loss: 0.3616349697113037 time take: 6.626355171203613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx: 200, loss: 0.5319328308105469 time take: 6.623592853546143\n",
      "idx: 300, loss: 0.48465731739997864 time take: 6.611387491226196\n",
      "epoch 25 time taken: 26.133150577545166s\n",
      "Training epoch: 26\n",
      "idx: 0, loss: 0.4033209979534149 time take: 0.23418903350830078\n",
      "idx: 100, loss: 0.5307964086532593 time take: 6.6245338916778564\n",
      "idx: 200, loss: 0.6812744140625 time take: 6.607555627822876\n",
      "idx: 300, loss: 0.4704570174217224 time take: 6.628464460372925\n",
      "epoch 26 time taken: 26.16669535636902s\n",
      "Training epoch: 27\n",
      "idx: 0, loss: 0.44897329807281494 time take: 0.2201521396636963\n",
      "idx: 100, loss: 0.39711782336235046 time take: 6.631270170211792\n",
      "idx: 200, loss: 0.48680832982063293 time take: 6.607851505279541\n",
      "idx: 300, loss: 0.4265068471431732 time take: 6.612071752548218\n",
      "epoch 27 time taken: 26.070226430892944s\n",
      "Training epoch: 28\n",
      "idx: 0, loss: 0.4318372309207916 time take: 0.25647926330566406\n",
      "idx: 100, loss: 0.4094410836696625 time take: 6.631103754043579\n",
      "idx: 200, loss: 0.552060067653656 time take: 6.613260746002197\n",
      "idx: 300, loss: 0.3872531056404114 time take: 6.609636545181274\n",
      "epoch 28 time taken: 26.100834131240845s\n",
      "Training epoch: 29\n",
      "idx: 0, loss: 0.41534364223480225 time take: 0.2651200294494629\n",
      "idx: 100, loss: 0.4778408706188202 time take: 6.647677659988403\n",
      "idx: 200, loss: 0.5035849213600159 time take: 6.628444671630859\n",
      "idx: 300, loss: 0.4365878999233246 time take: 6.608978986740112\n",
      "epoch 29 time taken: 26.14231538772583s\n",
      "Training epoch: 30\n",
      "idx: 0, loss: 0.3725419342517853 time take: 0.24914836883544922\n",
      "idx: 100, loss: 0.3928418457508087 time take: 6.614974021911621\n",
      "idx: 200, loss: 0.4000197649002075 time take: 6.608262300491333\n",
      "idx: 300, loss: 0.33072322607040405 time take: 6.609888792037964\n",
      "epoch 30 time taken: 26.090506315231323s\n",
      "Training epoch: 31\n",
      "idx: 0, loss: 0.40091264247894287 time take: 0.262845516204834\n",
      "idx: 100, loss: 0.373938649892807 time take: 6.621020317077637\n",
      "idx: 200, loss: 0.49274778366088867 time take: 6.610277891159058\n",
      "idx: 300, loss: 0.6633130311965942 time take: 6.650645732879639\n",
      "epoch 31 time taken: 26.13818120956421s\n",
      "Training epoch: 32\n",
      "idx: 0, loss: 0.4336968660354614 time take: 0.26326704025268555\n",
      "idx: 100, loss: 0.3964022099971771 time take: 6.634995698928833\n",
      "idx: 200, loss: 0.45800626277923584 time take: 6.608518362045288\n",
      "idx: 300, loss: 0.4098410904407501 time take: 6.606983184814453\n",
      "epoch 32 time taken: 26.10630464553833s\n",
      "Training epoch: 33\n",
      "idx: 0, loss: 0.36197391152381897 time take: 0.2497847080230713\n",
      "idx: 100, loss: 0.3232019543647766 time take: 6.66391921043396\n",
      "idx: 200, loss: 0.4431607127189636 time take: 6.618249416351318\n",
      "idx: 300, loss: 0.4732680320739746 time take: 6.672632455825806\n",
      "epoch 33 time taken: 26.249237060546875s\n",
      "Training epoch: 34\n",
      "idx: 0, loss: 0.31318771839141846 time take: 0.26873302459716797\n",
      "idx: 100, loss: 0.27761131525039673 time take: 6.625268936157227\n",
      "idx: 200, loss: 0.5422395467758179 time take: 6.613906145095825\n",
      "idx: 300, loss: 0.38638731837272644 time take: 6.617193937301636\n",
      "epoch 34 time taken: 26.11953592300415s\n",
      "Training epoch: 35\n",
      "idx: 0, loss: 0.32140424847602844 time take: 0.23248982429504395\n",
      "idx: 100, loss: 0.30483922362327576 time take: 6.626675844192505\n",
      "idx: 200, loss: 0.49137088656425476 time take: 6.613256454467773\n",
      "idx: 300, loss: 0.35803571343421936 time take: 6.610865592956543\n",
      "epoch 35 time taken: 26.081209421157837s\n",
      "Training epoch: 36\n",
      "idx: 0, loss: 0.2759820818901062 time take: 0.26247549057006836\n",
      "idx: 100, loss: 0.24324342608451843 time take: 6.651221513748169\n",
      "idx: 200, loss: 0.4169517159461975 time take: 6.608415842056274\n",
      "idx: 300, loss: 0.37059277296066284 time take: 6.613798379898071\n",
      "epoch 36 time taken: 26.13021945953369s\n",
      "Training epoch: 37\n",
      "idx: 0, loss: 0.37880611419677734 time take: 0.2744314670562744\n",
      "idx: 100, loss: 0.3307240605354309 time take: 6.621201992034912\n",
      "idx: 200, loss: 0.359340101480484 time take: 6.611787557601929\n",
      "idx: 300, loss: 0.3602723479270935 time take: 6.637214422225952\n",
      "epoch 37 time taken: 26.215047121047974s\n",
      "Training epoch: 38\n",
      "idx: 0, loss: 0.28569304943084717 time take: 0.27256035804748535\n",
      "idx: 100, loss: 0.28871336579322815 time take: 6.650775909423828\n",
      "idx: 200, loss: 0.4436419904232025 time take: 6.648826360702515\n",
      "idx: 300, loss: 0.32444247603416443 time take: 6.636751890182495\n",
      "epoch 38 time taken: 26.225910902023315s\n",
      "Training epoch: 39\n",
      "idx: 0, loss: 0.24141839146614075 time take: 0.2574591636657715\n",
      "idx: 100, loss: 0.3883143365383148 time take: 6.622856616973877\n",
      "idx: 200, loss: 0.3862057328224182 time take: 6.6077306270599365\n",
      "idx: 300, loss: 0.359780877828598 time take: 6.6072704792022705\n",
      "epoch 39 time taken: 26.087728261947632s\n",
      "Training epoch: 40\n",
      "idx: 0, loss: 0.1912984848022461 time take: 0.2637498378753662\n",
      "idx: 100, loss: 0.20823855698108673 time take: 6.623753309249878\n",
      "idx: 200, loss: 0.3135538101196289 time take: 6.6162824630737305\n",
      "idx: 300, loss: 0.3158135712146759 time take: 6.653340816497803\n",
      "epoch 40 time taken: 26.158212661743164s\n",
      "Training epoch: 41\n",
      "idx: 0, loss: 0.3630092144012451 time take: 0.2598905563354492\n",
      "idx: 100, loss: 0.2508988678455353 time take: 6.6194586753845215\n",
      "idx: 200, loss: 0.3249742388725281 time take: 6.607109308242798\n",
      "idx: 300, loss: 0.413053035736084 time take: 6.6106626987457275\n",
      "epoch 41 time taken: 26.085686206817627s\n",
      "Training epoch: 42\n",
      "idx: 0, loss: 0.24785713851451874 time take: 0.25182437896728516\n",
      "idx: 100, loss: 0.13660800457000732 time take: 6.625157833099365\n",
      "idx: 200, loss: 0.3666681945323944 time take: 6.606451034545898\n",
      "idx: 300, loss: 0.3598521053791046 time take: 6.608428955078125\n",
      "epoch 42 time taken: 26.084033250808716s\n",
      "Training epoch: 43\n",
      "idx: 0, loss: 0.23511898517608643 time take: 0.42936277389526367\n",
      "idx: 100, loss: 0.26418638229370117 time take: 6.62583589553833\n",
      "idx: 200, loss: 0.3551698923110962 time take: 6.608806371688843\n",
      "idx: 300, loss: 0.31363415718078613 time take: 6.609165906906128\n",
      "epoch 43 time taken: 26.264458179473877s\n",
      "Training epoch: 44\n",
      "idx: 0, loss: 0.3564997613430023 time take: 0.23176288604736328\n",
      "idx: 100, loss: 0.2269679307937622 time take: 6.64518404006958\n",
      "idx: 200, loss: 0.3880167603492737 time take: 6.638404846191406\n",
      "idx: 300, loss: 0.27550384402275085 time take: 6.639610767364502\n",
      "epoch 44 time taken: 26.146491527557373s\n",
      "Training epoch: 45\n",
      "idx: 0, loss: 0.21232780814170837 time take: 0.24936652183532715\n",
      "idx: 100, loss: 0.20234642922878265 time take: 6.654910087585449\n",
      "idx: 200, loss: 0.29248273372650146 time take: 6.662158966064453\n",
      "idx: 300, loss: 0.2707562744617462 time take: 6.607322931289673\n",
      "epoch 45 time taken: 26.1632981300354s\n",
      "Training epoch: 46\n",
      "idx: 0, loss: 0.21046996116638184 time take: 0.2546367645263672\n",
      "idx: 100, loss: 0.3787213861942291 time take: 6.618643760681152\n",
      "idx: 200, loss: 0.28811582922935486 time take: 6.677774667739868\n",
      "idx: 300, loss: 0.21703654527664185 time take: 6.614034175872803\n",
      "epoch 46 time taken: 26.15479350090027s\n",
      "Training epoch: 47\n",
      "idx: 0, loss: 0.20188060402870178 time take: 0.23340177536010742\n",
      "idx: 100, loss: 0.2742571532726288 time take: 6.631397247314453\n",
      "idx: 200, loss: 0.4329962730407715 time take: 6.6983091831207275\n",
      "idx: 300, loss: 0.2840009927749634 time take: 6.645304441452026\n",
      "epoch 47 time taken: 26.199376344680786s\n",
      "Training epoch: 48\n",
      "idx: 0, loss: 0.19442838430404663 time take: 0.2674741744995117\n",
      "idx: 100, loss: 0.13501325249671936 time take: 6.615125417709351\n",
      "idx: 200, loss: 0.3012010455131531 time take: 6.606243848800659\n",
      "idx: 300, loss: 0.255255788564682 time take: 6.691736459732056\n",
      "epoch 48 time taken: 26.278090715408325s\n",
      "Training epoch: 49\n",
      "idx: 0, loss: 0.19813480973243713 time take: 0.23497295379638672\n",
      "idx: 100, loss: 0.20374195277690887 time take: 6.656087398529053\n",
      "idx: 200, loss: 0.3593931794166565 time take: 6.608716011047363\n",
      "idx: 300, loss: 0.25831812620162964 time take: 6.614066123962402\n",
      "epoch 49 time taken: 26.121044635772705s\n",
      "Performing inference on batch: 0\n",
      "Performing inference on batch: 50\n",
      "Performing inference on batch: 0\n",
      "Performing inference on batch: 50\n",
      "FPR@95: 76.86, AUROC: 80.02 AUPR_IN: 81.92, AUPR_OUT: 76.03\n",
      "CCR: 0.00, 1.65, 22.50, 50.67, ACC: 82.72\n",
      "──────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: {'dataset': 'cifar',\n",
       "  'FPR@95': '76.86',\n",
       "  'AUROC': '80.02',\n",
       "  'AUPR_IN': '81.92',\n",
       "  'AUPR_OUT': '76.03',\n",
       "  'CCR_4': '0.00',\n",
       "  'CCR_3': '1.65',\n",
       "  'CCR_2': '22.50',\n",
       "  'CCR_1': '50.67',\n",
       "  'ACC': '82.72',\n",
       "  'optimizer_type': 'Adam',\n",
       "  'activation_function_type': 'softplus',\n",
       "  'postprocessor_type': 'odin'}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_cifar_adam_softplus_odin = {\n",
    "    \"batch_size\": 128,\n",
    "    \"n_classes\": 10,\n",
    "    \"dataset_name\": \"cifar\",\n",
    "    \"epochs\": 100,\n",
    "    \"version\": time.time(),\n",
    "    \"lr\": 0.01,\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 0.0005,\n",
    "    \"optimizer_type\": \"Adam\",\n",
    "    \"activation_function_type\": \"softplus\",\n",
    "    \"network\": \"resnet50\",\n",
    "    \"postprocessor_type\": \"odin\",\n",
    "    \"trials\": 3,\n",
    "    \"dataset_type\": \"cifar\",\n",
    "    \"results_dir\": \"cifar10-study\",\n",
    "    \"pretrained\": False\n",
    "}\n",
    "config_cifar_adam_softplus_odin[\"data_loaders\"] = get_data_loaders(config_cifar_adam_softplus_odin)\n",
    "run_full_oodn_pipeline(config_cifar_adam_softplus_odin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
