{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "bhw8NPwVRbdT",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "\n",
    "import time\n",
    "import json\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "\n",
    "from torchvision.datasets import mnist, FashionMNIST, CIFAR10, CIFAR100\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.nn import Module\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torchvision.models.resnet import Bottleneck, ResNet\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "from wilds import get_dataset\n",
    "from wilds.common.data_loaders import get_train_loader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from openood.evaluators import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "11glohxERbdU",
    "outputId": "a412b2d2-d294-470d-b27a-abd3627aa087",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1tHAXNmwRbdU",
    "outputId": "ae15afd4-3c31-4e5c-d557-178f188de89c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WtgqLe2SajQb",
    "outputId": "3961662e-2a97-4bb0-a56c-11d9e69e79a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rdr2143/oodn-final-project/OpenOOD-nndl\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWs4cttKRbdU",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Supported Activation Functions\n",
    "\n",
    "For activation functions, we are considering ReLU, Softplus, Swish. *Note that we may conduct experiments for a subset based on the compute resources available*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "wFDrLIC4RbdV",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_activation_fn(activation):\n",
    "    if activation == 'relu':\n",
    "        return nn.ReLU()\n",
    "    elif activation == 'softplus':\n",
    "        return nn.Softplus()\n",
    "    elif activation == 'swish':\n",
    "        return nn.Swish()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ORnYhf-1RbdV",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7vRFe847RbdV",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-H6TJweYRbdV",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self, num_classes, num_channel=3, activation='relu'):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.feature_size = 84\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=num_channel,\n",
    "                      out_channels=6,\n",
    "                      kernel_size=5,\n",
    "                      stride=1,\n",
    "                      padding=2), get_activation_fn(activation), nn.MaxPool2d(kernel_size=2))\n",
    "\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1),\n",
    "             get_activation_fn(activation), nn.MaxPool2d(kernel_size=2))\n",
    "\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16,\n",
    "                      out_channels=120,\n",
    "                      kernel_size=5,\n",
    "                      stride=1), get_activation_fn(activation))\n",
    "\n",
    "        self.classifier1 = nn.Linear(in_features=120, out_features=84)\n",
    "        self.relu = get_activation_fn(activation)\n",
    "        self.fc = nn.Linear(in_features=84, out_features=num_classes)\n",
    "\n",
    "    def get_fc(self):\n",
    "        fc = self.fc\n",
    "        return fc.weight.cpu().detach().numpy(), fc.bias.cpu().detach().numpy()\n",
    "\n",
    "    def forward(self, x, return_feature=False, return_feature_list=False):\n",
    "        feature1 = self.block1(x)\n",
    "        feature2 = self.block2(feature1)\n",
    "        feature3 = self.block3(feature2)\n",
    "        feature3 = feature3.view(feature3.shape[0], -1)\n",
    "        feature = self.relu(self.classifier1(feature3))\n",
    "        logits_cls = self.fc(feature)\n",
    "        feature_list = [feature1, feature2, feature3, feature]\n",
    "        if return_feature:\n",
    "            return logits_cls, feature\n",
    "        elif return_feature_list:\n",
    "            return logits_cls, feature_list\n",
    "        else:\n",
    "            return logits_cls\n",
    "\n",
    "    def forward_threshold(self, x, threshold):\n",
    "        feature1 = self.block1(x)\n",
    "        feature2 = self.block2(feature1)\n",
    "        feature3 = self.block3(feature2)\n",
    "        feature3 = feature3.view(feature3.shape[0], -1)\n",
    "        feature = self.relu(self.classifier1(feature3))\n",
    "        feature = feature.clip(max=threshold)\n",
    "        logits_cls = self.fc(feature)\n",
    "\n",
    "        return logits_cls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XYwc4beHRbdV",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "4EaTD1nVRbdW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model):\n",
    "    for name,param in model.named_parameters():\n",
    "        if not (name.startswith('layer4') or name.startswith('fc')):\n",
    "            param.requires_grad = False\n",
    "\n",
    "def get_resnet_model(activation_function_type, n_classes, use_pretrained=True):\n",
    "    resnet_model = models.resnet50(pretrained=use_pretrained)\n",
    "    \n",
    "    # if we use pretrained, then freeze the corresponding layers\n",
    "    if use_pretrained:\n",
    "        set_parameter_requires_grad(resnet_model, feature_extract)\n",
    "\n",
    "    set_activation_function(resnet_model,activation_function_type)\n",
    "    num_ftrs = resnet_model.fc.in_features\n",
    "    resnet_model.fc = nn.Linear(num_ftrs, n_classes)\n",
    "    resnet_model.to(device)\n",
    "    return resnet_model\n",
    "\n",
    "def set_activation_function(resnet_model, activation_function_type):\n",
    "    resnet_model.relu = get_activation_fn(activation_function_type)\n",
    "    resnet_model.layer1[0].relu = get_activation_fn(activation_function_type)\n",
    "    resnet_model.layer1[1].relu = get_activation_fn(activation_function_type)\n",
    "    resnet_model.layer1[2].relu = get_activation_fn(activation_function_type)\n",
    "\n",
    "    resnet_model.layer2[0].relu = get_activation_fn(activation_function_type)\n",
    "    resnet_model.layer2[1].relu = get_activation_fn(activation_function_type)\n",
    "    resnet_model.layer2[2].relu = get_activation_fn(activation_function_type)\n",
    "    resnet_model.layer2[3].relu = get_activation_fn(activation_function_type)\n",
    "\n",
    "    resnet_model.layer3[0].relu = get_activation_fn(activation_function_type)\n",
    "    resnet_model.layer3[1].relu = get_activation_fn(activation_function_type)\n",
    "    resnet_model.layer3[2].relu = get_activation_fn(activation_function_type)\n",
    "    resnet_model.layer3[3].relu = get_activation_fn(activation_function_type)\n",
    "    resnet_model.layer3[4].relu = get_activation_fn(activation_function_type)\n",
    "    resnet_model.layer3[5].relu = get_activation_fn(activation_function_type)\n",
    "\n",
    "\n",
    "    resnet_model.layer4[0].relu = get_activation_fn(activation_function_type)\n",
    "    resnet_model.layer4[1].relu = get_activation_fn(activation_function_type)\n",
    "    resnet_model.layer4[2].relu = get_activation_fn(activation_function_type)\n",
    "\n",
    "    return resnet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Mm1fB4piRbdW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_model(config):\n",
    "    activation_function_type = config[\"activation_function_type\"]\n",
    "    network_type = config[\"network\"]\n",
    "    n_classes = config[\"n_classes\"]\n",
    "\n",
    "    if network_type == \"lenet\":\n",
    "        model =  LeNet(num_classes=n_classes, num_channel=1, activation=activation_function_type)\n",
    "    elif network_type == \"resnet50\":\n",
    "        model = get_resnet_model(activation_function_type, n_classes, config['pretrained'])\n",
    "    else:\n",
    "        raise Exception(\"Currently we only support lenet or resnet50\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9veLq_LDRbdW",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKbri3J-RbdW",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Supported Post-Hoc OODN Processors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dPvVbPU4RbdW",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### The first post processor we consider is ODIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Am-I4gXqRbdW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ODINPostprocessor():\n",
    "    def __init__(self, temperature, noise):\n",
    "        self.temperature = temperature\n",
    "        self.noise = noise\n",
    "\n",
    "    def postprocess(self, net: nn.Module, data):\n",
    "        net.eval()\n",
    "        data.requires_grad = True\n",
    "        output = net(data)\n",
    "\n",
    "        # Calculating the perturbation we need to add, that is,\n",
    "        # the sign of gradient of cross entropy loss w.r.t. input\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        labels = output.detach().argmax(axis=1)\n",
    "\n",
    "        # Using temperature scaling\n",
    "        output = output / self.temperature\n",
    "\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Normalizing the gradient to binary in {0, 1}\n",
    "        gradient = torch.ge(data.grad.detach(), 0)\n",
    "        gradient = (gradient.float() - 0.5) * 2\n",
    "\n",
    "        # Scaling values taken from original code\n",
    "        gradient[:, 0] = (gradient[:, 0]) / (63.0 / 255.0)\n",
    "        if gradient.shape[1] == 3:\n",
    "            gradient[:, 1] = (gradient[:, 1]) / (62.1 / 255.0)\n",
    "            gradient[:, 2] = (gradient[:, 2]) / (66.7 / 255.0)\n",
    "\n",
    "        # Adding small perturbations to images\n",
    "        tempInputs = torch.add(data.detach(), gradient, alpha=-self.noise)\n",
    "        output = net(tempInputs)\n",
    "        output = output / self.temperature\n",
    "\n",
    "        # Calculating the confidence after adding perturbations\n",
    "        nnOutput = output.detach()\n",
    "        nnOutput = nnOutput - nnOutput.max(dim=1, keepdims=True).values\n",
    "        nnOutput = nnOutput.exp() / nnOutput.exp().sum(dim=1, keepdims=True)\n",
    "\n",
    "        conf, pred = nnOutput.max(dim=1)\n",
    "\n",
    "        return pred, conf\n",
    "\n",
    "    def inference(self, net: nn.Module, data_loader: DataLoader):\n",
    "        pred_list, conf_list, label_list = [], [], []\n",
    "        for idx, loaded_data in enumerate(data_loader):\n",
    "            data, label = loaded_data[0], loaded_data[1]\n",
    "            if idx % 50 == 0:\n",
    "                print(f'Performing inference on batch: {idx}')\n",
    "            pred, conf = self.postprocess(net, data.to(device))\n",
    "            for idx in range(len(data)):\n",
    "                pred_list.append(pred[idx].tolist())\n",
    "                conf_list.append(conf[idx].tolist())\n",
    "                label_list.append(label[idx].tolist())\n",
    "\n",
    "        # convert values into numpy array\n",
    "        pred_list = np.array(pred_list, dtype=int)\n",
    "        conf_list = np.array(conf_list)\n",
    "        label_list = np.array(label_list, dtype=int)\n",
    "\n",
    "        return pred_list, conf_list, label_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kz32VFq9RbdX",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### We consider the Maximum Classifier Discrepancy Post OODN method\n",
    "\n",
    "https://arxiv.org/pdf/1712.02560.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "7L45tl2cRbdX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MCDPostprocessor():\n",
    "    @torch.no_grad()\n",
    "    def postprocess(self, net: nn.Module, data):\n",
    "        logits1, logits2 = net(data)\n",
    "        score1 = torch.softmax(logits1, dim=1)\n",
    "        score2 = torch.softmax(logits2, dim=1)\n",
    "        conf = -torch.sum(torch.abs(score1 - score2), dim=1)\n",
    "        _, pred = torch.max(score1, dim=1)\n",
    "        return pred, conf\n",
    "\n",
    "    def inference(self, net: nn.Module, data_loader: DataLoader):\n",
    "        pred_list, conf_list, label_list = [], [], []\n",
    "        for idx, loaded_data in enumerate(data_loader):\n",
    "            data, label = loaded_data[0], loaded_data[1]\n",
    "            if idx % 50 == 0:\n",
    "                print(f'Performing inference on batch: {idx}')\n",
    "            pred, conf = self.postprocess(net, data.to(device))\n",
    "            for idx in range(len(data)):\n",
    "                pred_list.append(pred[idx].tolist())\n",
    "                conf_list.append(conf[idx].tolist())\n",
    "                label_list.append(label[idx].tolist())\n",
    "\n",
    "        # convert values into numpy array\n",
    "        pred_list = np.array(pred_list, dtype=int)\n",
    "        conf_list = np.array(conf_list)\n",
    "        label_list = np.array(label_list, dtype=int)\n",
    "\n",
    "        return pred_list, conf_list, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "0NHU_CaCRbdX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_postprocessor(postprocessor_type=\"odin\"):\n",
    "    if postprocessor_type == \"odin\":\n",
    "        postprocessor = ODINPostprocessor(1000, 0.0014)\n",
    "    elif postprocessor_type == \"mcd\":\n",
    "        postprocessor = MCDPostprocessor()\n",
    "    return postprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZfeGklQRbdX",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bGtatMpzRbdX",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Supported Out of Distribution Detection Metrics\n",
    "\n",
    "What metrics do we specifically care about here?\n",
    "\n",
    "**FPR@95** measures the false positive rate (FPR) when the true positive rate (TPR) is\n",
    "equal to 95%. Lower scores indicate better performance.\n",
    "\n",
    "**AUROC** measures the area under the\n",
    "Receiver Operating Characteristic (ROC) curve, which displays the relationship between TPR and\n",
    "FPR. The area under the ROC curve can be interpreted as the probability that a positive ID example\n",
    "will have a higher detection score than a negative OOD example.\n",
    "\n",
    "**AUPR** measures the area under\n",
    "the Precision-Recall (PR) curve. The PR curve is created by plotting precision versus recall. Similar\n",
    "to AUROC, we consider ID samples as positive, so that the score corresponds to the AUPR-In metric\n",
    "in some works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "Xm89cB-XRbdX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_oodn_metrics(model, postprocessor_type, id_test_loader, ood_test_loader, ood_name):\n",
    "    postprocessor = get_postprocessor(postprocessor_type)\n",
    "    id_pred, id_conf, id_gt = postprocessor.inference(\n",
    "                model, id_test_loader)\n",
    "\n",
    "    ood_pred, ood_conf, ood_gt = postprocessor.inference(\n",
    "        model, ood_test_loader)\n",
    "\n",
    "    ood_gt = -1 * np.ones_like(ood_gt)  # hard set to -1 as ood\n",
    "    pred = np.concatenate([id_pred, ood_pred])\n",
    "    conf = np.concatenate([id_conf, ood_conf])\n",
    "    label = np.concatenate([id_gt, ood_gt])\n",
    "    ood_metrics = metrics.compute_all_metrics(conf, label, pred)\n",
    "\n",
    "    return print_and_get_formatted_metrics(ood_metrics, ood_name)\n",
    "\n",
    "def print_and_get_formatted_metrics(metrics, dataset_name):\n",
    "    [fpr, auroc, aupr_in, aupr_out,\n",
    "     ccr_4, ccr_3, ccr_2, ccr_1, accuracy] \\\n",
    "     = metrics\n",
    "\n",
    "    write_content = {\n",
    "        'dataset': dataset_name,\n",
    "        'FPR@95': '{:.2f}'.format(100 * fpr),\n",
    "        'AUROC': '{:.2f}'.format(100 * auroc),\n",
    "        'AUPR_IN': '{:.2f}'.format(100 * aupr_in),\n",
    "        'AUPR_OUT': '{:.2f}'.format(100 * aupr_out),\n",
    "        'CCR_4': '{:.2f}'.format(100 * ccr_4),\n",
    "        'CCR_3': '{:.2f}'.format(100 * ccr_3),\n",
    "        'CCR_2': '{:.2f}'.format(100 * ccr_2),\n",
    "        'CCR_1': '{:.2f}'.format(100 * ccr_1),\n",
    "        'ACC': '{:.2f}'.format(100 * accuracy)\n",
    "    }\n",
    "\n",
    "    fieldnames = list(write_content.keys())\n",
    "\n",
    "    # print ood metric results\n",
    "    print('FPR@95: {:.2f}, AUROC: {:.2f}'.format(100 * fpr, 100 * auroc),\n",
    "          end=' ',\n",
    "          flush=True)\n",
    "    print('AUPR_IN: {:.2f}, AUPR_OUT: {:.2f}'.format(\n",
    "        100 * aupr_in, 100 * aupr_out),\n",
    "          flush=True)\n",
    "    print('CCR: {:.2f}, {:.2f}, {:.2f}, {:.2f},'.format(\n",
    "        ccr_4 * 100, ccr_3 * 100, ccr_2 * 100, ccr_1 * 100),\n",
    "          end=' ',\n",
    "          flush=True)\n",
    "    print('ACC: {:.2f}'.format(accuracy * 100), flush=True)\n",
    "    print(u'\\u2500' * 70, flush=True)\n",
    "    return write_content\n",
    "\n",
    "def load_results_into_df(dir_path):\n",
    "    res_files = [dir_path+each for each in listdir(dir_path)]\n",
    "    all_results = []\n",
    "    columns = ['optimizer_type', 'activation_function_type', 'postprocessor_type', 'trial', 'AUROC']\n",
    "    for fp in res_files:\n",
    "        f = open(fp)\n",
    "        data = json.load(f)\n",
    "        for trial, results in data.items():\n",
    "            all_results.append([\n",
    "                    results['optimizer_type'],\n",
    "                    results['activation_function_type'],\n",
    "                    results['postprocessor_type'],\n",
    "                    trial,\n",
    "                    float(results['AUROC'])\n",
    "                ])\n",
    "    df = pd.DataFrame(all_results, columns=columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "aEJLZ75aRbdX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_optimizer(model, config):\n",
    "    params = model.parameters()\n",
    "    lr = config['lr']\n",
    "    momentum = config['momentum']\n",
    "    weight_decay = config['weight_decay']\n",
    "    optimizer_type = config['optimizer_type']\n",
    "\n",
    "    print(f'Getting optimizer for type: {optimizer_type}...')\n",
    "    if optimizer_type == 'SGD':\n",
    "        return SGD(params,\n",
    "              lr=lr,\n",
    "              momentum=momentum,\n",
    "              weight_decay=weight_decay)\n",
    "    elif optimizer_type == 'Adam':\n",
    "        return Adam(params,\n",
    "                    lr=lr,\n",
    "                    weight_decay=weight_decay)\n",
    "    else:\n",
    "        raise Exception(\"Invalid optimizer_type provided, only SGD and Adam are supported currently\")\n",
    "\n",
    "def get_wilds_loader(dataset, split, batch_size):\n",
    "    d = dataset.get_subset(\n",
    "        split,\n",
    "        # frac=0.1,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.Resize((448, 448)), transforms.ToTensor()]\n",
    "        ),\n",
    "    )\n",
    "    # Prepare the standard data loader\n",
    "    return get_train_loader(\"standard\", d, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "def get_data_loaders(config):\n",
    "    data_loaders = {}\n",
    "    dataset_name = config[\"dataset_name\"]\n",
    "    dataset_type = config[\"dataset_type\"]\n",
    "    batch_size = config['batch_size']\n",
    "\n",
    "    wilds_id_test_split = \"id_val\" if dataset_name == \"camelyon17\" else \"id_test\"\n",
    "    if dataset_type == \"wilds\":\n",
    "        # wilds dataset\n",
    "        dataset = get_dataset(dataset=dataset_name, download=True)\n",
    "        data_loaders[\"train\"] = get_wilds_loader(dataset, \"train\", batch_size)\n",
    "        data_loaders[\"ood_test\"] = get_wilds_loader(dataset, \"test\", batch_size)\n",
    "        data_loaders[\"id_test\"] = get_wilds_loader(dataset, wilds_id_test_split, batch_size)\n",
    "        return\n",
    "    elif dataset_name == \"cifar\":\n",
    "        train_dataset = CIFAR10(root='data', download=True, train=True, transform=ToTensor())\n",
    "        test_dataset = CIFAR10(root='data', download=True, train=False, transform=ToTensor())\n",
    "        ood_test_dataset = CIFAR100(root='data', download=True, train=False, transform=ToTensor())\n",
    "    elif dataset_name == \"mnist\":\n",
    "        # mnist dataset\n",
    "        train_dataset = mnist.MNIST(root='data', download=True, train=True, transform=ToTensor())\n",
    "        test_dataset = mnist.MNIST(root='data', download=True, train=False, transform=ToTensor())\n",
    "        ood_test_dataset = mnist.FashionMNIST(root='data', download=True,train=False,transform=ToTensor())\n",
    "\n",
    "    data_loaders[\"train\"] = DataLoader(train_dataset, batch_size=batch_size)\n",
    "    data_loaders[\"id_test\"] = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    data_loaders[\"ood_test\"] = DataLoader(ood_test_dataset, batch_size=batch_size)\n",
    "\n",
    "    return data_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "JfRfN2fNRbdX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_resnet_model_given_opti_activation_fn(config):\n",
    "    # get the train loader\n",
    "    train_loader = config[\"data_loaders\"][\"train\"]\n",
    "\n",
    "    # get the resnet model with the replaced activation functions\n",
    "    model = get_model(config)\n",
    "    model.to(device)\n",
    "\n",
    "    # get the optimizer\n",
    "    sgd = get_optimizer(model, config)\n",
    "\n",
    "    loss_fn = CrossEntropyLoss()\n",
    "    for current_epoch in range(config['epochs']):\n",
    "        tic=time.time()\n",
    "        per_batch_time = time.time()\n",
    "        model.train()\n",
    "        print('Training epoch: {}'.format(current_epoch))\n",
    "        for idx, (loader_data) in enumerate(train_loader):\n",
    "            train_x, train_label = loader_data[0].to(device), loader_data[1].to(device)\n",
    "            sgd.zero_grad()\n",
    "            predict_y = model(train_x.float())\n",
    "            loss = loss_fn(predict_y, train_label.long())\n",
    "            if idx % 100 == 0:\n",
    "                print('idx: {}, loss: {} time take: {}'.format(idx, loss.sum().item(), time.time() - per_batch_time))\n",
    "                per_batch_time = time.time()\n",
    "            loss.backward()\n",
    "            sgd.step()\n",
    "        print(f\"epoch {current_epoch} time taken: {time.time()-tic}s\")\n",
    "    torch.save(model, config['model_name'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def run_full_oodn_pipeline(config):\n",
    "    metrics = {}\n",
    "    for i in range(config[\"trials\"]):\n",
    "        model_name = f\"models/{config['dataset_name']}_{config['network']}_{config['postprocessor_type']}_{config['activation_function_type']}_{config['optimizer_type']}_{i}.pkl\"\n",
    "        print(f'Running model: {model_name}...')\n",
    "        config['model_name'] = model_name\n",
    "        # train model\n",
    "        model = train_resnet_model_given_opti_activation_fn(config)\n",
    "        # calculate oodn metrics\n",
    "        metrics[i] = calculate_oodn_metrics(model,\n",
    "                               config['postprocessor_type'],\n",
    "                               config[\"data_loaders\"][\"id_test\"],\n",
    "                               config[\"data_loaders\"][\"ood_test\"],\n",
    "                               config[\"dataset_name\"])\n",
    "        metrics[i]['optimizer_type'] = config['optimizer_type']\n",
    "        metrics[i]['activation_function_type'] = config['activation_function_type']\n",
    "        metrics[i]['postprocessor_type'] = config['postprocessor_type']\n",
    "\n",
    "    experiment_name = f\"{config['results_dir']}/{config['dataset_name']}_{config['network']}_{config['postprocessor_type']}_{config['activation_function_type']}_{config['optimizer_type']}.json\"\n",
    "    with open(experiment_name, 'w') as fp:\n",
    "        json.dump(metrics, fp)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study 2: Resnet, CIFAR-10 (ID), CIFAR-100 (OOD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Study 2(a): Adam + ReLU + Odin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lzf8wgoTRbdY",
    "outputId": "b0d274c3-4075-4581-97dc-e3fce18f24d2",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Running model: models/cifar_resnet50_odin_relu_Adam_0.pkl...\n",
      "Getting optimizer for type: Adam...\n",
      "Training epoch: 0\n",
      "idx: 0, loss: 2.5875422954559326 time take: 0.01837635040283203\n",
      "idx: 100, loss: 3.111764430999756 time take: 3.7511818408966064\n",
      "idx: 200, loss: 3.209524393081665 time take: 3.7211434841156006\n",
      "idx: 300, loss: 2.1741185188293457 time take: 3.725675582885742\n",
      "idx: 400, loss: 2.156985282897949 time take: 3.892035961151123\n",
      "idx: 500, loss: 2.388465404510498 time take: 3.7181692123413086\n",
      "idx: 600, loss: 2.1290876865386963 time take: 3.73374342918396\n",
      "idx: 700, loss: 1.9732838869094849 time take: 3.7244277000427246\n",
      "idx: 800, loss: 2.1930158138275146 time take: 3.714919090270996\n",
      "idx: 900, loss: 1.832750678062439 time take: 3.7245051860809326\n",
      "idx: 1000, loss: 1.8897126913070679 time take: 3.7116236686706543\n",
      "idx: 1100, loss: 1.7903989553451538 time take: 3.730496406555176\n",
      "idx: 1200, loss: 1.947293996810913 time take: 3.728771209716797\n",
      "idx: 1300, loss: 1.7544749975204468 time take: 3.7316830158233643\n",
      "idx: 1400, loss: 1.502386450767517 time take: 3.7152047157287598\n",
      "idx: 1500, loss: 1.9216094017028809 time take: 3.720102548599243\n",
      "epoch 0 time taken: 58.39823532104492s\n",
      "Training epoch: 1\n",
      "idx: 0, loss: 1.982182502746582 time take: 0.0175936222076416\n",
      "idx: 100, loss: 2.0029311180114746 time take: 3.7147324085235596\n",
      "idx: 200, loss: 1.4159878492355347 time take: 3.7125132083892822\n",
      "idx: 300, loss: 1.6404569149017334 time take: 3.7023239135742188\n",
      "idx: 400, loss: 1.3842138051986694 time take: 3.7294795513153076\n",
      "idx: 500, loss: 1.4689322710037231 time take: 3.9173624515533447\n",
      "idx: 600, loss: 1.8147729635238647 time take: 3.7271993160247803\n",
      "idx: 700, loss: 1.6331827640533447 time take: 3.730930805206299\n",
      "idx: 800, loss: 1.7923675775527954 time take: 3.7214343547821045\n",
      "idx: 900, loss: 1.4056519269943237 time take: 3.7154667377471924\n",
      "idx: 1000, loss: 1.7147505283355713 time take: 3.7127890586853027\n",
      "idx: 1100, loss: 1.3702397346496582 time take: 3.72717022895813\n",
      "idx: 1200, loss: 1.5707545280456543 time take: 3.731839418411255\n",
      "idx: 1300, loss: 1.5081900358200073 time take: 3.7299511432647705\n",
      "idx: 1400, loss: 1.5638480186462402 time take: 3.721773624420166\n",
      "idx: 1500, loss: 1.958122730255127 time take: 3.723195791244507\n",
      "epoch 1 time taken: 58.363483905792236s\n",
      "Training epoch: 2\n",
      "idx: 0, loss: 1.501501202583313 time take: 0.017467975616455078\n",
      "idx: 100, loss: 1.6845183372497559 time take: 3.726450204849243\n",
      "idx: 200, loss: 1.3071482181549072 time take: 3.7240045070648193\n",
      "idx: 300, loss: 1.4165735244750977 time take: 3.729574203491211\n",
      "idx: 400, loss: 1.3171072006225586 time take: 3.726442813873291\n",
      "idx: 500, loss: 1.4461886882781982 time take: 3.8883321285247803\n",
      "idx: 600, loss: 1.7421534061431885 time take: 3.726553201675415\n",
      "idx: 700, loss: 1.6739387512207031 time take: 3.721926212310791\n",
      "idx: 800, loss: 1.4832611083984375 time take: 3.7241005897521973\n",
      "idx: 900, loss: 1.1852807998657227 time take: 3.7265191078186035\n",
      "idx: 1000, loss: 1.4707005023956299 time take: 3.7258832454681396\n",
      "idx: 1100, loss: 1.4354010820388794 time take: 3.7590057849884033\n",
      "idx: 1200, loss: 1.445478916168213 time take: 3.7514758110046387\n",
      "idx: 1300, loss: 1.431923747062683 time take: 3.748047113418579\n",
      "idx: 1400, loss: 1.3380303382873535 time take: 3.7247414588928223\n",
      "idx: 1500, loss: 1.807088851928711 time take: 3.7258307933807373\n",
      "epoch 2 time taken: 58.475539684295654s\n",
      "Training epoch: 3\n",
      "idx: 0, loss: 1.606971263885498 time take: 0.016849756240844727\n",
      "idx: 100, loss: 1.327863097190857 time take: 3.742264747619629\n",
      "idx: 200, loss: 1.225184440612793 time take: 3.729489803314209\n",
      "idx: 300, loss: 1.408616542816162 time take: 3.7303247451782227\n",
      "idx: 400, loss: 1.232917308807373 time take: 3.731844425201416\n",
      "idx: 500, loss: 1.4872002601623535 time take: 3.7490718364715576\n",
      "idx: 600, loss: 1.7226214408874512 time take: 3.8878111839294434\n",
      "idx: 700, loss: 1.5826387405395508 time take: 3.724921703338623\n",
      "idx: 800, loss: 1.5569177865982056 time take: 3.7283215522766113\n",
      "idx: 900, loss: 1.3180781602859497 time take: 3.727351188659668\n",
      "idx: 1000, loss: 1.457486629486084 time take: 3.7257235050201416\n",
      "idx: 1100, loss: 1.3951541185379028 time take: 3.7249510288238525\n",
      "idx: 1200, loss: 1.3156428337097168 time take: 3.729658603668213\n",
      "idx: 1300, loss: 1.2798949480056763 time take: 3.743572235107422\n",
      "idx: 1400, loss: 1.4987760782241821 time take: 3.7364726066589355\n",
      "idx: 1500, loss: 1.8177974224090576 time take: 3.7288308143615723\n",
      "epoch 3 time taken: 58.48653793334961s\n",
      "Training epoch: 4\n",
      "idx: 0, loss: 1.4835033416748047 time take: 0.017018556594848633\n",
      "idx: 100, loss: 1.311998724937439 time take: 3.7294790744781494\n",
      "idx: 200, loss: 1.1479661464691162 time take: 3.7268314361572266\n",
      "idx: 300, loss: 1.4469692707061768 time take: 3.732714891433716\n",
      "idx: 400, loss: 1.1405551433563232 time take: 3.7433676719665527\n",
      "idx: 500, loss: 1.5561786890029907 time take: 3.74137544631958\n",
      "idx: 600, loss: 1.5762861967086792 time take: 3.9029226303100586\n",
      "idx: 700, loss: 1.5506982803344727 time take: 3.725278615951538\n",
      "idx: 800, loss: 1.4993181228637695 time take: 3.7351319789886475\n",
      "idx: 900, loss: 1.0610289573669434 time take: 3.72462797164917\n",
      "idx: 1000, loss: 1.514145851135254 time take: 3.7327489852905273\n",
      "idx: 1100, loss: 1.3830764293670654 time take: 3.728358745574951\n",
      "idx: 1200, loss: 1.250449299812317 time take: 3.7261853218078613\n",
      "idx: 1300, loss: 1.3086971044540405 time take: 3.7235803604125977\n",
      "idx: 1400, loss: 1.2804625034332275 time take: 3.7337493896484375\n",
      "idx: 1500, loss: 1.6634318828582764 time take: 3.741684913635254\n",
      "epoch 4 time taken: 58.492899894714355s\n",
      "Training epoch: 5\n",
      "idx: 0, loss: 1.465691328048706 time take: 0.017179250717163086\n",
      "idx: 100, loss: 1.3786015510559082 time take: 3.7355377674102783\n",
      "idx: 200, loss: 1.0183736085891724 time take: 3.7234208583831787\n",
      "idx: 300, loss: 1.3469607830047607 time take: 3.7307889461517334\n",
      "idx: 400, loss: 1.1149992942810059 time take: 3.7254977226257324\n",
      "idx: 500, loss: 1.6176658868789673 time take: 3.7233166694641113\n",
      "idx: 600, loss: 1.4909560680389404 time take: 3.834836006164551\n",
      "idx: 700, loss: 1.5841556787490845 time take: 3.803467273712158\n",
      "idx: 800, loss: 1.352108120918274 time take: 3.7310354709625244\n",
      "idx: 900, loss: 1.1310116052627563 time take: 3.719012498855591\n",
      "idx: 1000, loss: 1.52490234375 time take: 3.7323925495147705\n",
      "idx: 1100, loss: 1.3132902383804321 time take: 3.72731876373291\n",
      "idx: 1200, loss: 1.1690526008605957 time take: 3.733069658279419\n",
      "idx: 1300, loss: 1.2849736213684082 time take: 3.732562780380249\n",
      "idx: 1400, loss: 1.2169620990753174 time take: 3.7376105785369873\n",
      "idx: 1500, loss: 1.687239170074463 time take: 3.7483558654785156\n",
      "epoch 5 time taken: 58.498664140701294s\n",
      "Training epoch: 6\n",
      "idx: 0, loss: 1.3652384281158447 time take: 0.01736283302307129\n",
      "idx: 100, loss: 1.4407612085342407 time take: 3.7328457832336426\n",
      "idx: 200, loss: 1.1419897079467773 time take: 3.731741428375244\n",
      "idx: 300, loss: 1.3829375505447388 time take: 3.730083703994751\n",
      "idx: 400, loss: 1.0343923568725586 time take: 3.7299673557281494\n",
      "idx: 500, loss: 1.6062651872634888 time take: 3.7272214889526367\n",
      "idx: 600, loss: 1.553651213645935 time take: 3.733095169067383\n",
      "idx: 700, loss: 1.5204365253448486 time take: 3.903808355331421\n",
      "idx: 800, loss: 1.4397075176239014 time take: 3.7413899898529053\n",
      "idx: 900, loss: 1.1080454587936401 time take: 3.733726978302002\n",
      "idx: 1000, loss: 1.6014792919158936 time take: 3.732574462890625\n",
      "idx: 1100, loss: 1.4004353284835815 time take: 3.7369272708892822\n",
      "idx: 1200, loss: 1.356921672821045 time take: 3.740906238555908\n",
      "idx: 1300, loss: 1.361776351928711 time take: 3.7320730686187744\n",
      "idx: 1400, loss: 1.1933361291885376 time take: 3.7344300746917725\n",
      "idx: 1500, loss: 1.799135684967041 time take: 3.7532453536987305\n",
      "epoch 6 time taken: 58.5586953163147s\n",
      "Training epoch: 7\n",
      "idx: 0, loss: 1.3506805896759033 time take: 0.017270803451538086\n",
      "idx: 100, loss: 1.549128770828247 time take: 3.7458462715148926\n",
      "idx: 200, loss: 1.2114651203155518 time take: 3.7397398948669434\n",
      "idx: 300, loss: 1.161165714263916 time take: 3.7437734603881836\n",
      "idx: 400, loss: 1.0428193807601929 time take: 3.7382404804229736\n",
      "idx: 500, loss: 1.4414843320846558 time take: 3.738631248474121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx: 600, loss: 1.637003779411316 time take: 3.733394145965576\n",
      "idx: 700, loss: 1.4566165208816528 time take: 3.900977373123169\n",
      "idx: 800, loss: 1.4047192335128784 time take: 3.7377631664276123\n",
      "idx: 900, loss: 1.082077980041504 time take: 3.738532304763794\n",
      "idx: 1000, loss: 1.5268466472625732 time take: 3.7313077449798584\n",
      "idx: 1100, loss: 1.2761977910995483 time take: 3.735593795776367\n",
      "idx: 1200, loss: 1.3910152912139893 time take: 3.7310738563537598\n",
      "idx: 1300, loss: 1.1709553003311157 time take: 3.7300710678100586\n",
      "idx: 1400, loss: 1.2751092910766602 time take: 3.7431952953338623\n",
      "idx: 1500, loss: 1.8988319635391235 time take: 3.743520498275757\n",
      "epoch 7 time taken: 58.58352541923523s\n",
      "Training epoch: 8\n",
      "idx: 0, loss: 1.3147186040878296 time take: 0.01716136932373047\n",
      "idx: 100, loss: 1.4423773288726807 time take: 3.7416269779205322\n",
      "idx: 200, loss: 1.1315058469772339 time take: 3.751114845275879\n",
      "idx: 300, loss: 1.2588536739349365 time take: 3.737154722213745\n",
      "idx: 400, loss: 1.0202786922454834 time take: 3.7360143661499023\n",
      "idx: 500, loss: 1.5190794467926025 time take: 3.7395334243774414\n",
      "idx: 600, loss: 1.5721298456192017 time take: 3.7558016777038574\n",
      "idx: 700, loss: 1.4625762701034546 time take: 3.754662275314331\n",
      "idx: 800, loss: 1.4393693208694458 time take: 3.9185431003570557\n",
      "idx: 900, loss: 1.1428515911102295 time take: 3.748783826828003\n",
      "idx: 1000, loss: 1.5252052545547485 time take: 3.756676197052002\n",
      "idx: 1100, loss: 1.2054606676101685 time take: 3.7555506229400635\n",
      "idx: 1200, loss: 1.3991384506225586 time take: 3.7400081157684326\n",
      "idx: 1300, loss: 1.2256739139556885 time take: 3.7356362342834473\n",
      "idx: 1400, loss: 1.1956887245178223 time take: 3.739267349243164\n",
      "idx: 1500, loss: 1.910762906074524 time take: 3.7335920333862305\n",
      "epoch 8 time taken: 58.68987822532654s\n",
      "Training epoch: 9\n",
      "idx: 0, loss: 1.329209566116333 time take: 0.017544984817504883\n",
      "idx: 100, loss: 1.4858330488204956 time take: 3.7353017330169678\n",
      "idx: 200, loss: 1.1543898582458496 time take: 3.741494655609131\n",
      "idx: 300, loss: 1.2335095405578613 time take: 3.7489736080169678\n",
      "idx: 400, loss: 1.1342586278915405 time take: 3.7309980392456055\n",
      "idx: 500, loss: 1.6700036525726318 time take: 3.7338342666625977\n",
      "idx: 600, loss: 1.5225927829742432 time take: 3.7348241806030273\n",
      "idx: 700, loss: 1.3678481578826904 time take: 3.7337849140167236\n",
      "idx: 800, loss: 1.3954219818115234 time take: 3.8901865482330322\n",
      "idx: 900, loss: 1.1921758651733398 time take: 3.7270853519439697\n"
     ]
    }
   ],
   "source": [
    "config_cifar_adam_relu_odin = {\n",
    "    \"batch_size\": 32,\n",
    "    \"n_classes\": 10,\n",
    "    \"dataset_name\": \"cifar\",\n",
    "    \"epochs\": 50,\n",
    "    \"version\": time.time(),\n",
    "    \"lr\": 0.01,\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 0.0005,\n",
    "    \"optimizer_type\": \"Adam\",\n",
    "    \"activation_function_type\": \"relu\",\n",
    "    \"network\": \"resnet50\",\n",
    "    \"postprocessor_type\": \"odin\",\n",
    "    \"trials\": 1,\n",
    "    \"dataset_type\": \"cifar\",\n",
    "    \"results_dir\": \"cifar10-study\",\n",
    "    \"pretrained\": False\n",
    "}\n",
    "config_cifar_adam_relu_odin[\"data_loaders\"] = get_data_loaders(config_cifar_adam_relu_odin)\n",
    "run_full_oodn_pipeline(config_cifar_adam_relu_odin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Study 2 (b.) Adam + Softplus + odin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_cifar_adam_softplus_odin = {\n",
    "    \"batch_size\": 32,\n",
    "    \"n_classes\": 10,\n",
    "    \"dataset_name\": \"cifar\",\n",
    "    \"epochs\": 3,\n",
    "    \"version\": time.time(),\n",
    "    \"lr\": 0.01,\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 0.0005,\n",
    "    \"optimizer_type\": \"Adam\",\n",
    "    \"activation_function_type\": \"softplus\",\n",
    "    \"network\": \"resnet50\",\n",
    "    \"postprocessor_type\": \"odin\",\n",
    "    \"trials\": 1,\n",
    "    \"dataset_type\": \"cifar\",\n",
    "    \"results_dir\": \"cifar10-study\",\n",
    "    \"pretrained\": False\n",
    "}\n",
    "config_cifar_adam_softplus_odin[\"data_loaders\"] = get_data_loaders(config_cifar_adam_softplus_odin)\n",
    "run_full_oodn_pipeline(config_cifar_adam_softplus_odin)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
