{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bhw8NPwVRbdT",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "\n",
    "import time\n",
    "import json\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "\n",
    "from torchvision.datasets import mnist, FashionMNIST, CIFAR10, CIFAR100\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.nn import Module\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torchvision.models.resnet import Bottleneck, ResNet\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "from wilds import get_dataset\n",
    "from wilds.common.data_loaders import get_train_loader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from openood.evaluators import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "11glohxERbdU",
    "outputId": "a412b2d2-d294-470d-b27a-abd3627aa087",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.2\n"
     ]
    }
   ],
   "source": [
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1tHAXNmwRbdU",
    "outputId": "ae15afd4-3c31-4e5c-d557-178f188de89c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WtgqLe2SajQb",
    "outputId": "3961662e-2a97-4bb0-a56c-11d9e69e79a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rdr2143/oodn-final-project/OpenOOD-nndl\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWs4cttKRbdU",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Supported Activation Functions\n",
    "\n",
    "For activation functions, we are considering ReLU, Softplus, Swish. *Note that we may conduct experiments for a subset based on the compute resources available*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "wFDrLIC4RbdV",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_activation_fn(activation):\n",
    "    if activation == 'relu':\n",
    "        return nn.ReLU()\n",
    "    elif activation == 'softplus':\n",
    "        return nn.Softplus()\n",
    "    elif activation == 'swish':\n",
    "        return nn.Swish()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ORnYhf-1RbdV",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7vRFe847RbdV",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-H6TJweYRbdV",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self, num_classes, num_channel=3, activation='relu'):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.feature_size = 84\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=num_channel,\n",
    "                      out_channels=6,\n",
    "                      kernel_size=5,\n",
    "                      stride=1,\n",
    "                      padding=2), get_activation_fn(activation), nn.MaxPool2d(kernel_size=2))\n",
    "\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1),\n",
    "             get_activation_fn(activation), nn.MaxPool2d(kernel_size=2))\n",
    "\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16,\n",
    "                      out_channels=120,\n",
    "                      kernel_size=5,\n",
    "                      stride=1), get_activation_fn(activation))\n",
    "\n",
    "        self.classifier1 = nn.Linear(in_features=120, out_features=84)\n",
    "        self.relu = get_activation_fn(activation)\n",
    "        self.fc = nn.Linear(in_features=84, out_features=num_classes)\n",
    "\n",
    "    def get_fc(self):\n",
    "        fc = self.fc\n",
    "        return fc.weight.cpu().detach().numpy(), fc.bias.cpu().detach().numpy()\n",
    "\n",
    "    def forward(self, x, return_feature=False, return_feature_list=False):\n",
    "        feature1 = self.block1(x)\n",
    "        feature2 = self.block2(feature1)\n",
    "        feature3 = self.block3(feature2)\n",
    "        feature3 = feature3.view(feature3.shape[0], -1)\n",
    "        feature = self.relu(self.classifier1(feature3))\n",
    "        logits_cls = self.fc(feature)\n",
    "        feature_list = [feature1, feature2, feature3, feature]\n",
    "        if return_feature:\n",
    "            return logits_cls, feature\n",
    "        elif return_feature_list:\n",
    "            return logits_cls, feature_list\n",
    "        else:\n",
    "            return logits_cls\n",
    "\n",
    "    def forward_threshold(self, x, threshold):\n",
    "        feature1 = self.block1(x)\n",
    "        feature2 = self.block2(feature1)\n",
    "        feature3 = self.block3(feature2)\n",
    "        feature3 = feature3.view(feature3.shape[0], -1)\n",
    "        feature = self.relu(self.classifier1(feature3))\n",
    "        feature = feature.clip(max=threshold)\n",
    "        logits_cls = self.fc(feature)\n",
    "\n",
    "        return logits_cls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XYwc4beHRbdV",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "4EaTD1nVRbdW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model):\n",
    "    for name,param in model.named_parameters():\n",
    "        if not (name.startswith('layer4') or name.startswith('fc')):\n",
    "            param.requires_grad = False\n",
    "\n",
    "def get_resnet_model(activation_function_type, n_classes, use_pretrained=True):\n",
    "    resnet_model = models.resnet50(pretrained=use_pretrained)\n",
    "    \n",
    "    # if we use pretrained, then freeze the corresponding layers\n",
    "    if use_pretrained:\n",
    "        set_parameter_requires_grad(resnet_model, feature_extract)\n",
    "\n",
    "    set_activation_function(resnet_model,activation_function_type)\n",
    "    num_ftrs = resnet_model.fc.in_features\n",
    "    resnet_model.fc = nn.Linear(num_ftrs, n_classes)\n",
    "    resnet_model.to(device)\n",
    "    return resnet_model\n",
    "\n",
    "def set_activation_function(resnet_model, activation_function_type):\n",
    "    resnet_model.relu = get_activation_fn(activation_function_type)\n",
    "    resnet_model.layer1[0].relu = get_activation_fn(activation_function_type)\n",
    "    resnet_model.layer1[1].relu = get_activation_fn(activation_function_type)\n",
    "    resnet_model.layer1[2].relu = get_activation_fn(activation_function_type)\n",
    "\n",
    "    resnet_model.layer2[0].relu = get_activation_fn(activation_function_type)\n",
    "    resnet_model.layer2[1].relu = get_activation_fn(activation_function_type)\n",
    "    resnet_model.layer2[2].relu = get_activation_fn(activation_function_type)\n",
    "    resnet_model.layer2[3].relu = get_activation_fn(activation_function_type)\n",
    "\n",
    "    resnet_model.layer3[0].relu = get_activation_fn(activation_function_type)\n",
    "    resnet_model.layer3[1].relu = get_activation_fn(activation_function_type)\n",
    "    resnet_model.layer3[2].relu = get_activation_fn(activation_function_type)\n",
    "    resnet_model.layer3[3].relu = get_activation_fn(activation_function_type)\n",
    "    resnet_model.layer3[4].relu = get_activation_fn(activation_function_type)\n",
    "    resnet_model.layer3[5].relu = get_activation_fn(activation_function_type)\n",
    "\n",
    "\n",
    "    resnet_model.layer4[0].relu = get_activation_fn(activation_function_type)\n",
    "    resnet_model.layer4[1].relu = get_activation_fn(activation_function_type)\n",
    "    resnet_model.layer4[2].relu = get_activation_fn(activation_function_type)\n",
    "\n",
    "    return resnet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Mm1fB4piRbdW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_model(config):\n",
    "    activation_function_type = config[\"activation_function_type\"]\n",
    "    network_type = config[\"network\"]\n",
    "    n_classes = config[\"n_classes\"]\n",
    "\n",
    "    if network_type == \"lenet\":\n",
    "        model =  LeNet(num_classes=n_classes, num_channel=1, activation=activation_function_type)\n",
    "    elif network_type == \"resnet50\":\n",
    "        model = get_resnet_model(activation_function_type, n_classes, config['pretrained'])\n",
    "    else:\n",
    "        raise Exception(\"Currently we only support lenet or resnet50\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9veLq_LDRbdW",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKbri3J-RbdW",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Supported Post-Hoc OODN Processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OODPostprocessor():\n",
    "    \n",
    "    def inference(self, net: nn.Module, data_loader: DataLoader):\n",
    "        pred_list, conf_list, label_list = [], [], []\n",
    "        for idx, loaded_data in enumerate(data_loader):\n",
    "            data, label = loaded_data[0], loaded_data[1]\n",
    "            if idx % 50 == 0:\n",
    "                print(f'Performing inference on batch: {idx}')\n",
    "            pred, conf = self.postprocess(net, data.to(device))\n",
    "            for idx in range(len(data)):\n",
    "                pred_list.append(pred[idx].tolist())\n",
    "                conf_list.append(conf[idx].tolist())\n",
    "                label_list.append(label[idx].tolist())\n",
    "\n",
    "        # convert values into numpy array\n",
    "        pred_list = np.array(pred_list, dtype=int)\n",
    "        conf_list = np.array(conf_list)\n",
    "        label_list = np.array(label_list, dtype=int)\n",
    "\n",
    "        return pred_list, conf_list, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODINPostprocessor(OODPostprocessor):\n",
    "    def __init__(self, temperature, noise):\n",
    "        super(OODPostprocessor)\n",
    "        self.temperature = temperature\n",
    "        self.noise = noise\n",
    "        \n",
    "    def postprocess(self, net: nn.Module, data):\n",
    "        net.eval()\n",
    "        data.requires_grad = True\n",
    "        output = net(data)\n",
    "\n",
    "        # Calculating the perturbation we need to add, that is,\n",
    "        # the sign of gradient of cross entropy loss w.r.t. input\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        labels = output.detach().argmax(axis=1)\n",
    "\n",
    "        # Using temperature scaling\n",
    "        output = output / self.temperature\n",
    "\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Normalizing the gradient to binary in {0, 1}\n",
    "        gradient = torch.ge(data.grad.detach(), 0)\n",
    "        gradient = (gradient.float() - 0.5) * 2\n",
    "\n",
    "        # Scaling values taken from original code       \n",
    "        gradient[:, 0] = (gradient[:, 0]) / (63.0 / 255.0)\n",
    "        if gradient.shape[1] == 3:\n",
    "            gradient[:, 1] = (gradient[:, 1]) / (62.1 / 255.0)\n",
    "            gradient[:, 2] = (gradient[:, 2]) / (66.7 / 255.0)\n",
    "\n",
    "        # Adding small perturbations to images\n",
    "        tempInputs = torch.add(data.detach(), gradient, alpha=-self.noise)\n",
    "        output = net(tempInputs)\n",
    "        output = output / self.temperature\n",
    "\n",
    "        # Calculating the confidence after adding perturbations\n",
    "        nnOutput = output.detach()\n",
    "        nnOutput = nnOutput - nnOutput.max(dim=1, keepdims=True).values\n",
    "        nnOutput = nnOutput.exp() / nnOutput.exp().sum(dim=1, keepdims=True)\n",
    "\n",
    "        conf, pred = nnOutput.max(dim=1)\n",
    "\n",
    "        return pred, conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCDPostprocessor(OODPostprocessor):\n",
    "    def __init__(self, samples: int = 30):\n",
    "        super(OODPostprocessor)\n",
    "        self.samples = samples  #: number :math:`N` of samples\n",
    "\n",
    "    def postprocess(self, model: torch.nn.Module, x: torch.Tensor) -> torch.Tensor:\n",
    "        mode_switch = False\n",
    "        if not model.training:\n",
    "            mode_switch = True\n",
    "\n",
    "            model.train()\n",
    "\n",
    "            for mod in model.modules():\n",
    "                # reset batch norm layers.\n",
    "                # TODO: are there other layers?\n",
    "                if isinstance(mod, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):\n",
    "                    mod.train(False)\n",
    "\n",
    "        results = None\n",
    "        with torch.no_grad():\n",
    "            for i in range(self.samples):\n",
    "                output = model(x).softmax(dim=1)\n",
    "                if results is None:\n",
    "                    results = torch.zeros(size=output.shape).to(device)\n",
    "                results += output\n",
    "        results /= self.samples\n",
    "\n",
    "        if mode_switch:\n",
    "            model.eval()\n",
    "        \n",
    "        conf, pred = results.max(dim=1)\n",
    "\n",
    "        return pred, conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_postprocessor(postprocessor_type=\"odin\"):\n",
    "    if postprocessor_type == \"odin\":\n",
    "        postprocessor = ODINPostprocessor(1000, 0.0014)\n",
    "    elif postprocessor_type == \"mcd\":\n",
    "        postprocessor = MCDPostprocessor(30)\n",
    "    return postprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bGtatMpzRbdX",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Supported Out of Distribution Detection Metrics\n",
    "\n",
    "What metrics do we specifically care about here?\n",
    "\n",
    "**FPR@95** measures the false positive rate (FPR) when the true positive rate (TPR) is\n",
    "equal to 95%. Lower scores indicate better performance.\n",
    "\n",
    "**AUROC** measures the area under the\n",
    "Receiver Operating Characteristic (ROC) curve, which displays the relationship between TPR and\n",
    "FPR. The area under the ROC curve can be interpreted as the probability that a positive ID example\n",
    "will have a higher detection score than a negative OOD example.\n",
    "\n",
    "**AUPR** measures the area under\n",
    "the Precision-Recall (PR) curve. The PR curve is created by plotting precision versus recall. Similar\n",
    "to AUROC, we consider ID samples as positive, so that the score corresponds to the AUPR-In metric\n",
    "in some works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Xm89cB-XRbdX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_oodn_metrics(model, postprocessor_type, id_test_loader, ood_test_loader, ood_name):\n",
    "    postprocessor = get_postprocessor(postprocessor_type)\n",
    "    id_pred, id_conf, id_gt = postprocessor.inference(\n",
    "                model, id_test_loader)\n",
    "\n",
    "    ood_pred, ood_conf, ood_gt = postprocessor.inference(\n",
    "        model, ood_test_loader)\n",
    "\n",
    "    ood_gt = -1 * np.ones_like(ood_gt)  # hard set to -1 as ood\n",
    "    pred = np.concatenate([id_pred, ood_pred])\n",
    "    conf = np.concatenate([id_conf, ood_conf])\n",
    "    label = np.concatenate([id_gt, ood_gt])\n",
    "    ood_metrics = metrics.compute_all_metrics(conf, label, pred)\n",
    "\n",
    "    return print_and_get_formatted_metrics(ood_metrics, ood_name)\n",
    "\n",
    "def print_and_get_formatted_metrics(metrics, dataset_name):\n",
    "    [fpr, auroc, aupr_in, aupr_out,\n",
    "     ccr_4, ccr_3, ccr_2, ccr_1, accuracy] \\\n",
    "     = metrics\n",
    "\n",
    "    write_content = {\n",
    "        'dataset': dataset_name,\n",
    "        'FPR@95': '{:.2f}'.format(100 * fpr),\n",
    "        'AUROC': '{:.2f}'.format(100 * auroc),\n",
    "        'AUPR_IN': '{:.2f}'.format(100 * aupr_in),\n",
    "        'AUPR_OUT': '{:.2f}'.format(100 * aupr_out),\n",
    "        'CCR_4': '{:.2f}'.format(100 * ccr_4),\n",
    "        'CCR_3': '{:.2f}'.format(100 * ccr_3),\n",
    "        'CCR_2': '{:.2f}'.format(100 * ccr_2),\n",
    "        'CCR_1': '{:.2f}'.format(100 * ccr_1),\n",
    "        'ACC': '{:.2f}'.format(100 * accuracy)\n",
    "    }\n",
    "\n",
    "    fieldnames = list(write_content.keys())\n",
    "\n",
    "    # print ood metric results\n",
    "    print('FPR@95: {:.2f}, AUROC: {:.2f}'.format(100 * fpr, 100 * auroc),\n",
    "          end=' ',\n",
    "          flush=True)\n",
    "    print('AUPR_IN: {:.2f}, AUPR_OUT: {:.2f}'.format(\n",
    "        100 * aupr_in, 100 * aupr_out),\n",
    "          flush=True)\n",
    "    print('CCR: {:.2f}, {:.2f}, {:.2f}, {:.2f},'.format(\n",
    "        ccr_4 * 100, ccr_3 * 100, ccr_2 * 100, ccr_1 * 100),\n",
    "          end=' ',\n",
    "          flush=True)\n",
    "    print('ACC: {:.2f}'.format(accuracy * 100), flush=True)\n",
    "    print(u'\\u2500' * 70, flush=True)\n",
    "    return write_content\n",
    "\n",
    "def load_results_into_df(dir_path):\n",
    "    res_files = [dir_path+each for each in listdir(dir_path)]\n",
    "    all_results = []\n",
    "    columns = ['optimizer_type', 'activation_function_type', 'postprocessor_type', 'trial', 'AUROC']\n",
    "    for fp in res_files:\n",
    "        f = open(fp)\n",
    "        data = json.load(f)\n",
    "        for trial, results in data.items():\n",
    "            all_results.append([\n",
    "                    results['optimizer_type'],\n",
    "                    results['activation_function_type'],\n",
    "                    results['postprocessor_type'],\n",
    "                    trial,\n",
    "                    float(results['AUROC'])\n",
    "                ])\n",
    "    df = pd.DataFrame(all_results, columns=columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "aEJLZ75aRbdX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_optimizer(model, config):\n",
    "    params = model.parameters()\n",
    "    lr = config['lr']\n",
    "    momentum = config['momentum']\n",
    "    weight_decay = config['weight_decay']\n",
    "    optimizer_type = config['optimizer_type']\n",
    "\n",
    "    print(f'Getting optimizer for type: {optimizer_type}...')\n",
    "    if optimizer_type == 'SGD':\n",
    "        return SGD(params,\n",
    "              lr=lr,\n",
    "              momentum=momentum,\n",
    "              weight_decay=weight_decay)\n",
    "    elif optimizer_type == 'Adam':\n",
    "        return Adam(params,\n",
    "                    lr=lr,\n",
    "                    weight_decay=weight_decay)\n",
    "    else:\n",
    "        raise Exception(\"Invalid optimizer_type provided, only SGD and Adam are supported currently\")\n",
    "\n",
    "def get_wilds_loader(dataset, split, batch_size):\n",
    "    d = dataset.get_subset(\n",
    "        split,\n",
    "        # frac=0.1,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.Resize((448, 448)), transforms.ToTensor()]\n",
    "        ),\n",
    "    )\n",
    "    # Prepare the standard data loader\n",
    "    return get_train_loader(\"standard\", d, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "def get_data_loaders(config):\n",
    "    data_loaders = {}\n",
    "    dataset_name = config[\"dataset_name\"]\n",
    "    dataset_type = config[\"dataset_type\"]\n",
    "    batch_size = config['batch_size']\n",
    "\n",
    "    wilds_id_test_split = \"id_val\" if dataset_name == \"camelyon17\" else \"id_test\"\n",
    "    if dataset_type == \"wilds\":\n",
    "        # wilds dataset\n",
    "        dataset = get_dataset(dataset=dataset_name, download=True)\n",
    "        data_loaders[\"train\"] = get_wilds_loader(dataset, \"train\", batch_size)\n",
    "        data_loaders[\"ood_test\"] = get_wilds_loader(dataset, \"test\", batch_size)\n",
    "        data_loaders[\"id_test\"] = get_wilds_loader(dataset, wilds_id_test_split, batch_size)\n",
    "        return\n",
    "    elif dataset_name == \"cifar\":\n",
    "        train_dataset = CIFAR10(root='data', download=True, train=True, transform=ToTensor())\n",
    "        test_dataset = CIFAR10(root='data', download=True, train=False, transform=ToTensor())\n",
    "        ood_test_dataset = CIFAR100(root='data', download=True, train=False, transform=ToTensor())\n",
    "    elif dataset_name == \"mnist\":\n",
    "        # mnist dataset\n",
    "        train_dataset = mnist.MNIST(root='data', download=True, train=True, transform=ToTensor())\n",
    "        test_dataset = mnist.MNIST(root='data', download=True, train=False, transform=ToTensor())\n",
    "        ood_test_dataset = mnist.FashionMNIST(root='data', download=True,train=False,transform=ToTensor())\n",
    "\n",
    "    data_loaders[\"train\"] = DataLoader(train_dataset, batch_size=batch_size)\n",
    "    data_loaders[\"id_test\"] = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    data_loaders[\"ood_test\"] = DataLoader(ood_test_dataset, batch_size=batch_size)\n",
    "\n",
    "    return data_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "JfRfN2fNRbdX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_resnet_model_given_opti_activation_fn(config):\n",
    "    # get the train loader\n",
    "    train_loader = config[\"data_loaders\"][\"train\"]\n",
    "\n",
    "    # get the resnet model with the replaced activation functions\n",
    "    model = get_model(config)\n",
    "    model.to(device)\n",
    "\n",
    "    # get the optimizer\n",
    "    sgd = get_optimizer(model, config)\n",
    "\n",
    "    loss_fn = CrossEntropyLoss()\n",
    "    for current_epoch in range(config['epochs']):\n",
    "        tic=time.time()\n",
    "        per_batch_time = time.time()\n",
    "        model.train()\n",
    "        print('Training epoch: {}'.format(current_epoch))\n",
    "        for idx, (loader_data) in enumerate(train_loader):\n",
    "            train_x, train_label = loader_data[0].to(device), loader_data[1].to(device)\n",
    "            sgd.zero_grad()\n",
    "            predict_y = model(train_x.float())\n",
    "            loss = loss_fn(predict_y, train_label.long())\n",
    "            if idx % 100 == 0:\n",
    "                print('idx: {}, loss: {} time take: {}'.format(idx, loss.sum().item(), time.time() - per_batch_time))\n",
    "                per_batch_time = time.time()\n",
    "            loss.backward()\n",
    "            sgd.step()\n",
    "        print(f\"epoch {current_epoch} time taken: {time.time()-tic}s\")\n",
    "    torch.save(model, config['model_name'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def run_full_oodn_pipeline(config):\n",
    "    metrics = {}\n",
    "    for i in range(config[\"trials\"]):\n",
    "        model_name = f\"models/{config['dataset_name']}_{config['network']}_{config['postprocessor_type']}_{config['activation_function_type']}_{config['optimizer_type']}_{i}.pkl\"\n",
    "        print(f'Running model: {model_name}...')\n",
    "        config['model_name'] = model_name\n",
    "        # train model\n",
    "        model = train_resnet_model_given_opti_activation_fn(config)\n",
    "        # calculate oodn metrics\n",
    "        metrics[i] = calculate_oodn_metrics(model,\n",
    "                               config['postprocessor_type'],\n",
    "                               config[\"data_loaders\"][\"id_test\"],\n",
    "                               config[\"data_loaders\"][\"ood_test\"],\n",
    "                               config[\"dataset_name\"])\n",
    "        metrics[i]['optimizer_type'] = config['optimizer_type']\n",
    "        metrics[i]['activation_function_type'] = config['activation_function_type']\n",
    "        metrics[i]['postprocessor_type'] = config['postprocessor_type']\n",
    "\n",
    "    experiment_name = f\"{config['results_dir']}/{config['dataset_name']}_{config['network']}_{config['postprocessor_type']}_{config['activation_function_type']}_{config['optimizer_type']}.json\"\n",
    "    with open(experiment_name, 'w') as fp:\n",
    "        json.dump(metrics, fp)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study 2: Resnet, CIFAR-10 (ID), CIFAR-100 (OOD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Study 2(a): Adam + ReLU + Odin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lzf8wgoTRbdY",
    "outputId": "b0d274c3-4075-4581-97dc-e3fce18f24d2",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config_cifar_adam_relu_odin = {\n",
    "    \"batch_size\": 32,\n",
    "    \"n_classes\": 10,\n",
    "    \"dataset_name\": \"cifar\",\n",
    "    \"epochs\": 100,\n",
    "    \"version\": time.time(),\n",
    "    \"lr\": 0.1,\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 0.0005,\n",
    "    \"optimizer_type\": \"Adam\",\n",
    "    \"activation_function_type\": \"relu\",\n",
    "    \"network\": \"resnet50\",\n",
    "    \"postprocessor_type\": \"odin\",\n",
    "    \"trials\": 3,\n",
    "    \"dataset_type\": \"cifar\",\n",
    "    \"results_dir\": \"cifar10-study\",\n",
    "    \"pretrained\": False\n",
    "}\n",
    "config_cifar_adam_relu_odin[\"data_loaders\"] = get_data_loaders(config_cifar_adam_relu_odin)\n",
    "run_full_oodn_pipeline(config_cifar_adam_relu_odin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Study 2 (b.) Adam + Softplus + odin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Running model: models/cifar_resnet50_odin_softplus_Adam_0.pkl...\n",
      "Getting optimizer for type: Adam...\n",
      "Training epoch: 0\n",
      "idx: 0, loss: 2.5051374435424805 time take: 0.021992206573486328\n",
      "idx: 100, loss: 4.812678813934326 time take: 4.541609525680542\n",
      "idx: 200, loss: 3.900937080383301 time take: 5.454892873764038\n",
      "idx: 300, loss: 4.439716339111328 time take: 4.475287914276123\n",
      "idx: 400, loss: 2.912879705429077 time take: 4.5268707275390625\n",
      "idx: 500, loss: 2.6974780559539795 time take: 4.474822521209717\n",
      "idx: 600, loss: 3.4684531688690186 time take: 4.510331869125366\n",
      "idx: 700, loss: 2.992316246032715 time take: 4.531476259231567\n",
      "idx: 800, loss: 2.17337703704834 time take: 4.534104108810425\n",
      "idx: 900, loss: 2.2893619537353516 time take: 4.51169490814209\n",
      "idx: 1000, loss: 2.312516689300537 time take: 4.580081462860107\n",
      "idx: 1100, loss: 2.6384294033050537 time take: 4.535020351409912\n",
      "idx: 1200, loss: 1.9389495849609375 time take: 4.563130855560303\n",
      "idx: 1300, loss: 2.421428680419922 time take: 4.5182554721832275\n",
      "idx: 1400, loss: 2.141387462615967 time take: 4.6260223388671875\n",
      "idx: 1500, loss: 2.874006986618042 time take: 5.42875075340271\n",
      "epoch 0 time taken: 72.65275287628174s\n",
      "Training epoch: 1\n",
      "idx: 0, loss: 2.7627530097961426 time take: 0.0211331844329834\n",
      "idx: 100, loss: 2.1934690475463867 time take: 4.453796148300171\n",
      "idx: 200, loss: 2.331641435623169 time take: 4.526191234588623\n",
      "idx: 300, loss: 3.008915901184082 time take: 4.528118371963501\n",
      "idx: 400, loss: 2.0831339359283447 time take: 4.4619081020355225\n",
      "idx: 500, loss: 2.6096742153167725 time take: 4.525943040847778\n",
      "idx: 600, loss: 2.666293144226074 time take: 4.528318643569946\n",
      "idx: 700, loss: 3.16438627243042 time take: 4.622676610946655\n",
      "idx: 800, loss: 2.1094393730163574 time take: 4.544381141662598\n",
      "idx: 900, loss: 2.349752426147461 time take: 4.534487009048462\n",
      "idx: 1000, loss: 2.272826671600342 time take: 4.5649449825286865\n",
      "idx: 1100, loss: 2.5928993225097656 time take: 4.559516668319702\n",
      "idx: 1200, loss: 2.504871368408203 time take: 5.454233407974243\n",
      "idx: 1300, loss: 2.298064947128296 time take: 4.570152759552002\n",
      "idx: 1400, loss: 2.459913730621338 time take: 4.451876878738403\n",
      "idx: 1500, loss: 2.6422646045684814 time take: 4.520028352737427\n",
      "epoch 1 time taken: 71.65394878387451s\n",
      "Training epoch: 2\n",
      "idx: 0, loss: 2.2424709796905518 time take: 0.020164012908935547\n",
      "idx: 100, loss: 2.783845901489258 time take: 4.479581117630005\n",
      "idx: 200, loss: 1.9741449356079102 time take: 4.54889988899231\n",
      "idx: 300, loss: 2.9089267253875732 time take: 4.4939539432525635\n",
      "idx: 400, loss: 2.0798470973968506 time take: 4.536684513092041\n",
      "idx: 500, loss: 2.2533583641052246 time take: 4.693512916564941\n",
      "idx: 600, loss: 3.0238959789276123 time take: 4.526664972305298\n",
      "idx: 700, loss: 2.9568638801574707 time take: 4.581829071044922\n",
      "idx: 800, loss: 2.1019985675811768 time take: 4.571687936782837\n",
      "idx: 900, loss: 1.9158879518508911 time take: 4.335540533065796\n",
      "idx: 1000, loss: 3.0655481815338135 time take: 4.523226022720337\n",
      "idx: 1100, loss: 2.087733745574951 time take: 4.562527894973755\n",
      "idx: 1200, loss: 2.484830141067505 time take: 4.5482048988342285\n",
      "idx: 1300, loss: 2.751163959503174 time take: 4.541868209838867\n",
      "idx: 1400, loss: 1.9336594343185425 time take: 4.557410955429077\n",
      "idx: 1500, loss: 2.2357900142669678 time take: 4.538105010986328\n",
      "epoch 2 time taken: 70.83861565589905s\n",
      "Training epoch: 3\n",
      "idx: 0, loss: 2.627253293991089 time take: 0.021663665771484375\n",
      "idx: 100, loss: 2.0599513053894043 time take: 4.4722208976745605\n",
      "idx: 200, loss: 2.0058462619781494 time take: 4.620641708374023\n",
      "idx: 300, loss: 2.748288154602051 time take: 4.51023530960083\n",
      "idx: 400, loss: 1.7977285385131836 time take: 4.556018114089966\n",
      "idx: 500, loss: 2.2732653617858887 time take: 4.461364984512329\n",
      "idx: 600, loss: 2.8362417221069336 time take: 4.262193918228149\n",
      "idx: 700, loss: 3.8548407554626465 time take: 4.422154426574707\n",
      "idx: 800, loss: 2.241771936416626 time take: 4.557605028152466\n",
      "idx: 900, loss: 1.370100975036621 time take: 4.554421424865723\n",
      "idx: 1000, loss: 2.857090473175049 time take: 4.570370197296143\n",
      "idx: 1100, loss: 2.049969434738159 time take: 4.5281805992126465\n",
      "idx: 1200, loss: 1.8191279172897339 time take: 4.556250333786011\n",
      "idx: 1300, loss: 2.7877094745635986 time take: 4.567207336425781\n",
      "idx: 1400, loss: 1.8678158521652222 time take: 4.563762903213501\n",
      "idx: 1500, loss: 2.8026750087738037 time take: 4.646617412567139\n",
      "epoch 3 time taken: 70.7610170841217s\n",
      "Training epoch: 4\n",
      "idx: 0, loss: 2.2819571495056152 time take: 0.020925283432006836\n",
      "idx: 100, loss: 2.2965731620788574 time take: 4.5748631954193115\n",
      "idx: 200, loss: 2.197460412979126 time take: 4.565808057785034\n",
      "idx: 300, loss: 3.342435121536255 time take: 4.332759618759155\n",
      "idx: 400, loss: 2.414050817489624 time take: 4.5780322551727295\n",
      "idx: 500, loss: 2.1442880630493164 time take: 4.581547975540161\n",
      "idx: 600, loss: 2.5621700286865234 time take: 4.568480968475342\n",
      "idx: 700, loss: 3.005222797393799 time take: 4.561633586883545\n",
      "idx: 800, loss: 1.6358797550201416 time take: 4.530090570449829\n",
      "idx: 900, loss: 1.510671615600586 time take: 4.548740386962891\n",
      "idx: 1000, loss: 2.779064416885376 time take: 4.56097936630249\n",
      "idx: 1100, loss: 1.9015458822250366 time take: 4.5219361782073975\n",
      "idx: 1200, loss: 1.6402925252914429 time take: 4.570160627365112\n",
      "idx: 1300, loss: 2.8126933574676514 time take: 4.724393606185913\n",
      "idx: 1400, loss: 2.01686692237854 time take: 4.5939249992370605\n",
      "idx: 1500, loss: 2.604886054992676 time take: 5.399880647659302\n",
      "epoch 4 time taken: 72.32255363464355s\n",
      "Training epoch: 5\n",
      "idx: 0, loss: 2.728125810623169 time take: 0.02201104164123535\n",
      "idx: 100, loss: 2.8695969581604004 time take: 4.579242467880249\n",
      "idx: 200, loss: 1.8152246475219727 time take: 4.547378301620483\n",
      "idx: 300, loss: 3.4215781688690186 time take: 4.554858684539795\n",
      "idx: 400, loss: 2.321531057357788 time take: 4.551355600357056\n",
      "idx: 500, loss: 1.972841501235962 time take: 4.5757155418396\n",
      "idx: 600, loss: 2.29538631439209 time take: 4.4556567668914795\n",
      "idx: 700, loss: 2.92168927192688 time take: 4.414000988006592\n",
      "idx: 800, loss: 2.1701319217681885 time take: 4.566551685333252\n",
      "idx: 900, loss: 2.232478380203247 time take: 4.56411075592041\n",
      "idx: 1000, loss: 2.300123929977417 time take: 4.648187875747681\n",
      "idx: 1100, loss: 1.9546120166778564 time take: 4.4378743171691895\n",
      "idx: 1200, loss: 1.64017915725708 time take: 4.961445331573486\n",
      "idx: 1300, loss: 2.557715654373169 time take: 5.021184206008911\n",
      "idx: 1400, loss: 2.216139316558838 time take: 4.561705827713013\n",
      "idx: 1500, loss: 2.942528009414673 time take: 4.585415840148926\n",
      "epoch 5 time taken: 71.91158103942871s\n",
      "Training epoch: 6\n",
      "idx: 0, loss: 3.0456244945526123 time take: 0.021552085876464844\n",
      "idx: 100, loss: 2.437931537628174 time take: 4.5726189613342285\n",
      "idx: 200, loss: 1.8276996612548828 time take: 4.520824193954468\n",
      "idx: 300, loss: 3.7448558807373047 time take: 4.510819435119629\n",
      "idx: 400, loss: 1.7224003076553345 time take: 4.583418130874634\n",
      "idx: 500, loss: 2.1481385231018066 time take: 4.543190956115723\n",
      "idx: 600, loss: 2.732841968536377 time take: 4.5475029945373535\n",
      "idx: 700, loss: 3.2037429809570312 time take: 4.528565406799316\n",
      "idx: 800, loss: 1.3905820846557617 time take: 4.587865591049194\n",
      "idx: 900, loss: 1.9310096502304077 time take: 4.883507251739502\n",
      "idx: 1000, loss: 2.525144577026367 time take: 4.782081604003906\n",
      "idx: 1100, loss: 2.0954980850219727 time take: 3.6866061687469482\n",
      "idx: 1200, loss: 2.3716838359832764 time take: 3.6610467433929443\n",
      "idx: 1300, loss: 2.386615514755249 time take: 3.6718266010284424\n",
      "idx: 1400, loss: 2.0494203567504883 time take: 3.666099786758423\n",
      "idx: 1500, loss: 2.9754629135131836 time take: 3.6621828079223633\n",
      "epoch 6 time taken: 66.72274899482727s\n",
      "Training epoch: 7\n",
      "idx: 0, loss: 2.7202084064483643 time take: 0.01757526397705078\n",
      "idx: 100, loss: 2.878091335296631 time take: 3.658754825592041\n",
      "idx: 200, loss: 2.128901720046997 time take: 3.652738571166992\n",
      "idx: 300, loss: 3.2206833362579346 time take: 3.6488254070281982\n",
      "idx: 400, loss: 1.7806339263916016 time take: 3.6586296558380127\n",
      "idx: 500, loss: 3.061450958251953 time take: 3.6596012115478516\n",
      "idx: 600, loss: 3.0089924335479736 time take: 3.6490418910980225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx: 700, loss: 2.9740405082702637 time take: 3.65785551071167\n",
      "idx: 800, loss: 1.9935061931610107 time take: 3.7709641456604004\n",
      "idx: 900, loss: 1.677092432975769 time take: 3.650024652481079\n",
      "idx: 1000, loss: 2.703641891479492 time take: 3.6572048664093018\n",
      "idx: 1100, loss: 1.9307852983474731 time take: 3.6521189212799072\n",
      "idx: 1200, loss: 1.9030886888504028 time take: 3.6499905586242676\n",
      "idx: 1300, loss: 2.421614170074463 time take: 3.654635429382324\n",
      "idx: 1400, loss: 1.9173314571380615 time take: 3.655120372772217\n",
      "idx: 1500, loss: 2.624476432800293 time take: 3.644597291946411\n",
      "epoch 7 time taken: 57.22394371032715s\n",
      "Training epoch: 8\n",
      "idx: 0, loss: 2.6190903186798096 time take: 0.017633676528930664\n",
      "idx: 100, loss: 2.801884889602661 time take: 3.6499006748199463\n",
      "idx: 200, loss: 2.37754225730896 time take: 3.677839756011963\n",
      "idx: 300, loss: 2.0216219425201416 time take: 3.702834129333496\n",
      "idx: 400, loss: 1.9060289859771729 time take: 3.6549112796783447\n",
      "idx: 500, loss: 2.0855607986450195 time take: 3.6623592376708984\n",
      "idx: 600, loss: 2.616997241973877 time take: 3.6673684120178223\n",
      "idx: 700, loss: 2.675197124481201 time take: 3.6755919456481934\n",
      "idx: 800, loss: 1.4033186435699463 time take: 3.797548770904541\n",
      "idx: 900, loss: 1.8928142786026 time take: 3.6526710987091064\n",
      "idx: 1000, loss: 2.688342571258545 time take: 3.6506006717681885\n",
      "idx: 1100, loss: 1.9879406690597534 time take: 3.6554551124572754\n",
      "idx: 1200, loss: 2.088244915008545 time take: 3.6460466384887695\n",
      "idx: 1300, loss: 2.0208730697631836 time take: 3.65250825881958\n",
      "idx: 1400, loss: 2.1539697647094727 time take: 3.648808240890503\n",
      "idx: 1500, loss: 2.08876633644104 time take: 3.6535093784332275\n",
      "epoch 8 time taken: 57.36310338973999s\n",
      "Training epoch: 9\n",
      "idx: 0, loss: 2.097630262374878 time take: 0.018201589584350586\n",
      "idx: 100, loss: 2.535710334777832 time take: 3.6581244468688965\n",
      "idx: 200, loss: 1.9778485298156738 time take: 3.6827902793884277\n",
      "idx: 300, loss: 2.5644049644470215 time take: 3.6491801738739014\n",
      "idx: 400, loss: 1.981204628944397 time take: 3.642087936401367\n",
      "idx: 500, loss: 2.718363046646118 time take: 3.646202325820923\n",
      "idx: 600, loss: 2.301093578338623 time take: 3.6625208854675293\n",
      "idx: 700, loss: 4.166839599609375 time take: 3.652926445007324\n",
      "idx: 800, loss: 1.7737131118774414 time take: 3.677025318145752\n",
      "idx: 900, loss: 1.7081905603408813 time take: 3.7871170043945312\n",
      "idx: 1000, loss: 3.243818521499634 time take: 3.648857355117798\n",
      "idx: 1100, loss: 2.1568756103515625 time take: 3.65791392326355\n",
      "idx: 1200, loss: 2.2409281730651855 time take: 3.6781058311462402\n",
      "idx: 1300, loss: 2.039914131164551 time take: 3.6600232124328613\n",
      "idx: 1400, loss: 2.067941665649414 time take: 3.662123441696167\n",
      "idx: 1500, loss: 2.1161646842956543 time take: 3.6525936126708984\n",
      "epoch 9 time taken: 57.3274290561676s\n",
      "Training epoch: 10\n",
      "idx: 0, loss: 2.633369207382202 time take: 0.017266273498535156\n",
      "idx: 100, loss: 2.1352908611297607 time take: 3.6612441539764404\n",
      "idx: 200, loss: 2.3633575439453125 time take: 3.6548068523406982\n",
      "idx: 300, loss: 2.1307318210601807 time take: 3.6486573219299316\n",
      "idx: 400, loss: 2.149102210998535 time take: 3.6535139083862305\n",
      "idx: 500, loss: 1.9927557706832886 time take: 3.644166946411133\n",
      "idx: 600, loss: 2.346811294555664 time take: 3.6464786529541016\n",
      "idx: 700, loss: 3.665865421295166 time take: 3.663956642150879\n",
      "idx: 800, loss: 1.4389718770980835 time take: 3.650752544403076\n",
      "idx: 900, loss: 1.785749077796936 time take: 3.668304920196533\n",
      "idx: 1000, loss: 2.1823248863220215 time take: 3.785412073135376\n",
      "idx: 1100, loss: 1.72092604637146 time take: 3.637669563293457\n",
      "idx: 1200, loss: 1.9406458139419556 time take: 3.653177261352539\n",
      "idx: 1300, loss: 2.3339080810546875 time take: 3.646915912628174\n",
      "idx: 1400, loss: 1.6076475381851196 time take: 3.6492600440979004\n",
      "idx: 1500, loss: 2.7930045127868652 time take: 3.6604068279266357\n",
      "epoch 10 time taken: 57.20885872840881s\n",
      "Training epoch: 11\n",
      "idx: 0, loss: 2.3758230209350586 time take: 0.017520427703857422\n",
      "idx: 100, loss: 2.4521799087524414 time take: 4.467871427536011\n",
      "idx: 200, loss: 2.182061195373535 time take: 4.440028429031372\n",
      "idx: 300, loss: 2.0605356693267822 time take: 4.488194942474365\n",
      "idx: 400, loss: 2.3425774574279785 time take: 4.439260005950928\n",
      "idx: 500, loss: 3.033010482788086 time take: 4.568437576293945\n",
      "idx: 600, loss: 2.937084674835205 time take: 4.589484214782715\n",
      "idx: 700, loss: 3.2125720977783203 time take: 4.503532886505127\n",
      "idx: 800, loss: 2.008901357650757 time take: 4.536400079727173\n",
      "idx: 900, loss: 1.7790591716766357 time take: 4.643679857254028\n",
      "idx: 1000, loss: 2.5015182495117188 time take: 4.4204864501953125\n",
      "idx: 1100, loss: 1.8274664878845215 time take: 4.570006370544434\n",
      "idx: 1200, loss: 1.264825463294983 time take: 4.535661697387695\n",
      "idx: 1300, loss: 2.006701946258545 time take: 4.289938926696777\n",
      "idx: 1400, loss: 1.636020302772522 time take: 4.43972373008728\n",
      "idx: 1500, loss: 2.824503183364868 time take: 4.57331657409668\n",
      "epoch 11 time taken: 70.37609720230103s\n",
      "Training epoch: 12\n",
      "idx: 0, loss: 1.920665979385376 time take: 0.02213883399963379\n",
      "idx: 100, loss: 2.601271629333496 time take: 4.562015056610107\n",
      "idx: 200, loss: 2.2891628742218018 time take: 4.568626403808594\n",
      "idx: 300, loss: 1.7198076248168945 time take: 4.5792930126190186\n",
      "idx: 400, loss: 2.366790771484375 time take: 4.559717893600464\n",
      "idx: 500, loss: 2.61232852935791 time take: 4.591086149215698\n",
      "idx: 600, loss: 2.6140849590301514 time take: 4.712427139282227\n",
      "idx: 700, loss: 3.2032172679901123 time take: 4.5699896812438965\n",
      "idx: 800, loss: 1.6110172271728516 time take: 4.513411998748779\n",
      "idx: 900, loss: 1.7705857753753662 time take: 4.34102725982666\n",
      "idx: 1000, loss: 2.1832587718963623 time take: 4.580918312072754\n",
      "idx: 1100, loss: 1.7756017446517944 time take: 4.5435426235198975\n",
      "idx: 1200, loss: 1.8150357007980347 time take: 4.385306358337402\n",
      "idx: 1300, loss: 2.2257609367370605 time take: 4.583010673522949\n",
      "idx: 1400, loss: 1.8737249374389648 time take: 4.537576198577881\n",
      "idx: 1500, loss: 2.5491342544555664 time take: 4.532343149185181\n",
      "epoch 12 time taken: 70.97379350662231s\n",
      "Training epoch: 13\n",
      "idx: 0, loss: 2.1003103256225586 time take: 0.02207636833190918\n",
      "idx: 100, loss: 2.189063787460327 time take: 4.522965669631958\n",
      "idx: 200, loss: 2.431770086288452 time take: 4.579947471618652\n",
      "idx: 300, loss: 1.7512850761413574 time take: 4.582285642623901\n",
      "idx: 400, loss: 2.164745807647705 time take: 4.670304536819458\n",
      "idx: 500, loss: 2.7862753868103027 time take: 4.506179332733154\n",
      "idx: 600, loss: 1.911227822303772 time take: 3.8322060108184814\n",
      "idx: 700, loss: 3.7972497940063477 time take: 3.6475722789764404\n",
      "idx: 800, loss: 1.8546409606933594 time take: 3.6412880420684814\n",
      "idx: 900, loss: 2.043236255645752 time take: 3.644850254058838\n",
      "idx: 1000, loss: 2.0490260124206543 time take: 3.6331140995025635\n",
      "idx: 1100, loss: 1.9132704734802246 time take: 3.9708282947540283\n",
      "idx: 1200, loss: 1.9203134775161743 time take: 4.503713846206665\n",
      "idx: 1300, loss: 2.0447440147399902 time take: 4.586753606796265\n",
      "idx: 1400, loss: 1.8091503381729126 time take: 4.579169034957886\n",
      "idx: 1500, loss: 2.427194833755493 time take: 4.598402500152588\n",
      "epoch 13 time taken: 66.37800478935242s\n",
      "Training epoch: 14\n",
      "idx: 0, loss: 2.9033870697021484 time take: 0.020345687866210938\n",
      "idx: 100, loss: 2.089599847793579 time take: 4.521273374557495\n",
      "idx: 200, loss: 1.7462857961654663 time take: 4.706334829330444\n",
      "idx: 300, loss: 2.4093873500823975 time take: 4.6234824657440186\n",
      "idx: 400, loss: 1.8805676698684692 time take: 4.596155166625977\n",
      "idx: 500, loss: 2.9195446968078613 time take: 4.578035354614258\n",
      "idx: 600, loss: 2.6925246715545654 time take: 4.575347423553467\n",
      "idx: 700, loss: 2.375425100326538 time take: 5.5448291301727295\n",
      "idx: 800, loss: 2.008735179901123 time take: 4.587993621826172\n",
      "idx: 900, loss: 2.3050007820129395 time take: 4.415584087371826\n",
      "idx: 1000, loss: 2.5354833602905273 time take: 4.521315574645996\n",
      "idx: 1100, loss: 1.990340232849121 time take: 4.492166757583618\n",
      "idx: 1200, loss: 1.9147768020629883 time take: 4.526777267456055\n",
      "idx: 1300, loss: 2.0580642223358154 time take: 4.578808307647705\n",
      "idx: 1400, loss: 1.7192312479019165 time take: 4.579589128494263\n",
      "idx: 1500, loss: 2.865576982498169 time take: 4.673256158828735\n",
      "epoch 14 time taken: 72.40904378890991s\n",
      "Training epoch: 15\n",
      "idx: 0, loss: 2.2736353874206543 time take: 0.020831584930419922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx: 100, loss: 1.8731220960617065 time take: 4.57926869392395\n",
      "idx: 200, loss: 2.236649990081787 time take: 4.498907804489136\n",
      "idx: 300, loss: 2.7139971256256104 time take: 4.4468419551849365\n",
      "idx: 400, loss: 2.365205764770508 time take: 5.505019903182983\n",
      "idx: 500, loss: 2.89172625541687 time take: 4.594611883163452\n",
      "idx: 600, loss: 2.556126117706299 time take: 4.5916361808776855\n",
      "idx: 700, loss: 3.8112246990203857 time take: 4.58705997467041\n",
      "idx: 800, loss: 1.9136977195739746 time take: 4.589562654495239\n",
      "idx: 900, loss: 1.9263142347335815 time take: 4.578585863113403\n",
      "idx: 1000, loss: 2.659060478210449 time take: 4.53690767288208\n",
      "idx: 1100, loss: 2.2981348037719727 time take: 4.5846405029296875\n",
      "idx: 1200, loss: 2.2562971115112305 time take: 4.566627025604248\n",
      "idx: 1300, loss: 1.7477540969848633 time take: 4.741864919662476\n",
      "idx: 1400, loss: 1.8502662181854248 time take: 4.5208821296691895\n",
      "idx: 1500, loss: 2.802837371826172 time take: 4.857735633850098\n",
      "epoch 15 time taken: 73.28112888336182s\n",
      "Training epoch: 16\n",
      "idx: 0, loss: 2.8848462104797363 time take: 0.017376184463500977\n",
      "idx: 100, loss: 2.340782642364502 time take: 3.6167616844177246\n",
      "idx: 200, loss: 2.6805217266082764 time take: 3.615635871887207\n",
      "idx: 300, loss: 2.896951675415039 time take: 3.624114751815796\n",
      "idx: 400, loss: 2.3289897441864014 time take: 3.6273317337036133\n",
      "idx: 500, loss: 2.8633625507354736 time take: 3.6192305088043213\n",
      "idx: 600, loss: 2.4470434188842773 time take: 3.634700059890747\n",
      "idx: 700, loss: 3.4174983501434326 time take: 3.635977029800415\n",
      "idx: 800, loss: 1.598078727722168 time take: 3.6234331130981445\n",
      "idx: 900, loss: 2.0048916339874268 time take: 3.6210551261901855\n",
      "idx: 1000, loss: 2.8446085453033447 time take: 3.626335382461548\n",
      "idx: 1100, loss: 1.3672126531600952 time take: 3.616487979888916\n",
      "idx: 1200, loss: 2.0876758098602295 time take: 3.7490668296813965\n",
      "idx: 1300, loss: 1.8351099491119385 time take: 3.6393015384674072\n",
      "idx: 1400, loss: 1.8756201267242432 time take: 3.616441011428833\n",
      "idx: 1500, loss: 2.7271933555603027 time take: 3.6204981803894043\n",
      "epoch 16 time taken: 56.77433443069458s\n",
      "Training epoch: 17\n",
      "idx: 0, loss: 2.9965245723724365 time take: 0.0174715518951416\n",
      "idx: 100, loss: 2.5896592140197754 time take: 3.6263511180877686\n",
      "idx: 200, loss: 2.146387815475464 time take: 3.619042158126831\n",
      "idx: 300, loss: 2.6727652549743652 time take: 3.622462511062622\n",
      "idx: 400, loss: 2.5435128211975098 time take: 3.61320161819458\n",
      "idx: 500, loss: 2.764418125152588 time take: 3.6133875846862793\n",
      "idx: 600, loss: 2.1936943531036377 time take: 3.6176023483276367\n",
      "idx: 700, loss: 3.2789254188537598 time take: 3.6134419441223145\n",
      "idx: 800, loss: 1.5050941705703735 time take: 3.6057400703430176\n",
      "idx: 900, loss: 1.4937632083892822 time take: 3.6103768348693848\n",
      "idx: 1000, loss: 2.4976532459259033 time take: 3.5971274375915527\n",
      "idx: 1100, loss: 1.6848409175872803 time take: 3.6108179092407227\n",
      "idx: 1200, loss: 1.933973789215088 time take: 3.598367691040039\n",
      "idx: 1300, loss: 2.103839635848999 time take: 3.72408390045166\n",
      "idx: 1400, loss: 1.6993486881256104 time take: 3.598768472671509\n",
      "idx: 1500, loss: 2.413837194442749 time take: 3.598254919052124\n",
      "epoch 17 time taken: 56.54087543487549s\n",
      "Training epoch: 18\n",
      "idx: 0, loss: 2.8538780212402344 time take: 0.017238616943359375\n",
      "idx: 100, loss: 2.4121527671813965 time take: 3.600853443145752\n",
      "idx: 200, loss: 2.046645402908325 time take: 3.605624198913574\n",
      "idx: 300, loss: 2.7459819316864014 time take: 3.6019511222839355\n",
      "idx: 400, loss: 2.0161218643188477 time take: 3.6117165088653564\n",
      "idx: 500, loss: 2.0352935791015625 time take: 3.6156117916107178\n",
      "idx: 600, loss: 1.8308104276657104 time take: 3.616988182067871\n",
      "idx: 700, loss: 3.513333559036255 time take: 3.608821153640747\n",
      "idx: 800, loss: 1.65188729763031 time take: 3.6123573780059814\n",
      "idx: 900, loss: 1.82823646068573 time take: 3.605487585067749\n",
      "idx: 1000, loss: 2.180927038192749 time take: 3.617615222930908\n",
      "idx: 1100, loss: 1.9173753261566162 time take: 3.602494716644287\n",
      "idx: 1200, loss: 1.971449613571167 time take: 3.611900806427002\n",
      "idx: 1300, loss: 2.241058588027954 time take: 3.6033754348754883\n",
      "idx: 1400, loss: 1.6103579998016357 time take: 3.725983142852783\n",
      "idx: 1500, loss: 2.1005702018737793 time take: 3.601595878601074\n",
      "epoch 18 time taken: 56.51259732246399s\n",
      "Training epoch: 19\n",
      "idx: 0, loss: 2.083698272705078 time take: 0.01718902587890625\n",
      "idx: 100, loss: 2.0414412021636963 time take: 3.601438045501709\n",
      "idx: 200, loss: 1.8332655429840088 time take: 3.6016013622283936\n",
      "idx: 300, loss: 3.0410406589508057 time take: 3.6084823608398438\n",
      "idx: 400, loss: 3.2449967861175537 time take: 3.605923891067505\n",
      "idx: 500, loss: 2.541165590286255 time take: 3.6055548191070557\n",
      "idx: 600, loss: 2.570011854171753 time take: 3.6005570888519287\n"
     ]
    }
   ],
   "source": [
    "config_cifar_adam_softplus_odin = {\n",
    "    \"batch_size\": 32,\n",
    "    \"n_classes\": 10,\n",
    "    \"dataset_name\": \"cifar\",\n",
    "    \"epochs\": 100,\n",
    "    \"version\": time.time(),\n",
    "    \"lr\": 0.01,\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 0.0005,\n",
    "    \"optimizer_type\": \"Adam\",\n",
    "    \"activation_function_type\": \"softplus\",\n",
    "    \"network\": \"resnet50\",\n",
    "    \"postprocessor_type\": \"odin\",\n",
    "    \"trials\": 3,\n",
    "    \"dataset_type\": \"cifar\",\n",
    "    \"results_dir\": \"cifar10-study\",\n",
    "    \"pretrained\": False\n",
    "}\n",
    "config_cifar_adam_softplus_odin[\"data_loaders\"] = get_data_loaders(config_cifar_adam_softplus_odin)\n",
    "run_full_oodn_pipeline(config_cifar_adam_softplus_odin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
