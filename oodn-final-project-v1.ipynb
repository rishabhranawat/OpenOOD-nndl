{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e586cf4c",
   "metadata": {},
   "source": [
    "### Exploring The Impact of Optimizers and Activation Functions On OODN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "9b10d253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "\n",
    "from torchvision.datasets import mnist, FashionMNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.nn import Module\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torchvision.models.resnet import Bottleneck, ResNet\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from openood.evaluators import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d1e6eae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f61ebd",
   "metadata": {},
   "source": [
    "### Supported Activation Functions\n",
    "\n",
    "For activation functions, we are considering ReLU, Softplus, Swish. *Note that we may conduct experiments for a subset based on the compute resources available*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9fe7a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation_fn(activation):\n",
    "    if activation == 'relu':\n",
    "        return nn.ReLU()\n",
    "    elif activation == 'softplus':\n",
    "        return nn.Softplus()\n",
    "    elif activation == 'swish':\n",
    "        return nn.Swish()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a392498b",
   "metadata": {},
   "source": [
    "### Supported Networks\n",
    "\n",
    "Currently, we support LeNet and ResNet50."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ecec6e",
   "metadata": {},
   "source": [
    "### LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac0b312c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self, num_classes, num_channel=3, activation='relu'):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.feature_size = 84\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=num_channel,\n",
    "                      out_channels=6,\n",
    "                      kernel_size=5,\n",
    "                      stride=1,\n",
    "                      padding=2), get_activation_fn(activation), nn.MaxPool2d(kernel_size=2))\n",
    "\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1),\n",
    "             get_activation_fn(activation), nn.MaxPool2d(kernel_size=2))\n",
    "\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16,\n",
    "                      out_channels=120,\n",
    "                      kernel_size=5,\n",
    "                      stride=1), get_activation_fn(activation))\n",
    "\n",
    "        self.classifier1 = nn.Linear(in_features=120, out_features=84)\n",
    "        self.relu = get_activation_fn(activation)\n",
    "        self.fc = nn.Linear(in_features=84, out_features=num_classes)\n",
    "\n",
    "    def get_fc(self):\n",
    "        fc = self.fc\n",
    "        return fc.weight.cpu().detach().numpy(), fc.bias.cpu().detach().numpy()\n",
    "\n",
    "    def forward(self, x, return_feature=False, return_feature_list=False):\n",
    "        feature1 = self.block1(x)\n",
    "        feature2 = self.block2(feature1)\n",
    "        feature3 = self.block3(feature2)\n",
    "        feature3 = feature3.view(feature3.shape[0], -1)\n",
    "        feature = self.relu(self.classifier1(feature3))\n",
    "        logits_cls = self.fc(feature)\n",
    "        feature_list = [feature1, feature2, feature3, feature]\n",
    "        if return_feature:\n",
    "            return logits_cls, feature\n",
    "        elif return_feature_list:\n",
    "            return logits_cls, feature_list\n",
    "        else:\n",
    "            return logits_cls\n",
    "\n",
    "    def forward_threshold(self, x, threshold):\n",
    "        feature1 = self.block1(x)\n",
    "        feature2 = self.block2(feature1)\n",
    "        feature3 = self.block3(feature2)\n",
    "        feature3 = feature3.view(feature3.shape[0], -1)\n",
    "        feature = self.relu(self.classifier1(feature3))\n",
    "        feature = feature.clip(max=threshold)\n",
    "        logits_cls = self.fc(feature)\n",
    "\n",
    "        return logits_cls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b6dc2b",
   "metadata": {},
   "source": [
    "### ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "df1775f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet50(ResNet):\n",
    "    def __init__(self,\n",
    "                 block=Bottleneck,\n",
    "                 layers=[3, 4, 6, 3],\n",
    "                 num_classes=1000):\n",
    "        super(ResNet50, self).__init__(block=block,\n",
    "                                       layers=layers,\n",
    "                                       num_classes=num_classes)\n",
    "        self.feature_size = 2048\n",
    "\n",
    "\n",
    "    def forward(self, x, return_feature=False, return_feature_list=False):\n",
    "        feature1 = self.relu(self.bn1(self.conv1(x)))\n",
    "        feature1 = self.maxpool(feature1)\n",
    "        feature2 = self.layer1(feature1)\n",
    "        feature3 = self.layer2(feature2)\n",
    "        feature4 = self.layer3(feature3)\n",
    "        feature5 = self.layer4(feature4)\n",
    "        feature5 = self.avgpool(feature5)\n",
    "        feature = feature5.view(feature5.size(0), -1)\n",
    "        logits_cls = self.fc(feature)\n",
    "\n",
    "        feature_list = [feature1, feature2, feature3, feature4, feature5]\n",
    "        if return_feature:\n",
    "            return logits_cls, feature\n",
    "        elif return_feature_list:\n",
    "            return logits_cls, feature_list\n",
    "        else:\n",
    "            return logits_cls\n",
    "\n",
    "    def forward_threshold(self, x, threshold):\n",
    "        feature1 = self.relu(self.bn1(self.conv1(x)))\n",
    "        feature1 = self.maxpool(feature1)\n",
    "        feature2 = self.layer1(feature1)\n",
    "        feature3 = self.layer2(feature2)\n",
    "        feature4 = self.layer3(feature3)\n",
    "        feature5 = self.layer4(feature4)\n",
    "        feature5 = self.avgpool(feature5)\n",
    "        feature = feature5.clip(max=threshold)\n",
    "        feature = feature.view(feature.size(0), -1)\n",
    "        logits_cls = self.fc(feature)\n",
    "\n",
    "        return logits_cls\n",
    "\n",
    "    def get_fc(self):\n",
    "        fc = self.fc\n",
    "        return fc.weight.cpu().detach().numpy(), fc.bias.cpu().detach().numpy()\n",
    "    \n",
    "def get_resnet_model(activation_function_type, n_classes):\n",
    "    resnet_model = ResNet50(num_classes=n_classes)\n",
    "    resnet_model.to(device)    \n",
    "    return resnet_model\n",
    "\n",
    "def set_activation_function(resnet_model, activation_function_type):\n",
    "    resnet_model.relu = get_activation_fn(activation_function_type)\n",
    "    resnet_model.layer1[0].relu = get_activation_fn(activation_function_type)\n",
    "    resnet_model.layer1[1].relu = get_activation_fn(activation_function_type)\n",
    "    resnet_model.layer1[2].relu = get_activation_fn(activation_function_type)\n",
    "\n",
    "    resnet_model.layer2[0].relu = get_activation_fn(activation_function_type)\n",
    "    resnet_model.layer2[1].relu = get_activation_fn(activation_function_type)\n",
    "    resnet_model.layer2[2].relu = get_activation_fn(activation_function_type)\n",
    "    resnet_model.layer2[3].relu = get_activation_fn(activation_function_type)\n",
    "\n",
    "    resnet_model.layer3[0].relu = get_activation_fn(activation_function_type)\n",
    "    resnet_model.layer3[1].relu = get_activation_fn(activation_function_type)\n",
    "    resnet_model.layer3[2].relu = get_activation_fn(activation_function_type)\n",
    "    resnet_model.layer3[3].relu = get_activation_fn(activation_function_type)\n",
    "    resnet_model.layer3[4].relu = get_activation_fn(activation_function_type)\n",
    "    resnet_model.layer3[5].relu = get_activation_fn(activation_function_type)\n",
    "\n",
    "\n",
    "    resnet_model.layer4[0].relu = get_activation_fn(activation_function_type)\n",
    "    resnet_model.layer4[1].relu = get_activation_fn(activation_function_type)\n",
    "    resnet_model.layer4[2].relu = get_activation_fn(activation_function_type)\n",
    "    \n",
    "    return resnet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "2724a62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(config):\n",
    "    activation_function_type = config[\"activation_function_type\"]\n",
    "    network_type = config[\"network\"]\n",
    "    n_classes = config[\"n_classes\"]\n",
    "    \n",
    "    if network_type == \"lenet\":\n",
    "        model =  LeNet(num_classes=n_classes, num_channel=1, activation=activation_function_type)\n",
    "    elif network_type == \"resnet50\":\n",
    "        model = get_resnet_model(activation_function_type, n_classes)\n",
    "    else:\n",
    "        raise Exception(\"Currently we only support lenet or resnet50\")\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd0b36d",
   "metadata": {},
   "source": [
    "### Supported Post-Hoc OODN Processors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec960ab0",
   "metadata": {},
   "source": [
    "#### The first post processor we consider is ODIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "664264fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODINPostprocessor():\n",
    "    def __init__(self, temperature, noise):\n",
    "        self.temperature = temperature\n",
    "        self.noise = noise\n",
    "        \n",
    "    def postprocess(self, net: nn.Module, data):\n",
    "        net.eval()\n",
    "        data.requires_grad = True\n",
    "        output = net(data)\n",
    "\n",
    "        # Calculating the perturbation we need to add, that is,\n",
    "        # the sign of gradient of cross entropy loss w.r.t. input\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        labels = output.detach().argmax(axis=1)\n",
    "\n",
    "        # Using temperature scaling\n",
    "        output = output / self.temperature\n",
    "\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Normalizing the gradient to binary in {0, 1}\n",
    "        gradient = torch.ge(data.grad.detach(), 0)\n",
    "        gradient = (gradient.float() - 0.5) * 2\n",
    "\n",
    "        # Scaling values taken from original code       \n",
    "        gradient[:, 0] = (gradient[:, 0]) / (63.0 / 255.0)\n",
    "        if gradient.shape[1] == 3:\n",
    "            gradient[:, 1] = (gradient[:, 1]) / (62.1 / 255.0)\n",
    "            gradient[:, 2] = (gradient[:, 2]) / (66.7 / 255.0)\n",
    "\n",
    "        # Adding small perturbations to images\n",
    "        tempInputs = torch.add(data.detach(), gradient, alpha=-self.noise)\n",
    "        output = net(tempInputs)\n",
    "        output = output / self.temperature\n",
    "\n",
    "        # Calculating the confidence after adding perturbations\n",
    "        nnOutput = output.detach()\n",
    "        nnOutput = nnOutput - nnOutput.max(dim=1, keepdims=True).values\n",
    "        nnOutput = nnOutput.exp() / nnOutput.exp().sum(dim=1, keepdims=True)\n",
    "\n",
    "        conf, pred = nnOutput.max(dim=1)\n",
    "\n",
    "        return pred, conf\n",
    "    \n",
    "    def inference(self, net: nn.Module, data_loader: DataLoader):\n",
    "        pred_list, conf_list, label_list = [], [], []\n",
    "        for idx, loaded_data in enumerate(data_loader):\n",
    "            data, label = loaded_data[0], loaded_data[1]\n",
    "            if idx % 5 == 0:\n",
    "                print(f'Performing inference on batch: {idx}')\n",
    "            pred, conf = self.postprocess(net, data.to(device))\n",
    "            for idx in range(len(data)):\n",
    "                pred_list.append(pred[idx].tolist())\n",
    "                conf_list.append(conf[idx].tolist())\n",
    "                label_list.append(label[idx].tolist())\n",
    "\n",
    "        # convert values into numpy array\n",
    "        pred_list = np.array(pred_list, dtype=int)\n",
    "        conf_list = np.array(conf_list)\n",
    "        label_list = np.array(label_list, dtype=int)\n",
    "\n",
    "        return pred_list, conf_list, label_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84256e7d",
   "metadata": {},
   "source": [
    "#### We consider the Maximum Classifier Discrepancy Post OODN method\n",
    "\n",
    "https://arxiv.org/pdf/1712.02560.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5074e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCDPostprocessor(BasePostprocessor):\n",
    "    @torch.no_grad()\n",
    "    def postprocess(self, net: nn.Module, data: Any):\n",
    "        logits1, logits2 = net(data, return_double=True)\n",
    "        score1 = torch.softmax(logits1, dim=1)\n",
    "        score2 = torch.softmax(logits2, dim=1)\n",
    "        conf = -torch.sum(torch.abs(score1 - score2), dim=1)\n",
    "        _, pred = torch.max(score1, dim=1)\n",
    "        return pred, conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "a95fbf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_postprocessor(postprocessor_type=\"odin\"):\n",
    "    if postprocessor_type == \"odin\":\n",
    "        postprocessor = ODINPostprocessor(1000, 0.0014)\n",
    "    return postprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b011d87b",
   "metadata": {},
   "source": [
    "### Supported Out of Distribution Detection Metrics\n",
    "\n",
    "What metrics do we specifically care about here?\n",
    "\n",
    "**FPR@95** measures the false positive rate (FPR) when the true positive rate (TPR) is\n",
    "equal to 95%. Lower scores indicate better performance. \n",
    "\n",
    "**AUROC** measures the area under the\n",
    "Receiver Operating Characteristic (ROC) curve, which displays the relationship between TPR and\n",
    "FPR. The area under the ROC curve can be interpreted as the probability that a positive ID example\n",
    "will have a higher detection score than a negative OOD example. \n",
    "\n",
    "**AUPR** measures the area under\n",
    "the Precision-Recall (PR) curve. The PR curve is created by plotting precision versus recall. Similar\n",
    "to AUROC, we consider ID samples as positive, so that the score corresponds to the AUPR-In metric\n",
    "in some works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "59e1eb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_oodn_metrics(model, postprocessor_type, id_test_loader, ood_test_loader, ood_name):\n",
    "    postprocessor = get_postprocessor(postprocessor_type)\n",
    "    id_pred, id_conf, id_gt = postprocessor.inference(\n",
    "                model, id_test_loader)\n",
    "\n",
    "    ood_pred, ood_conf, ood_gt = postprocessor.inference(\n",
    "        model, ood_test_loader)\n",
    "\n",
    "    ood_gt = -1 * np.ones_like(ood_gt)  # hard set to -1 as ood\n",
    "    pred = np.concatenate([id_pred, ood_pred])\n",
    "    conf = np.concatenate([id_conf, ood_conf])\n",
    "    label = np.concatenate([id_gt, ood_gt])\n",
    "    ood_metrics = metrics.compute_all_metrics(conf, label, pred)\n",
    "\n",
    "    print_formatted_metrics(ood_metrics, ood_name)\n",
    "\n",
    "def print_formatted_metrics(metrics, dataset_name):\n",
    "    [fpr, auroc, aupr_in, aupr_out,\n",
    "     ccr_4, ccr_3, ccr_2, ccr_1, accuracy] \\\n",
    "     = metrics\n",
    "\n",
    "    write_content = {\n",
    "        'dataset': dataset_name,\n",
    "        'FPR@95': '{:.2f}'.format(100 * fpr),\n",
    "        'AUROC': '{:.2f}'.format(100 * auroc),\n",
    "        'AUPR_IN': '{:.2f}'.format(100 * aupr_in),\n",
    "        'AUPR_OUT': '{:.2f}'.format(100 * aupr_out),\n",
    "        'CCR_4': '{:.2f}'.format(100 * ccr_4),\n",
    "        'CCR_3': '{:.2f}'.format(100 * ccr_3),\n",
    "        'CCR_2': '{:.2f}'.format(100 * ccr_2),\n",
    "        'CCR_1': '{:.2f}'.format(100 * ccr_1),\n",
    "        'ACC': '{:.2f}'.format(100 * accuracy)\n",
    "    }\n",
    "\n",
    "    fieldnames = list(write_content.keys())\n",
    "\n",
    "    # print ood metric results\n",
    "    print('FPR@95: {:.2f}, AUROC: {:.2f}'.format(100 * fpr, 100 * auroc),\n",
    "          end=' ',\n",
    "          flush=True)\n",
    "    print('AUPR_IN: {:.2f}, AUPR_OUT: {:.2f}'.format(\n",
    "        100 * aupr_in, 100 * aupr_out),\n",
    "          flush=True)\n",
    "    print('CCR: {:.2f}, {:.2f}, {:.2f}, {:.2f},'.format(\n",
    "        ccr_4 * 100, ccr_3 * 100, ccr_2 * 100, ccr_1 * 100),\n",
    "          end=' ',\n",
    "          flush=True)\n",
    "    print('ACC: {:.2f}'.format(accuracy * 100), flush=True)\n",
    "    print(u'\\u2500' * 70, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "72027116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, config):\n",
    "    params = model.parameters()\n",
    "    lr = config['lr']\n",
    "    momentum = config['momentum']\n",
    "    weight_decay = config['weight_decay']\n",
    "    optimizer_type = config['optimizer_type']\n",
    "    \n",
    "    print(f'Getting optimizer for type: {optimizer_type}...')\n",
    "    if optimizer_type == 'SGD':\n",
    "        return SGD(params, \n",
    "              lr=lr, \n",
    "              momentum=momentum,\n",
    "              weight_decay=weight_decay)\n",
    "    elif optimizer_type == 'Adam':\n",
    "        return Adam(params, \n",
    "                    lr=lr, \n",
    "                    weight_decay=weight_decay)\n",
    "    else:\n",
    "        raise Exception(\"Invalid optimizer_type provided, only SGD and Adam are supported currently\")\n",
    "        \n",
    "def get_data_loaders(config):\n",
    "    data_loaders = {}\n",
    "    dataset_name = config[\"dataset_name\"]\n",
    "    dataset_type = config[\"dataset_type\"]\n",
    "    batch_size = config['batch_size']\n",
    "    \n",
    "    if dataset_type == \"wilds\":\n",
    "        # wilds dataset\n",
    "        dataset = get_dataset(dataset=dataset_name, download=True)\n",
    "        data_loaders[\"train\"] = get_wilds_loader(dataset, \"train\", batch_size)\n",
    "        data_loaders[\"ood_test\"] = get_wilds_loader(dataset, \"test\", batch_size)\n",
    "        data_loaders[\"id_test\"] = get_wilds_loader(dataset, \"id_test\", batch_size)\n",
    "    elif dataset_name == \"mnist\":\n",
    "        # mnist dataset\n",
    "        train_dataset = mnist.MNIST(root='data', download=False, train=True, transform=ToTensor())\n",
    "        test_dataset = mnist.MNIST(root='data', download=False, train=False, transform=ToTensor())\n",
    "        fashion_test_dataset = mnist.FashionMNIST(root='data', download=True,train=False,transform=ToTensor())\n",
    "\n",
    "        data_loaders[\"train\"] = DataLoader(train_dataset, batch_size=batch_size)\n",
    "        data_loaders[\"id_test\"] = DataLoader(test_dataset, batch_size=batch_size)\n",
    "        data_loaders[\"ood_test\"] = DataLoader(fashion_test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    return data_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "e9540513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_resnet_model_given_opti_activation_fn(config):\n",
    "    # get the train loader\n",
    "    train_loader = config[\"data_loaders\"][\"train\"]\n",
    "    \n",
    "    # get the resnet model with the replaced activation functions\n",
    "    model = get_model(config)\n",
    "    model.to(device)\n",
    "    \n",
    "    # get the optimizer\n",
    "    sgd = get_optimizer(model, config)\n",
    "    \n",
    "    loss_fn = CrossEntropyLoss()\n",
    "\n",
    "    for current_epoch in range(config['epochs']):\n",
    "        model.train()\n",
    "        print('Training epoch: {}'.format(current_epoch))\n",
    "        for idx, (loader_data) in enumerate(train_loader):\n",
    "            train_x, train_label = loader_data[0].to(device), loader_data[1].to(device)\n",
    "            sgd.zero_grad()\n",
    "            predict_y = model(train_x.float())\n",
    "            loss = loss_fn(predict_y, train_label.long())\n",
    "            if idx % 100 == 0:\n",
    "                print('idx: {}, loss: {}'.format(idx, loss.sum().item()))\n",
    "            loss.backward()\n",
    "            sgd.step()\n",
    "    \n",
    "    model_name = f\"models/{config['network']}_{config['dataset_name']}_{config['version']}.pkl\"\n",
    "    torch.save(model, model_name)\n",
    "    return model\n",
    "\n",
    "def run_full_oodn_pipeline(config):\n",
    "    # train model\n",
    "    model = train_resnet_model_given_opti_activation_fn(config)\n",
    "    # calculate oodn metrics\n",
    "    calculate_oodn_metrics(model,\n",
    "                           \"odin\", \n",
    "                           config[\"data_loaders\"][\"id_test\"], \n",
    "                           config[\"data_loaders\"][\"ood_test\"], \n",
    "                           config[\"dataset_name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a842905",
   "metadata": {},
   "source": [
    "### Study 1: LeNet5, MNIST as the ID Dataset, FashionMNIST as the OOD Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2758e4",
   "metadata": {},
   "source": [
    "#### Study 1(a): Combination: SGD + ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "cb6aafb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting optimizer for type: SGD...\n",
      "Training epoch: 0\n",
      "idx: 0, loss: 2.297888994216919\n",
      "idx: 100, loss: 2.308368682861328\n",
      "idx: 200, loss: 2.304520606994629\n",
      "idx: 300, loss: 2.302668333053589\n",
      "idx: 400, loss: 2.3048624992370605\n",
      "Training epoch: 1\n",
      "idx: 0, loss: 2.29297137260437\n",
      "idx: 100, loss: 2.3060293197631836\n",
      "idx: 200, loss: 2.3049848079681396\n",
      "idx: 300, loss: 2.3015921115875244\n",
      "idx: 400, loss: 2.3056366443634033\n",
      "Training epoch: 2\n",
      "idx: 0, loss: 2.293072462081909\n",
      "idx: 100, loss: 2.306029796600342\n",
      "idx: 200, loss: 2.3049848079681396\n",
      "idx: 300, loss: 2.301593780517578\n",
      "idx: 400, loss: 2.3056366443634033\n",
      "Training epoch: 3\n",
      "idx: 0, loss: 2.293072462081909\n",
      "idx: 100, loss: 2.306029796600342\n",
      "idx: 200, loss: 2.3049848079681396\n",
      "idx: 300, loss: 2.3015921115875244\n",
      "idx: 400, loss: 2.3056366443634033\n",
      "Training epoch: 4\n",
      "idx: 0, loss: 2.293072462081909\n",
      "idx: 100, loss: 2.306029796600342\n",
      "idx: 200, loss: 2.3049848079681396\n",
      "idx: 300, loss: 2.3015921115875244\n",
      "idx: 400, loss: 2.3056318759918213\n",
      "Training epoch: 5\n",
      "idx: 0, loss: 2.2930707931518555\n",
      "idx: 100, loss: 2.306034803390503\n",
      "idx: 200, loss: 2.304975748062134\n",
      "idx: 300, loss: 2.3015851974487305\n",
      "idx: 400, loss: 2.3056375980377197\n",
      "Training epoch: 6\n",
      "idx: 0, loss: 2.293072462081909\n",
      "idx: 100, loss: 2.306029796600342\n",
      "idx: 200, loss: 2.3049848079681396\n",
      "idx: 300, loss: 2.3015921115875244\n",
      "idx: 400, loss: 2.3056366443634033\n",
      "Training epoch: 7\n",
      "idx: 0, loss: 2.293071746826172\n",
      "idx: 100, loss: 2.306029796600342\n",
      "idx: 200, loss: 2.3049848079681396\n",
      "idx: 300, loss: 2.3015921115875244\n",
      "idx: 400, loss: 2.3056366443634033\n",
      "Training epoch: 8\n",
      "idx: 0, loss: 2.293072462081909\n",
      "idx: 100, loss: 2.306029796600342\n",
      "idx: 200, loss: 2.3049848079681396\n",
      "idx: 300, loss: 2.3015921115875244\n",
      "idx: 400, loss: 2.3056366443634033\n",
      "Training epoch: 9\n",
      "idx: 0, loss: 2.293072462081909\n",
      "idx: 100, loss: 2.306029796600342\n",
      "idx: 200, loss: 2.3049848079681396\n",
      "idx: 300, loss: 2.3015921115875244\n",
      "idx: 400, loss: 2.3056366443634033\n",
      "Performing inference on batch: 0\n",
      "Performing inference on batch: 5\n",
      "Performing inference on batch: 10\n",
      "Performing inference on batch: 15\n",
      "Performing inference on batch: 20\n",
      "Performing inference on batch: 25\n",
      "Performing inference on batch: 30\n",
      "Performing inference on batch: 35\n",
      "Performing inference on batch: 40\n",
      "Performing inference on batch: 45\n",
      "Performing inference on batch: 50\n",
      "Performing inference on batch: 55\n",
      "Performing inference on batch: 60\n",
      "Performing inference on batch: 65\n",
      "Performing inference on batch: 70\n",
      "Performing inference on batch: 75\n",
      "Performing inference on batch: 0\n",
      "Performing inference on batch: 5\n",
      "Performing inference on batch: 10\n",
      "Performing inference on batch: 15\n",
      "Performing inference on batch: 20\n",
      "Performing inference on batch: 25\n",
      "Performing inference on batch: 30\n",
      "Performing inference on batch: 35\n",
      "Performing inference on batch: 40\n",
      "Performing inference on batch: 45\n",
      "Performing inference on batch: 50\n",
      "Performing inference on batch: 55\n",
      "Performing inference on batch: 60\n",
      "Performing inference on batch: 65\n",
      "Performing inference on batch: 70\n",
      "Performing inference on batch: 75\n",
      "FPR@95: 100.00, AUROC: 50.00 AUPR_IN: 75.00, AUPR_OUT: 75.00\n",
      "CCR: 0.00, 0.00, 0.00, 0.00, ACC: 10.28\n",
      "──────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "config_1a = {\n",
    "    \"batch_size\": 128,\n",
    "    \"n_classes\": 10,\n",
    "    \"dataset_name\": \"mnist\",\n",
    "    \"epochs\": 10,\n",
    "    \"version\": time.time(),\n",
    "    \"lr\": 0.1,\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 0.0005,\n",
    "    \"optimizer_type\": \"SGD\",\n",
    "    \"activation_function_type\": \"relu\",\n",
    "    \"network\": \"lenet\",\n",
    "    \"postprocessor_type\": \"odin\",\n",
    "    \"dataset_type\": \"mnist\"\n",
    "}\n",
    "config_1a[\"data_loaders\"] = get_data_loaders(config_1a)\n",
    "run_full_oodn_pipeline(config_1a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8333ab13",
   "metadata": {},
   "source": [
    "#### 1 (b) SGD + SoftPlus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "53d82538",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting optimizer for type: SGD...\n",
      "Training epoch: 0\n",
      "idx: 0, loss: 2.376993417739868\n",
      "idx: 100, loss: 2.3054940700531006\n",
      "idx: 200, loss: 1.3798028230667114\n",
      "idx: 300, loss: 0.3979167938232422\n",
      "idx: 400, loss: 0.3049400746822357\n",
      "Training epoch: 1\n",
      "idx: 0, loss: 0.09874945133924484\n",
      "idx: 100, loss: 0.1565113067626953\n",
      "idx: 200, loss: 0.16769079864025116\n",
      "idx: 300, loss: 0.07754665613174438\n",
      "idx: 400, loss: 0.22025398910045624\n",
      "Training epoch: 2\n",
      "idx: 0, loss: 0.12479684501886368\n",
      "idx: 100, loss: 0.08266792446374893\n",
      "idx: 200, loss: 0.12717781960964203\n",
      "idx: 300, loss: 0.06364835053682327\n",
      "idx: 400, loss: 0.16900688409805298\n",
      "Training epoch: 3\n",
      "idx: 0, loss: 0.09916403144598007\n",
      "idx: 100, loss: 0.03581171855330467\n",
      "idx: 200, loss: 0.14071375131607056\n",
      "idx: 300, loss: 0.0668136402964592\n",
      "idx: 400, loss: 0.15953058004379272\n",
      "Training epoch: 4\n",
      "idx: 0, loss: 0.10201045125722885\n",
      "idx: 100, loss: 0.034900370985269547\n",
      "idx: 200, loss: 0.1346389651298523\n",
      "idx: 300, loss: 0.06171699985861778\n",
      "idx: 400, loss: 0.11651628464460373\n",
      "Training epoch: 5\n",
      "idx: 0, loss: 0.0920725166797638\n",
      "idx: 100, loss: 0.050943560898303986\n",
      "idx: 200, loss: 0.14645643532276154\n",
      "idx: 300, loss: 0.06175169721245766\n",
      "idx: 400, loss: 0.0906241163611412\n",
      "Training epoch: 6\n",
      "idx: 0, loss: 0.08754538744688034\n",
      "idx: 100, loss: 0.05179022625088692\n",
      "idx: 200, loss: 0.09199076890945435\n",
      "idx: 300, loss: 0.07242026925086975\n",
      "idx: 400, loss: 0.10324882715940475\n",
      "Training epoch: 7\n",
      "idx: 0, loss: 0.053747281432151794\n",
      "idx: 100, loss: 0.020525457337498665\n",
      "idx: 200, loss: 0.11832331120967865\n",
      "idx: 300, loss: 0.044497739523649216\n",
      "idx: 400, loss: 0.08906630426645279\n",
      "Training epoch: 8\n",
      "idx: 0, loss: 0.09129838645458221\n",
      "idx: 100, loss: 0.030178464949131012\n",
      "idx: 200, loss: 0.05502891167998314\n",
      "idx: 300, loss: 0.04500502720475197\n",
      "idx: 400, loss: 0.08357533812522888\n",
      "Training epoch: 9\n",
      "idx: 0, loss: 0.03681686893105507\n",
      "idx: 100, loss: 0.012684333138167858\n",
      "idx: 200, loss: 0.0869780033826828\n",
      "idx: 300, loss: 0.04315680265426636\n",
      "idx: 400, loss: 0.11034522950649261\n",
      "Performing inference on batch: 0\n",
      "Performing inference on batch: 5\n",
      "Performing inference on batch: 10\n",
      "Performing inference on batch: 15\n",
      "Performing inference on batch: 20\n",
      "Performing inference on batch: 25\n",
      "Performing inference on batch: 30\n",
      "Performing inference on batch: 35\n",
      "Performing inference on batch: 40\n",
      "Performing inference on batch: 45\n",
      "Performing inference on batch: 50\n",
      "Performing inference on batch: 55\n",
      "Performing inference on batch: 60\n",
      "Performing inference on batch: 65\n",
      "Performing inference on batch: 70\n",
      "Performing inference on batch: 75\n",
      "Performing inference on batch: 0\n",
      "Performing inference on batch: 5\n",
      "Performing inference on batch: 10\n",
      "Performing inference on batch: 15\n",
      "Performing inference on batch: 20\n",
      "Performing inference on batch: 25\n",
      "Performing inference on batch: 30\n",
      "Performing inference on batch: 35\n",
      "Performing inference on batch: 40\n",
      "Performing inference on batch: 45\n",
      "Performing inference on batch: 50\n",
      "Performing inference on batch: 55\n",
      "Performing inference on batch: 60\n",
      "Performing inference on batch: 65\n",
      "Performing inference on batch: 70\n",
      "Performing inference on batch: 75\n",
      "FPR@95: 10.04, AUROC: 98.06 AUPR_IN: 98.34, AUPR_OUT: 97.69\n",
      "CCR: 47.46, 61.76, 79.54, 93.60, ACC: 97.32\n",
      "──────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "config_1b = {\n",
    "    \"batch_size\": 128,\n",
    "    \"dataset_name\": \"mnist\",\n",
    "    \"n_classes\": 10,\n",
    "    \"epochs\": 10,\n",
    "    \"version\": time.time(),\n",
    "    \"lr\": 0.1,\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 0.0005,\n",
    "    \"optimizer_type\": \"SGD\",\n",
    "    \"activation_function_type\": \"softplus\",\n",
    "    \"network\": \"lenet\",\n",
    "    \"postprocessor_type\": \"odin\",\n",
    "    \"dataset_type\": \"mnist\"\n",
    "}\n",
    "config_1b[\"data_loaders\"] = get_data_loaders(config_1b)\n",
    "run_full_oodn_pipeline(config_1b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ece712",
   "metadata": {},
   "source": [
    "#### 1(c) Adam + ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "efca18a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting optimizer for type: Adam...\n",
      "Training epoch: 0\n",
      "idx: 0, loss: 2.2955210208892822\n",
      "idx: 100, loss: 0.1766306608915329\n",
      "idx: 200, loss: 0.1108548566699028\n",
      "idx: 300, loss: 0.10212960094213486\n",
      "idx: 400, loss: 0.3330700695514679\n",
      "Training epoch: 1\n",
      "idx: 0, loss: 0.08501432090997696\n",
      "idx: 100, loss: 0.17185954749584198\n",
      "idx: 200, loss: 0.09786579757928848\n",
      "idx: 300, loss: 0.08116117119789124\n",
      "idx: 400, loss: 0.22271095216274261\n",
      "Training epoch: 2\n",
      "idx: 0, loss: 0.05359487608075142\n",
      "idx: 100, loss: 0.05140208080410957\n",
      "idx: 200, loss: 0.06912437081336975\n",
      "idx: 300, loss: 0.07847478985786438\n",
      "idx: 400, loss: 0.18062497675418854\n",
      "Training epoch: 3\n",
      "idx: 0, loss: 0.076697438955307\n",
      "idx: 100, loss: 0.10458298027515411\n",
      "idx: 200, loss: 0.1426263004541397\n",
      "idx: 300, loss: 0.11266268044710159\n",
      "idx: 400, loss: 0.18627998232841492\n",
      "Training epoch: 4\n",
      "idx: 0, loss: 0.07869116961956024\n",
      "idx: 100, loss: 0.10094181448221207\n",
      "idx: 200, loss: 0.05744136869907379\n",
      "idx: 300, loss: 0.06453729420900345\n",
      "idx: 400, loss: 0.11319413036108017\n",
      "Training epoch: 5\n",
      "idx: 0, loss: 0.06832613050937653\n",
      "idx: 100, loss: 0.08583221584558487\n",
      "idx: 200, loss: 0.07588335126638412\n",
      "idx: 300, loss: 0.08425242453813553\n",
      "idx: 400, loss: 0.16785404086112976\n",
      "Training epoch: 6\n",
      "idx: 0, loss: 0.03997800126671791\n",
      "idx: 100, loss: 0.029650766402482986\n",
      "idx: 200, loss: 0.08559603244066238\n",
      "idx: 300, loss: 0.08272907882928848\n",
      "idx: 400, loss: 0.08008729666471481\n",
      "Training epoch: 7\n",
      "idx: 0, loss: 0.03862752392888069\n",
      "idx: 100, loss: 0.0355803482234478\n",
      "idx: 200, loss: 0.08047810941934586\n",
      "idx: 300, loss: 0.03927309438586235\n",
      "idx: 400, loss: 0.15475380420684814\n",
      "Training epoch: 8\n",
      "idx: 0, loss: 0.07910441607236862\n",
      "idx: 100, loss: 0.06932332366704941\n",
      "idx: 200, loss: 0.07620660960674286\n",
      "idx: 300, loss: 0.07114056497812271\n",
      "idx: 400, loss: 0.06869550049304962\n",
      "Training epoch: 9\n",
      "idx: 0, loss: 0.04089872166514397\n",
      "idx: 100, loss: 0.06208046153187752\n",
      "idx: 200, loss: 0.03491291031241417\n",
      "idx: 300, loss: 0.07402260601520538\n",
      "idx: 400, loss: 0.08799336105585098\n",
      "Performing inference on batch: 0\n",
      "Performing inference on batch: 5\n",
      "Performing inference on batch: 10\n",
      "Performing inference on batch: 15\n",
      "Performing inference on batch: 20\n",
      "Performing inference on batch: 25\n",
      "Performing inference on batch: 30\n",
      "Performing inference on batch: 35\n",
      "Performing inference on batch: 40\n",
      "Performing inference on batch: 45\n",
      "Performing inference on batch: 50\n",
      "Performing inference on batch: 55\n",
      "Performing inference on batch: 60\n",
      "Performing inference on batch: 65\n",
      "Performing inference on batch: 70\n",
      "Performing inference on batch: 75\n",
      "Performing inference on batch: 0\n",
      "Performing inference on batch: 5\n",
      "Performing inference on batch: 10\n",
      "Performing inference on batch: 15\n",
      "Performing inference on batch: 20\n",
      "Performing inference on batch: 25\n",
      "Performing inference on batch: 30\n",
      "Performing inference on batch: 35\n",
      "Performing inference on batch: 40\n",
      "Performing inference on batch: 45\n",
      "Performing inference on batch: 50\n",
      "Performing inference on batch: 55\n",
      "Performing inference on batch: 60\n",
      "Performing inference on batch: 65\n",
      "Performing inference on batch: 70\n",
      "Performing inference on batch: 75\n",
      "FPR@95: 0.52, AUROC: 99.64 AUPR_IN: 99.70, AUPR_OUT: 99.59\n",
      "CCR: 83.73, 90.37, 95.60, 97.59, ACC: 98.30\n",
      "──────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "config_1c = {\n",
    "    \"batch_size\": 128,\n",
    "    \"dataset_name\": \"mnist\",\n",
    "    \"n_classes\": 10,\n",
    "    \"epochs\": 10,\n",
    "    \"version\": time.time(),\n",
    "    \"lr\": 0.01,\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 0.0005,\n",
    "    \"optimizer_type\": \"Adam\",\n",
    "    \"activation_function_type\": \"relu\",\n",
    "    \"network\": \"lenet\",\n",
    "    \"postprocessor_type\": \"odin\",\n",
    "    \"dataset_type\": \"mnist\"\n",
    "}\n",
    "config_1c[\"data_loaders\"] = get_data_loaders(config_1c)\n",
    "run_full_oodn_pipeline(config_1c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31331dd",
   "metadata": {},
   "source": [
    "#### 1(d) Adam + Softplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "d431c687",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting optimizer for type: Adam...\n",
      "Training epoch: 0\n",
      "idx: 0, loss: 2.27415132522583\n",
      "idx: 100, loss: 2.308807849884033\n",
      "idx: 200, loss: 0.7072268724441528\n",
      "idx: 300, loss: 0.18639841675758362\n",
      "idx: 400, loss: 0.3177991807460785\n",
      "Training epoch: 1\n",
      "idx: 0, loss: 0.11948294937610626\n",
      "idx: 100, loss: 0.06955642253160477\n",
      "idx: 200, loss: 0.11207076907157898\n",
      "idx: 300, loss: 0.07860124111175537\n",
      "idx: 400, loss: 0.22918754816055298\n",
      "Training epoch: 2\n",
      "idx: 0, loss: 0.07632873207330704\n",
      "idx: 100, loss: 0.05131511762738228\n",
      "idx: 200, loss: 0.10534033179283142\n",
      "idx: 300, loss: 0.08326361328363419\n",
      "idx: 400, loss: 0.19460807740688324\n",
      "Training epoch: 3\n",
      "idx: 0, loss: 0.05745804309844971\n",
      "idx: 100, loss: 0.07289870083332062\n",
      "idx: 200, loss: 0.08870033919811249\n",
      "idx: 300, loss: 0.1120220348238945\n",
      "idx: 400, loss: 0.21049143373966217\n",
      "Training epoch: 4\n",
      "idx: 0, loss: 0.08756529539823532\n",
      "idx: 100, loss: 0.06747358292341232\n",
      "idx: 200, loss: 0.15172752737998962\n",
      "idx: 300, loss: 0.09393009543418884\n",
      "idx: 400, loss: 0.16247057914733887\n",
      "Training epoch: 5\n",
      "idx: 0, loss: 0.055809617042541504\n",
      "idx: 100, loss: 0.12098618596792221\n",
      "idx: 200, loss: 0.09025582671165466\n",
      "idx: 300, loss: 0.110438771545887\n",
      "idx: 400, loss: 0.20125843584537506\n",
      "Training epoch: 6\n",
      "idx: 0, loss: 0.07142750173807144\n",
      "idx: 100, loss: 0.04791184142231941\n",
      "idx: 200, loss: 0.06108155474066734\n",
      "idx: 300, loss: 0.09656951576471329\n",
      "idx: 400, loss: 0.1698736697435379\n",
      "Training epoch: 7\n",
      "idx: 0, loss: 0.042597007006406784\n",
      "idx: 100, loss: 0.05889628082513809\n",
      "idx: 200, loss: 0.09362927824258804\n",
      "idx: 300, loss: 0.08048923313617706\n",
      "idx: 400, loss: 0.14935505390167236\n",
      "Training epoch: 8\n",
      "idx: 0, loss: 0.05421086773276329\n",
      "idx: 100, loss: 0.0937429741024971\n",
      "idx: 200, loss: 0.04892805963754654\n",
      "idx: 300, loss: 0.09579654783010483\n",
      "idx: 400, loss: 0.15903609991073608\n",
      "Training epoch: 9\n",
      "idx: 0, loss: 0.08198251575231552\n",
      "idx: 100, loss: 0.022096728906035423\n",
      "idx: 200, loss: 0.060744401067495346\n",
      "idx: 300, loss: 0.09230687469244003\n",
      "idx: 400, loss: 0.09625759720802307\n",
      "Performing inference on batch: 0\n",
      "Performing inference on batch: 5\n",
      "Performing inference on batch: 10\n",
      "Performing inference on batch: 15\n",
      "Performing inference on batch: 20\n",
      "Performing inference on batch: 25\n",
      "Performing inference on batch: 30\n",
      "Performing inference on batch: 35\n",
      "Performing inference on batch: 40\n",
      "Performing inference on batch: 45\n",
      "Performing inference on batch: 50\n",
      "Performing inference on batch: 55\n",
      "Performing inference on batch: 60\n",
      "Performing inference on batch: 65\n",
      "Performing inference on batch: 70\n",
      "Performing inference on batch: 75\n",
      "Performing inference on batch: 0\n",
      "Performing inference on batch: 5\n",
      "Performing inference on batch: 10\n",
      "Performing inference on batch: 15\n",
      "Performing inference on batch: 20\n",
      "Performing inference on batch: 25\n",
      "Performing inference on batch: 30\n",
      "Performing inference on batch: 35\n",
      "Performing inference on batch: 40\n",
      "Performing inference on batch: 45\n",
      "Performing inference on batch: 50\n",
      "Performing inference on batch: 55\n",
      "Performing inference on batch: 60\n",
      "Performing inference on batch: 65\n",
      "Performing inference on batch: 70\n",
      "Performing inference on batch: 75\n",
      "FPR@95: 13.54, AUROC: 97.72 AUPR_IN: 98.01, AUPR_OUT: 97.40\n",
      "CCR: 40.33, 59.82, 76.68, 92.41, ACC: 97.28\n",
      "──────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "config_1d = {\n",
    "    \"batch_size\": 128,\n",
    "    \"dataset_name\": \"mnist\",\n",
    "    \"n_classes\": 10,\n",
    "    \"epochs\": 10,\n",
    "    \"version\": time.time(),\n",
    "    \"lr\": 0.01,\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 0.0005,\n",
    "    \"optimizer_type\": \"Adam\",\n",
    "    \"activation_function_type\": \"softplus\",\n",
    "    \"network\": \"lenet\",\n",
    "    \"postprocessor_type\": \"odin\",\n",
    "    \"dataset_type\": \"mnist\"\n",
    "}\n",
    "config_1d[\"data_loaders\"] = get_data_loaders(config_1d)\n",
    "run_full_oodn_pipeline(config_1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978879fa",
   "metadata": {},
   "source": [
    "### Study 2: Resnet50, WILDS iwildcam as the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a2e95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset to data/camelyon17_v1.0...\n",
      "You can also download the dataset manually at https://wilds.stanford.edu/downloads.\n",
      "Downloading https://worksheets.codalab.org/rest/bundles/0xe45e15f39fb54e9d9e919556af67aabe/contents/blob/ to data/camelyon17_v1.0/archive.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c92a50b0f3d43b8832cf210759d5366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10658709504 [00:00<?, ?Byte/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config_2a = {\n",
    "    \"batch_size\": 16,\n",
    "    \"n_classes\": 2,\n",
    "    \"dataset_name\": \"camelyon17\",\n",
    "    \"epochs\": 10,\n",
    "    \"version\": time.time(),\n",
    "    \"lr\": 0.01,\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 0.0005,\n",
    "    \"optimizer_type\": \"SGD\",\n",
    "    \"activation_function_type\": \"softplus\",\n",
    "    \"network\": \"resnet50\",\n",
    "    \"postprocessor_type\": \"odin\",\n",
    "    \"dataset_type\": \"wilds\"\n",
    "}\n",
    "config_2a[\"data_loaders\"] = get_data_loaders(config_2a)\n",
    "# run_full_oodn_pipeline(config_1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0608b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_full_oodn_pipeline(config_2a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
