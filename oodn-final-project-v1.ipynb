{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a9f701b",
   "metadata": {},
   "source": [
    "### Steps:\n",
    "\n",
    "* Train LeNet5 model on mnist\n",
    "* Use ODIN postprocessor on mnistood\n",
    "* Get evaluation metrics\n",
    "* Load iWildsCam dataset\n",
    "* Trian on Resnet\n",
    "* Use ODIN postprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "9b10d253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torchvision.datasets import mnist, FashionMNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.optim import SGD\n",
    "from torch.nn import Module\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torchvision.models.resnet import Bottleneck, ResNet\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from openood.evaluators import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f160171",
   "metadata": {},
   "source": [
    "### Supported Activation Functions\n",
    "\n",
    "For activation functions, we are considering ReLU, Softplus, Swish. *Note that we may conduct experiments for a subset based on the compute resources available*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "b325b23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation_fn(activation):\n",
    "    if activation == 'relu':\n",
    "        return nn.ReLU()\n",
    "    elif activation == 'softplus':\n",
    "        return nn.Softplus()\n",
    "    elif activation == 'swish':\n",
    "        return nn.Swish()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a6eec2",
   "metadata": {},
   "source": [
    "### Supported Networks\n",
    "\n",
    "Currently, we support LeNet and ResNet50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "033e99bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self, num_classes, num_channel=3, activation='relu'):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.feature_size = 84\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=num_channel,\n",
    "                      out_channels=6,\n",
    "                      kernel_size=5,\n",
    "                      stride=1,\n",
    "                      padding=2), get_activation_fn(activation), nn.MaxPool2d(kernel_size=2))\n",
    "\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1),\n",
    "             get_activation_fn(activation), nn.MaxPool2d(kernel_size=2))\n",
    "\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16,\n",
    "                      out_channels=120,\n",
    "                      kernel_size=5,\n",
    "                      stride=1), get_activation_fn(activation))\n",
    "\n",
    "        self.classifier1 = nn.Linear(in_features=120, out_features=84)\n",
    "        self.relu = get_activation_fn(activation)\n",
    "        self.fc = nn.Linear(in_features=84, out_features=num_classes)\n",
    "\n",
    "    def get_fc(self):\n",
    "        fc = self.fc\n",
    "        return fc.weight.cpu().detach().numpy(), fc.bias.cpu().detach().numpy()\n",
    "\n",
    "    def forward(self, x, return_feature=False, return_feature_list=False):\n",
    "        feature1 = self.block1(x)\n",
    "        feature2 = self.block2(feature1)\n",
    "        feature3 = self.block3(feature2)\n",
    "        feature3 = feature3.view(feature3.shape[0], -1)\n",
    "        feature = self.relu(self.classifier1(feature3))\n",
    "        logits_cls = self.fc(feature)\n",
    "        feature_list = [feature1, feature2, feature3, feature]\n",
    "        if return_feature:\n",
    "            return logits_cls, feature\n",
    "        elif return_feature_list:\n",
    "            return logits_cls, feature_list\n",
    "        else:\n",
    "            return logits_cls\n",
    "\n",
    "    def forward_threshold(self, x, threshold):\n",
    "        feature1 = self.block1(x)\n",
    "        feature2 = self.block2(feature1)\n",
    "        feature3 = self.block3(feature2)\n",
    "        feature3 = feature3.view(feature3.shape[0], -1)\n",
    "        feature = self.relu(self.classifier1(feature3))\n",
    "        feature = feature.clip(max=threshold)\n",
    "        logits_cls = self.fc(feature)\n",
    "\n",
    "        return logits_cls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f9f5ba",
   "metadata": {},
   "source": [
    "### Supported Post-Hoc OODN Processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "dd5fd6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODINPostprocessor():\n",
    "    def __init__(self, temperature, noise):\n",
    "        self.temperature = temperature\n",
    "        self.noise = noise\n",
    "        \n",
    "    def postprocess(self, net: nn.Module, data):\n",
    "        net.eval()\n",
    "        data.requires_grad = True\n",
    "        output = net(data)\n",
    "\n",
    "        # Calculating the perturbation we need to add, that is,\n",
    "        # the sign of gradient of cross entropy loss w.r.t. input\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        labels = output.detach().argmax(axis=1)\n",
    "\n",
    "        # Using temperature scaling\n",
    "        output = output / self.temperature\n",
    "\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Normalizing the gradient to binary in {0, 1}\n",
    "        gradient = torch.ge(data.grad.detach(), 0)\n",
    "        gradient = (gradient.float() - 0.5) * 2\n",
    "\n",
    "        # Scaling values taken from original code\n",
    "        gradient[:, 0] = (gradient[:, 0]) / (63.0 / 255.0)\n",
    "#         gradient[:, 1] = (gradient[:, 1]) / (62.1 / 255.0)\n",
    "#         gradient[:, 2] = (gradient[:, 2]) / (66.7 / 255.0)\n",
    "\n",
    "        # Adding small perturbations to images\n",
    "        tempInputs = torch.add(data.detach(), gradient, alpha=-self.noise)\n",
    "        output = net(tempInputs)\n",
    "        output = output / self.temperature\n",
    "\n",
    "        # Calculating the confidence after adding perturbations\n",
    "        nnOutput = output.detach()\n",
    "        nnOutput = nnOutput - nnOutput.max(dim=1, keepdims=True).values\n",
    "        nnOutput = nnOutput.exp() / nnOutput.exp().sum(dim=1, keepdims=True)\n",
    "\n",
    "        conf, pred = nnOutput.max(dim=1)\n",
    "\n",
    "        return pred, conf\n",
    "    \n",
    "    def inference(self, net: nn.Module, data_loader: DataLoader):\n",
    "        pred_list, conf_list, label_list = [], [], []\n",
    "        for idx, (data, label) in enumerate(data_loader):\n",
    "            pred, conf = self.postprocess(net, data)\n",
    "            for idx in range(len(data)):\n",
    "                pred_list.append(pred[idx].cpu().tolist())\n",
    "                conf_list.append(conf[idx].cpu().tolist())\n",
    "                label_list.append(label[idx].cpu().tolist())\n",
    "\n",
    "        # convert values into numpy array\n",
    "        pred_list = np.array(pred_list, dtype=int)\n",
    "        conf_list = np.array(conf_list)\n",
    "        label_list = np.array(label_list, dtype=int)\n",
    "\n",
    "        return pred_list, conf_list, label_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d53b41",
   "metadata": {},
   "source": [
    "### Supported Out of Distribution Detection Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "6f53afd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_formatted_metrics(metrics, dataset_name):\n",
    "    [fpr, auroc, aupr_in, aupr_out,\n",
    "     ccr_4, ccr_3, ccr_2, ccr_1, accuracy] \\\n",
    "     = metrics\n",
    "\n",
    "    write_content = {\n",
    "        'dataset': dataset_name,\n",
    "        'FPR@95': '{:.2f}'.format(100 * fpr),\n",
    "        'AUROC': '{:.2f}'.format(100 * auroc),\n",
    "        'AUPR_IN': '{:.2f}'.format(100 * aupr_in),\n",
    "        'AUPR_OUT': '{:.2f}'.format(100 * aupr_out),\n",
    "        'CCR_4': '{:.2f}'.format(100 * ccr_4),\n",
    "        'CCR_3': '{:.2f}'.format(100 * ccr_3),\n",
    "        'CCR_2': '{:.2f}'.format(100 * ccr_2),\n",
    "        'CCR_1': '{:.2f}'.format(100 * ccr_1),\n",
    "        'ACC': '{:.2f}'.format(100 * accuracy)\n",
    "    }\n",
    "\n",
    "    fieldnames = list(write_content.keys())\n",
    "\n",
    "    # print ood metric results\n",
    "    print('FPR@95: {:.2f}, AUROC: {:.2f}'.format(100 * fpr, 100 * auroc),\n",
    "          end=' ',\n",
    "          flush=True)\n",
    "    print('AUPR_IN: {:.2f}, AUPR_OUT: {:.2f}'.format(\n",
    "        100 * aupr_in, 100 * aupr_out),\n",
    "          flush=True)\n",
    "    print('CCR: {:.2f}, {:.2f}, {:.2f}, {:.2f},'.format(\n",
    "        ccr_4 * 100, ccr_3 * 100, ccr_2 * 100, ccr_1 * 100),\n",
    "          end=' ',\n",
    "          flush=True)\n",
    "    print('ACC: {:.2f}'.format(accuracy * 100), flush=True)\n",
    "    print(u'\\u2500' * 70, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "5d89a721",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet50(ResNet):\n",
    "    def __init__(self,\n",
    "                 block=Bottleneck,\n",
    "                 layers=[3, 4, 6, 3],\n",
    "                 num_classes=1000):\n",
    "        super(ResNet50, self).__init__(block=block,\n",
    "                                       layers=layers,\n",
    "                                       num_classes=num_classes)\n",
    "        self.feature_size = 2048\n",
    "\n",
    "\n",
    "    def forward(self, x, return_feature=False, return_feature_list=False):\n",
    "        feature1 = self.relu(self.bn1(self.conv1(x)))\n",
    "        feature1 = self.maxpool(feature1)\n",
    "        feature2 = self.layer1(feature1)\n",
    "        feature3 = self.layer2(feature2)\n",
    "        feature4 = self.layer3(feature3)\n",
    "        feature5 = self.layer4(feature4)\n",
    "        feature5 = self.avgpool(feature5)\n",
    "        feature = feature5.view(feature5.size(0), -1)\n",
    "        logits_cls = self.fc(feature)\n",
    "\n",
    "        feature_list = [feature1, feature2, feature3, feature4, feature5]\n",
    "        if return_feature:\n",
    "            return logits_cls, feature\n",
    "        elif return_feature_list:\n",
    "            return logits_cls, feature_list\n",
    "        else:\n",
    "            return logits_cls\n",
    "\n",
    "    def forward_threshold(self, x, threshold):\n",
    "        feature1 = self.relu(self.bn1(self.conv1(x)))\n",
    "        feature1 = self.maxpool(feature1)\n",
    "        feature2 = self.layer1(feature1)\n",
    "        feature3 = self.layer2(feature2)\n",
    "        feature4 = self.layer3(feature3)\n",
    "        feature5 = self.layer4(feature4)\n",
    "        feature5 = self.avgpool(feature5)\n",
    "        feature = feature5.clip(max=threshold)\n",
    "        feature = feature.view(feature.size(0), -1)\n",
    "        logits_cls = self.fc(feature)\n",
    "\n",
    "        return logits_cls\n",
    "\n",
    "    def get_fc(self):\n",
    "        fc = self.fc\n",
    "        return fc.weight.cpu().detach().numpy(), fc.bias.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a893ec",
   "metadata": {},
   "source": [
    "### Full OODN Flow On LeNet5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f6b957",
   "metadata": {},
   "source": [
    "### MNIST Training and In-Distribution Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "2205033c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = mnist.MNIST(root='data', download=False, train=True, transform=ToTensor())\n",
    "test_dataset = mnist.MNIST(root='data', download=False, train=False, transform=ToTensor())\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a06f97",
   "metadata": {},
   "source": [
    "### Training LeNet5 On MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "4d49351a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx: 0, loss: 2.423340082168579\n",
      "idx: 100, loss: 2.3033783435821533\n",
      "idx: 200, loss: 2.309399366378784\n",
      "idx: 300, loss: 2.059486150741577\n",
      "idx: 400, loss: 0.45519348978996277\n",
      "accuracy: 0.86\n",
      "idx: 0, loss: 0.42277392745018005\n",
      "idx: 100, loss: 0.31062746047973633\n",
      "idx: 200, loss: 0.186587855219841\n",
      "idx: 300, loss: 0.14288684725761414\n",
      "idx: 400, loss: 0.527208149433136\n",
      "accuracy: 0.95\n",
      "idx: 0, loss: 0.15106496214866638\n",
      "idx: 100, loss: 0.1727137267589569\n",
      "idx: 200, loss: 0.152151957154274\n",
      "idx: 300, loss: 0.11541634798049927\n",
      "idx: 400, loss: 0.4183972179889679\n",
      "accuracy: 0.96\n",
      "idx: 0, loss: 0.1316715031862259\n",
      "idx: 100, loss: 0.055190905928611755\n",
      "idx: 200, loss: 0.1544199436903\n",
      "idx: 300, loss: 0.08166653662919998\n",
      "idx: 400, loss: 0.2765331268310547\n",
      "accuracy: 0.98\n",
      "idx: 0, loss: 0.09350304305553436\n",
      "idx: 100, loss: 0.051850125193595886\n",
      "idx: 200, loss: 0.16075889766216278\n",
      "idx: 300, loss: 0.08697907626628876\n",
      "idx: 400, loss: 0.23928365111351013\n",
      "accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "model = LeNet(num_classes=10, num_channel=1, activation='softplus')\n",
    "sgd = SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=0.0005)\n",
    "loss_fn = CrossEntropyLoss()\n",
    "all_epoch = 5\n",
    "\n",
    "for current_epoch in range(all_epoch):\n",
    "    model.train()\n",
    "    for idx, (train_x, train_label) in enumerate(train_loader):\n",
    "        sgd.zero_grad()\n",
    "        predict_y = model(train_x.float())\n",
    "        loss = loss_fn(predict_y, train_label.long())\n",
    "        if idx % 100 == 0:\n",
    "            print('idx: {}, loss: {}'.format(idx, loss.sum().item()))\n",
    "        loss.backward()\n",
    "        sgd.step()\n",
    "\n",
    "    all_correct_num = 0\n",
    "    all_sample_num = 0\n",
    "    model.eval()\n",
    "    for idx, (test_x, test_label) in enumerate(test_loader):\n",
    "        predict_y = model(test_x.float()).detach()\n",
    "        predict_y = np.argmax(predict_y, axis=-1)\n",
    "        current_correct_num = predict_y == test_label\n",
    "        all_correct_num += np.sum(current_correct_num.numpy(), axis=-1)\n",
    "        all_sample_num += current_correct_num.shape[0]\n",
    "    acc = all_correct_num / all_sample_num\n",
    "    print('accuracy: {:.2f}'.format(acc))\n",
    "    \n",
    "    if current_epoch % 25 == 0:\n",
    "        torch.save(model, 'models/mnist_{:.2f}.pkl'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afbb97c",
   "metadata": {},
   "source": [
    "### Loading OOD Dataset for MNIST - FashionMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "0ba4c34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_test_dataset = mnist.FashionMNIST(root='data', download=True,train=False,transform=ToTensor())\n",
    "fashion_test_loader = DataLoader(fashion_test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "d2939510",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "83bc710e",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 1000\n",
    "noise = 0.0014\n",
    "postprocessor = ODINPostprocessor(temperature, noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "e31a4f25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "id_pred, id_conf, id_gt = postprocessor.inference(\n",
    "            model, test_loader)\n",
    "\n",
    "ood_pred, ood_conf, ood_gt = postprocessor.inference(\n",
    "    model, fashion_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "03856055",
   "metadata": {},
   "outputs": [],
   "source": [
    "ood_gt = -1 * np.ones_like(ood_gt)  # hard set to -1 as ood\n",
    "pred = np.concatenate([id_pred, ood_pred])\n",
    "conf = np.concatenate([id_conf, ood_conf])\n",
    "label = np.concatenate([id_gt, ood_gt])\n",
    "ood_metrics = metrics.compute_all_metrics(conf, label, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "7c9ea2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPR@95: 9.65, AUROC: 98.02 AUPR_IN: 98.27, AUPR_OUT: 97.70\n",
      "CCR: 14.22, 48.70, 80.68, 94.02, ACC: 97.41\n",
      "──────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "print_formatted_metrics(ood_metrics, 'fashion_mnist')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccf73db",
   "metadata": {},
   "source": [
    "### iWildCam/WILDS Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2f4fc00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wilds import get_dataset\n",
    "from wilds.common.data_loaders import get_train_loader\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b3711dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_dataset(dataset=\"iwildcam\", download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "88f66849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = get_dataset(dataset=\"iwildcam\", download=True)\n",
    "\n",
    "# # Get the training set\n",
    "train_data = dataset.get_subset(\n",
    "    \"id_test\",\n",
    "    transform=transforms.Compose(\n",
    "        [transforms.Resize((448, 448)), transforms.ToTensor()]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a5c71618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8154"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e878068b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
